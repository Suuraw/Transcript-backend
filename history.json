[
    {
        "videoId": "7LXKPXbXxMQ",
        "timestamp": 1752300247.330563,
        "transcript": "because this has got you a bit hot under the cover. No, there's three things at play here to be honest. The first thing is that there is a serious issue with the Duke's ball. Both captains talked about it before the game. We've seen it in this game in this session. It's been changed twice. We've seen it in the last few years really, the Duke's ball going out of shape. The second point of play here is that I think the ball is changed too often. I think we're getting a bit precious about cricket balls in the history of the game. A cricket ball gets old and a cricket ball gets soft. And I think we're getting a bit sort of addicted to having the perfect cricket ball for 80 overs. Go back for generations, new new ball, new opening batters, see off that new ball and it gets easier. The third thing at play is that they got through in that first hour and Bumra was unplayable. They had a Duke's ball that was doing something and I looked up from my laptop at the back of the com box and looked up and went, \"They're changing the ball.\" Why would you change a ball that is doing something to a random box of balls? You know nothing about that. You know everything about this. This is doing it. The magician boomer is doing something with it. In this box, you have no knowledge of what those balls are going to do. And you go, \"Oh, we'll take that one.\" And then they find out that one isn't doing anything and they get upset. I get why they're getting upset. It did look older. It did look softer. But why change? Why take the gamble? I thought that was a real bizarre thing to do when you've got something, especially in this time when the Duke's ball is so all over the place. When you've got something, stick to it. They didn't. Did Schuman Gill overstep the line with the way he complained or was he justified in doing so cuz you could see the glee on his face when they eventually changed it again. Big smile, lobed it to Kale Ro who started polishing it up. Here we go. We got a harder ball. It was close after drinks because I think he was almost trying to make up for his original mistake. When it's your mistake, I think you get a bit I think you do get a bit crosser. I think he'd realize that he'd given up the asset of that ball that was doing everything for an asset that wasn't. And now at drinks, he's looking at this thing saying, \"This is soft. This is old. How have we got to this situation?\" So, he's angry, I think, more with himself for getting rid of that other ball. And he did push it with the line with the umpires. In the end for him, he got another one as well, but the horse had bolted. Two batters had got in.",
        "summary": "",
        "title": "Nasser Hussain NOT happy with how often the cricket ball is changed"
    },
    {
        "videoId": "SmW8wArpuJ8",
        "timestamp": 1752315292.768339,
        "transcript": "The first morning of the first day of a test match at Lords and the drama is about to be hyped up because here comes Mr. Ravi Shastri ready to do the toss. Do you get excited doing these? Especially excited a little nervous because uh this is a special ground you know so much tradition history when you're in the members there that long room itself can be very intimidating. Uh speaking to some of the members you know you can see their excitement. first day of a test match. Not just the players, it's even the spectators and even someone like me. The nerves are jangling a bit. Your nerves are jangling. That surprises me. How many of these have you done? Yeah. It's good. You must have those butterflies, you know, for me to rev it up there. So, how big do you need to rev this one up? This is special. Well, I mean, you can't have it's won one in the series. You can't ask for anything better. It's an enthralling series so far. England will want to fight back and get their own back in this test match. Set up beautifully this Anderson Tendulkar Trophy. How much prep do you have to do for this? Less the better because otherwise if you do too much you might just, you know, miss the real thing out there in the middle. What are you hoping to get from the captains and these sort of things? Because Bumble's point of view is always what we need from the toss, who's batting, who's bowling, who's playing. But there's a bit more theater and drama to get into. And what I would ask them is the pitch and I I'll be very interested to see the decision on winning the toss. Both captains wanted to field in the last game. It's another hot day today. Temperatures in excess of 30. Little more grass though in the surface. So I won't be surprised again if fielding is the option. England might stick to that with Archer playing, Bumbra playing, even India might stick to that. But let's see how it goes. I know you got the inner ear pieces in to try and block out some noise, but Lors is quite peaceful. What are those World Cup finals like? met IPL finals or something like that. That's crazy. That's crazy. I mean, you've got to be hearing the producer very very, you know, closely because when the countdown begins for the toss, you know, you you got to be ready, you know. Now, just just explain what happens here then. So, the captains are with Richie Richardson now. What what do they do? What's the process that happens before you take They will exchange sheets. Uh they will mention what their Lebanon is, their combination is, and then take it from there. I always think you should be the first person to know who the teams are, but you always seem to get it quite late, don't you? I get it late. Yeah. So, here we go. What would you do? What do you think Shubman will do if he wants? I think Ben might still feel. Do you? Uh, Shubman might back. Okay. Well, let's see. I'll let you go and do your thing, Ravi. Thank you very much indeed. Here's Ravi Shastri. It's the Anderson Tendulkar Trophy. It's cricket's home. It's the iconic Lord's cricket ground and it's the venue for test match three in the five match series between England and India as we come up to toss time here at Lords. The two captains Ben Stokes England Schubank India Richie Richardson from the West Indies is the match referee and Ben will give it a flick. Heads. Heads is the call. What's it Richie? Ben, you want the toss again? What have you decided? Yeah, we're going to have a bat. What do you make of the surface and something to do with the weather? Um, yeah, surface is generally as it always is here at Lords on um, you know, early on and yeah, sun's out baking. Two test matches, 1-1. Plenty to play for. What's the mood like in the dressing room? Yeah, mood's good. Um, you know, it's been um, you know, very well fought first two test matches, obviously going into this third one, 1-1. Um, set the series up nice, but uh, yeah, we're obviously pushing towards coming away from Lords 2-1. How are the bodies coping? because not a not a big gap between the last test and this one. Yeah, bodies are all good. Obviously quick turnaround. So, um using that time that we had off in between these tests wisely and and everyone's fresh and ready to go. Ben Stokes likes playing at Lords, so you'll be looking for something special here. Yeah, I think everyone likes playing Lords. Um obviously everyone in our camp and every time that we have a touring team here, I think this week's um very special and you know, it's one of those weeks where you got to try and enjoy it as much as you can. And your combination for this pitch? Um yeah, we got one change from last week. Um Joff Ratcher comes in for Josh Tong. Have a good one, Ben. Thank you very much. Cheers. Shan, you would have won the toss. What would you have done? Uh, I was a bit confused this morning, but uh I think I would have bowled first uh and came in yesterday. The wicket had a little bit of green on it and we have seen if at all there is anything in the wicket. I think it's on the first day and first session. The series beautifully poised, great character shown by the boys in the last test match to bounce back. Yes, definitely. I think the contributions from everyone came in. And uh all the batsmen chipped in, all the bowlers chipped in and I think uh that is what we were aiming as a team. If all 11 of us can contribute uh in the field in the in the bowl in the bat, we'd be in a good space. Hard work for the bowlers that last test match. Good pitch, 20 wickets taken. How are their bodies recovered in four days? Yeah, I think so. Yeah. Uh the bowlers are feeling pretty confident. Wasn't easy on that kind of a wicket to be able to take 20 wickets but uh the attitude and the the efforts that they put in were truly appreciated and I think we were well rewarded for that. And how's Shuman Gil feeling? Yeah, you've you've been in the in the two test matches 430 runs must have taken a toll on your body. I'm feeling great you know uh as a batsman you always expect yourself to be in the middle in middle of the competition and it's feeling good your uh the hard work when it pays off it's quite satisfying. and your team for today's game for this pitch. We have got one change. Uh Bumra comes back in for Prasad. Have a good one. Shan, thank you. Well, that's the news from the center. It is England who won the toss and they've decided to bat first.",
        "summary": "",
        "title": "Ravi Shastri's guide to performing a coin toss"
    },
    {
        "videoId": "Pb-AwAPBE10",
        "timestamp": 1752386544.601938,
        "transcript": "Well, you know what Nas is like, Wardy? He likes to just take the crowd in. He walked all the way over to the Coronation Gronation Gardens and the crowd were clapping and cheering him and he so enjoyed that he walked all the way back to the nursery ground to join us. We're delighted to have been joined by Ajinki Rahani who's played 85 tests for India, scored more than 5,000 test runs, 12 tests hundreds, captained India in six games, winning four of them. So, an exceptional record. I nearly said former India batter, but that would be quite wrong. You're 37 years of age, but you haven't given up hopes of playing test cricket again. Not at all. First of all, it's good to be here. I still want to play test cricket. I'm really passionate about know uh playing test cricket and at this moment I'm enjoying my cricket. So just here for few days know I carried my trainers my training clothes so that I can know keep myself fit. Our domestic season is starting. So just uh the preparation has just begun. How realistic is it for you at 37 years of age having had a wonderful career obviously which we'll come to but it feels like there's been a changing of the guard with the retirements of Virat Kohli and Roit Sharma Kale Rahul who we watched this morning kind of one of the few remaining established players in there. Do you have conversations with the selectors uh or is it just a case of you've got to keep playing churning out the runs and hope for the best? See for me it's all about focusing on the controllable things you know I frankly I tried to have conversations with the selectors but again know the things as a player I cannot control I mean I got no response for me what as a player I can do is know keep playing cricket keep enjoying the game and give my best each and every time as I said I love playing test cricket I love playing red ball and it's a passion it's a love for the game that always keeps me going quite like to have a bat out there as well that looks a pretty flat pitch this morning. It looks really flat. I think after a long time I'm seeing flat wickets, slow wickets here in England. Uh last two test matches been really high scoring. Uh yeah, what would a mood been like in that dressing room because they would have been on such a high they were all getting ready on the balcony. Brilliant partnership, two brilliant players and then suddenly must be a little bit deflated on there because in Rishad Pant tried to been concerned about his partner, he's just opened the door for England. I think so. So I think that run out was crucial for from English point of view. No Kale and Brisha Pun they were going really well as Ada said batting on this wicket flat and slow wicket as a batsman if you can just keep your head down know and if you think I'm going to play good cricket you're not going to get out but I think that was a brilliant run out by Stokes. Great feeling as a fielder I think just one or a couple of balls left for the lunch. Have that intensity have that attitude to get that run out that was magnificent. So I I think Indian team will be slightly on the edge because you know you need one more partnership from Indian team point of view to get that big lead. Kale Ra is batting really well. If he gets his 100 I think in his mindset he's looking to get a bigger score. He's a bit like you. He's got a lot of his hundreds away from home. You got eight from 12. What what do you think between you? What do you think you have to get right? I know these are not uh English typical English conditions but in general playing away from home. What does he do well? What did you do well? I think it's all about the preparation when you're playing away from home, it's all about the preparation and again assisting the conditions and the situations well and if you are as a batsman if you are set it's all about get make the most of it. I used to I I mean I batted at number five number six in test cricket in India you know the flat wickets sometimes you get bored sitting in a dressing room and when you get in number five or number six the score is around 300 350 for three and then when in uh when when when we play abroad it's 20 for three 50 for three and as a five number six batter you are straight in I always like to go inside and know and keep contributing for the team I hate sitting in the dressing room that was the reason I think I got more hundreds outside India than uh India But again, Kale's preparation has been amazing. I've been watching him play. I've been know seeing his preparation even in shorter format, red ball format. He's really focused about his game and his attitude has been fantastic and really he's really hungry for runs. Tell us a little bit about your background. You were born about 5 hours outside of Mumbai but moved towards the city very very quickly. Um you know what was the cricket like when you were growing up? We hear a lot about the Mumbai school of batting. Was that something you were very aware of as a kid? I mean I I'm fairly certain there was absolutely no sport, no cricket in your family. So where did the love come from? I I was born in a place called Ahmed Nagar which is 5 hours as you mentioned uh from Mumbai and we moved when I was 10 days old. Uh my father was working in Mumbai. So we had to move uh into Mumbai but again we were staying in Dumbuli that time which was two which is 2 hours away from the city. For me it was all about know I used to play with my my friends in our backyard and just love started from there and I every day I used to go with my bat and used to play with tennis ball. It was always about having fun and I started enjoying the enjoying the game but as you mentioned there's no sporting background in my family. It was pure love. The more I started playing cricket I started enjoying it. And uh talking about Mumbai cricket I used to travel from Dumbi to the CST terminals was 2 hours journey one way uh with a kid baguh on my shoulder. So I think that made me really tough as a player you know when when I when I used to get out early I used to while coming back to feel really sad you know when you travel 2 hours one way come back 2 hours another way and when you get out in the game you know you feel really sad but the hunger the the inspiration and also the passion actually started from there. Um let's talk about happy memories here at Lords 100 in that 2014 test match. I think we've got a little bit of footage of this game. What what are your memories of the match first of all? Absolutely. Absolutely. I really enjoyed batting on that wicket. You know that 100 was really special. You know, we went on to win the test match. But I I'll tell you an incident. A couple of days before the test match, we had our team dinner in Indian High Commission and many many people they came up to me probably they they went up to many players and they said no scoring 100 at loss is something special. You walking down the stairs you know walking through that long room and coming into the ground. Uh that walk was something special but again know we were in a tough situation uh in that game 110 for seven odd and then getting 100 from there. uh the crucial partnership between Gheshwar Kumar and myself was amazing but I think uh because of the 100 and also we won the test match uh that inning was really special. You got out to Anderson in one innings and broad in the other. Who was the better bowler? Please say Jimmy Anderson for us please. They both are equally they both are the quality bowlers. Jimmy you know Jimmy and Broady I think they must be really happy not playing on the wicket. Now, Ishant Sharma, he got a few wickets though. He got seven for in that in that second innings. And England, I remember because I had to do the post-match press conference with Alistister Cook and I had to ask him was he going to resign because England were under tremendous pressure. The England captain was under tremendous pressure. Did you get that sense in that series? Yes, we did. But I think we knew that oh, England will come back really harder. They know the conditions really well here. They had a very good team when we played against uh England in 2014. Uh know when you lose a test match at home there's a pressure on the captain also on the team but I think uh we don't wanted to take it for granted you know for us it was all always about yes we won the test match there but again it's focus on the next game and then move forward. We thought yes England will be under pressure captain will be under pressure but we we knew that they will come back really stronger. Um let's talk about the the tour of Australia in 2021. An unbelievable win in in the Border Gavvas. You had to take over from Virat Kohli after the first game uh when India bowled out for what was it 36 at Adelaide. You got a 100 in the next game, the Boxing Day test match. First of all, do you see that as your kind of crowning moment leading India to that great victory in Australia? Absolutely. Yes. I mean that series was something really special for me. The preparation started when we were in Dubai playing IPL. You know I got to know that BC had told me Virat is available for the first test match and this then he's flying back home and you will take the charge. I have a habit of meing maintaining a diary. So I had few plans ready you know before that we tooured Australia two three times. I know the kind of cricket Australia play. So for me it was always about you know no making the plan and the plus point was when we went to Australia we had three practice games or three four day games that really helped us know preparing for the test test match but the what happened in ads we never imagined know 36 all out but from there when I when I took charge of the team for three test matches it's it was always about you know giving importance to every player you know every player in the team is equally important you know how can I give them the freedom and everyone can come up to me and know tell them whatever they feel. So I think it was always about the enjoying cricket and trying to help each other out rather than focusing or stressing too much on the result or the outcome. So on Melbourne onwards we started know thinking about one test match at a time and then trying to do well on the field and then just back our game and what happened in the uh in Brisbane was something special. Um I mean we played in a team that was 45 all out or at least I played you were 12 but 36 is I mean you don't kind of imagine it. Can you just take us into the dressing room at the end of that game? First of all we'll move on to the great wins. What was the dressing room like when you've just been blown away for 36 and what was Ravi Shastri like as coach? Tell us what Ravi said after 36 all out. No firstly I think the dressing room was quiet. No, I remember when you when you go to Adelaide, you walk through that tunnel towards the dressing room morning know just before the game when we were walking towards the dressing room and we thought if we if we can get 275 280 on the board fourth innings uh we have a game in hand and we we scored 36 all out uh and then everyone dressing room was completely quiet everyone was just knowing down but I think the Ravi Rai's energy I think his you know you guys know know really well I think he's he was really positive No, he said let's not think about cricket next two days let's chill no whatever you guys want to do please do it but after that when we go to Melbourne we'll focus on the game let's take for me and what we discussed was let's take these three test match series after this after adellet let's give our best but I think his his motivation his inspiration and his words during that time was amazing I asked DK about your captaincy and what style of captain because you were a very successful captain at state level he said you were a bold bowlers captain and you're very calm and softly spoken here but quite a feisty in-your-face sort of captain. What does he mean by a bowlless captain and do you wish you'd captain India a bit more? Uh yes I uh I would I mean I would have loved to but again I'm really happy I captain six test matches and the result what we got uh I'm really happy with that but again I like to give freedom to our ballers because ballers are the ballers are the one they're going to win the match for you win the series for you if you have if you can give the freedom to the ballers I know it's always about that so I feel for me yes I have two three plans ready but it's first is always about giving them the freedom. No, go according to the plan, go according to the field. If they're not doing well, just support them. It's always about that. So, yeah, I have always been the ball ballers captain. Did did you find it quite difficult taking over from Virat? He he stepped away obviously for the birth of his child. He's a very different style of captain. He's a very different kind of character in the field. Very expressive, very emotional. You strike me, I don't know you well, but you strike me as much calmer, you know, the lower heartbeat. Did you find that tricky transition? Did you feel the temptation to try and live up to how Virat captained on the field of play or did you just say no, I'm very different. I know my own style. I'm confident in my own style. I'll just do it in my own way. Absolutely. I know my own style and I always wanted to back my style know rather than copying any individual. All the captains, they have their different styles. For me, it was always about staying true to my character and just backing my instincts. I like to know back my instincts, maybe take chances or maybe go to some different plans where no one can think. I like to observe people and know I played under MS Dhoni, I played under Virat Kohli. I always used to observe them that their how they used to react to different situations. But when I became the captain in test cricket for those three test matches and also one of one of test match against Australia in Darham Shala and also New Zealand I always wanted to back my style and uh back my thinking and instincts. What about Schubman? What what style of captain remembering you're still trying to get back into his side by the way. Um but what when you see him in IPL cricket and you see him out there in these three test matches so far how would you describe Shubman as a captain? I guess he's still learning. He still has long way to go. That's my point of view. Uh I'm I'm always been a frank guy. I like to put my opinion in a very frank manner. I think he has a long way to go. He's still learning about about the test cricket. See t T20 format is completely different. But test cricket sometimes I feel you got to be really proactive know about in and out fields. Think about the sessions. How can you bowl a particular session in a different manner? Maybe sometimes you got to be got to bowl defensive line to uh because right now the the cricket England is playing they're looking to attack. If you can just slow it down. So as a captain I feel in test cricket you got to be proactive. You cannot just know lose your track. It's it has to be you know one session at a time. You got to know take one hour at a time and then take it forward from then. Talking about Schuman, I think he still has a long way to go and I think he's still learning and and Rishad Pant was a a young player when you captain in that series in Australia. He was probably, I don't know, a dozen matches into his test career. Um, has his success surprised you in any way? and knowing how he played and plays and played as captain did you just give him complete free reign to play in his own way or did you try and give him certain instructions or just let him be so in Australia I'll tell you a story in Brisbane especially we tried to know convey message to Rishapan whatever message we know sending it out to he did the opposite so we thought we thought you know we better not send a message let him play how how he wants to Okay. Or just know sometimes you send the opposite message the wrong message so that Risha will do us the thing right thing. So but again no he's been fantastic for us know batting on that five number five or number six in test cricket taking on the ballers know being fearless all the time and uh you need one guy like that in your team in especially in test format to take the opposition on and and the take the game on. So I think he's been doing really well. Um and give us an overview of of cricket in India at the moment. You're obviously captaining KKR in the IPL. We see the IPL growing and growing and becoming ever more dominant. How do you sense where test cricket stands, where the IPL stands and where young players stand in terms of their ambitions in in various formats. See the younger players, I mean the talent which we have in India is amazing. And know we all know that the IPL is providing a great platform to to the youngsters uh exposing them to a higher level and know they're playing against world-class ballers. They're playing against I know bowlers are bowling against world class batsmen. So T20 format is going like crazily but I feel I would love to see here some movement for the fast ballers. Test cricket you don't want to see flat and slow wicket. If ballers can get slightly help again T20 format is basically a batsman game but again in test cricket if you can see a little bit of movement a little bit of pace and bounce in the wicket which is exactly what Shubman said actually after the last game even though he made 269 and 161 it'd be very easy having made those runs to kind of talk up how difficult the conditions were but he said no these pitches are too flat and they're taking something away from test cricket if you don't have that balance between bat and All absolutely. I mean you you want to see two batsmen are facing a bowler or two bowlers bowling that spell 1 hour spell grinding it out first first day of the no test match first session grinding it out go to the lunchtime after lunch we all know that wicket generally becomes really good but I would love to see a little bit more contest between bat and ball in test format and the Indian fans while I was going to St. Johnswood station, Baker Street station, Coronation Garden, you were here and all these Indian fans, Edgebaston last day was all India. Absolutely love their cricket and love you as a player. Yeah. No, it's been amazing the support which we get in know from the Indian fans all over the world has been amazing. So, uh and we are all always grateful for that. They travel all around the world, you know, we've been to Australia, England, South Africa, New Zealand, wherever you say, the fans has been amazing for us and their support. It means a lot you know the when when you know even in the seat they sit here watch the game supporters they enjoy the game. So it's so good to see all the fans Indian fans and all the English fans you know enjoying the cricket test cricket that's what I like to see you know people are actually enjoying test cricket because test cricket is a real game. Great to see you Jinx enjoy the rest of your stay. You're off back home next week I think. So uh you'll see the end of what hopefully will be a terrific test match. The sun is beating down here at the nursery end. beating down on NASA's head. He's going to need some sun cream on that in five minutes time.",
        "summary": "As a technical educator, here is a detailed, structured summary of the cricket match toss at Lord's, designed for learners to understand the process, strategy, and context of a Test match.\n\n---\n\n## The Test Match Toss Explained: Drama, Strategy, and Decision-Making at Lord's\n\nThis transcript captures the excitement and strategic depth surrounding the coin toss at the start of a **Test Match** \u2013 the longest and most traditional form of cricket \u2013 specifically at the iconic **Lord's Cricket Ground** in London. It's not just a coin flip; it's a crucial decision that can significantly influence the game's outcome.\n\n### 1. Pre-Toss Build-Up: The Atmosphere and Broadcaster's Perspective\n\nThe segment begins with broadcaster Ravi Shastri preparing to conduct the toss. This part highlights several key aspects:\n\n*   **The Special Aura of Lord's:** Known as the \"Home of Cricket,\" Lord's is steeped in **tradition and history**. Ravi Shastri emphasizes the intimidating yet exciting atmosphere, particularly in the \"Long Room\" where players pass through. Even seasoned professionals like him feel \"jangling nerves\" and \"butterflies,\" underscoring the immense pressure and privilege of being there.\n*   **Series Context: The \"Anderson Tendulkar Trophy\":** This is a specific name given to the Test series between England and India, named after two cricketing legends (James Anderson for England, Sachin Tendulkar for India). The series is currently poised at 1-1, making this third match at Lord's critical.\n*   **Broadcaster's Preparation:** Shastri shares his approach to preparation (\"less the better\"), aiming for spontaneity and authenticity rather than over-rehearsal. He focuses on eliciting drama and key information from the captains.\n*   **Anticipating the Captains' Decisions:** This is where the strategy comes in. Factors influencing the toss decision are discussed:\n    *   **The Pitch:** Described as having \"a little more grass\" than usual, despite being a hot day. A grassy or \"green\" pitch typically favors fast bowlers early in the match, making \"fielding\" (bowling first) an attractive option.\n    *   **Weather:** A \"hot day\" (over 30\u00b0C) means the pitch will likely dry out and potentially become easier for batting as the match progresses.\n    *   **Team Strengths:** Both teams have strong fast bowlers (e.g., Archer, Bumrah), which could encourage them to bowl first if conditions allow.\n*   **Broadcasting Logistics:** Shastri mentions \"inner ear pieces\" \u2013 essential tools for broadcasters to hear instructions from the producer, especially during live countdowns to ensure precise timing for the toss. He contrasts the relative peacefulness of Lord's with the \"crazy\" noise of major events like World Cup or IPL finals.\n\n### 2. The Toss Process Explained\n\nRavi Shastri describes the formal procedure before the coin flip:\n\n*   **Role of the Match Referee:** Richie Richardson from the West Indies is the \"Match Referee.\" Their role is to ensure the match is played according to the Laws of Cricket and the playing conditions, acting as an impartial authority.\n*   **Team Submission:** Before the toss, the captains \"exchange sheets.\" On these sheets, they formally list their **Playing XI** (the 11 players selected for their team) and their **combination** (how the team is structured, e.g., number of batsmen, bowlers, all-rounders). This is why Shastri often gets the team news \"late\" \u2013 it's officially confirmed at this point.\n*   **The Coin Flip:** Ben Stokes (England captain) performs the flip, calling \"Heads.\" The Match Referee confirms the outcome.\n\n### 3. Captains' Interviews: Decisions and Reasoning\n\nAfter the toss, Ravi Shastri interviews both captains to understand their decisions and team status.\n\n#### A. Ben Stokes (England Captain)\n\n*   **Toss Result:** England won the toss and decided to **bat first**.\n*   **Reasoning for Batting First:**\n    *   **Pitch Assessment:** Stokes notes the \"surface is generally as it always is here at Lords on early on,\" implying a belief that while it might offer some challenge initially, it will settle down and become good for batting, especially with the \"sun's out baking.\"\n    *   **Series Goal:** With the series at 1-1, England is \"pushing towards coming away from Lords 2-1.\" Batting first allows them to put up a score and put pressure on India.\n*   **Team Mood & Fitness:** The mood in the dressing room is \"good.\" Despite a \"quick turnaround\" (short break) between Test matches, the players' \"bodies are all good\" and \"fresh,\" having used the off-time wisely.\n*   **Enjoying Lord's:** Stokes echoes the sentiment that \"everyone likes playing Lords\" due to its special atmosphere.\n*   **Team Change:** England made one change: **Jofra Archer** (a fast bowler) replaced Josh Tongue. This change impacts their \"combination.\"\n\n#### B. Shubman Gill (India Captain)\n\n*   **Hypothetical Decision (if he had won):** Gill admits he was \"a bit confused this morning,\" but ultimately he \"would have bowled first.\"\n*   **Reasoning for Bowling First:**\n    *   **Pitch Conditions:** He observed that the \"wicket had a little bit of green on it\" yesterday. He believes any advantage for bowlers on such a pitch would be concentrated \"on the first day and first session.\" This suggests a strategy to exploit early movement and bounce to get quick wickets.\n*   **Team Performance & Spirit:** Gill highlights the \"great character shown by the boys in the last test match to bounce back.\" He emphasizes the importance of **team contributions** \u2013 \"all the batsmen chipped in, all the bowlers chipped in\" \u2013 for overall success.\n*   **Bowler Recovery:** Acknowledging the \"hard work\" of the bowlers in the last match (where they took \"20 wickets\" \u2013 meaning they bowled out the opposition twice), he states they are \"feeling pretty confident\" and \"well rewarded\" for their efforts.\n*   **Personal Form:** Shubman Gill, as a batsman, expresses satisfaction with his performance (\"430 runs\" in two matches), noting it feels good when \"hard work... pays off.\"\n*   **Team Change:** India also made one change: **Bumra** (referring to Jasprit Bumrah, their star fast bowler) came back into the team for Prasad. This reinforces the focus on fast bowling capabilities.\n\n### 4. Key Takeaways for Learners\n\n*   **The Toss is a Strategic Decision:** It's never just random. Captains weigh pitch conditions (grass, dryness), weather forecasts, their team's strengths (e.g., strong bowlers for a green pitch), and the opponent's weaknesses.\n*   **Pitch Reading is Crucial:** Understanding how the pitch might behave over five days (e.g., favoring bowlers early, batsmen later) is a key skill for captains.\n*   **Team Balance (\"Combination\") Matters:** The decision on who to include in the Playing XI (e.g., an extra fast bowler, a spinner) is directly linked to the pitch conditions and the chosen strategy after winning the toss.\n*   **Test Cricket is a Marathon:** Player fitness and recovery (\"bodies coping,\" \"quick turnaround\") are vital, as Test matches are physically demanding and often follow in quick succession.\n*   **The Human Element:** Even at the highest level, professionals experience nerves and excitement, adding to the drama and spectacle of the sport.\n*   **Outcome:** England won the toss and decided to **bat first**. This sets the stage for the match, with England hoping to put up a big score, while India will aim to take early wickets.\n\n---\n\n### Glossary of Key Cricket Concepts\n\n*   **Test Match:** The longest format of cricket, played over five days between two teams of 11 players. It's considered the ultimate test of cricketing skill and endurance.\n*   **Lord's Cricket Ground:** A historic cricket venue in London, often referred to as the \"Home of Cricket.\"\n*   **Toss:** The coin flip at the start of a match. The captain who wins the toss decides whether their team will **bat first** (try to score runs) or **field first** (try to get the other team out by bowling).\n*   **Pitch / Wicket:** The central strip of the cricket field where most of the action happens. Its condition (e.g., grassy, dry, cracked) significantly influences how the ball behaves.\n*   **Bat First:** The team that bats first sends two batsmen out to try and score as many runs as possible.\n*   **Field First / Bowl First:** The team that fields first sends their bowlers out to try and get the opposing batsmen out.\n*   **Playing XI:** The 11 players selected to play for a team in a specific match.\n*   **Combination:** Refers to the balance of players in a team, e.g., how many specialist batsmen, bowlers, or all-rounders.\n*   **Match Referee:** An independent official who oversees the match to ensure it is played fairly and according to the rules.\n*   **20 Wickets Taken:** In a Test Match, a team needs to dismiss all 10 batsmen of the opposing team *twice* (total of 20 wickets) to win the match, assuming they also score enough runs.",
        "title": "\"I still want to play Test cricket!\" | Ajinkya Rahane speaks to Athers & Nasser at Lord's"
    },
    {
        "videoId": "Tf-1JqraVEU",
        "timestamp": 1752483224.68152,
        "transcript": "The atmosphere me and Kumar were waiting at the other end to come across with the players and all the Indian fans every time Akash deep block one was akash deep akash deep and then is off stump went cartwheeling and the England fans absolutely went ballistic. The atmosphere of this ground has been fantastic. One all you know tomorrow is going to be such a massive day. It that stokes at the end there just gave England a bit but this is the man they want. And the net obviously Rishad Punt now can do something dangerous. This is for me now a just India maybe 5248 sort of game but that pitch tomorrow morning they've got to use the new the newish ball and hit those cracks hard. How are you seen it? I think England's going to be a lot more confident than India at the moment. I think the way I saw India bat today there were a few jitters especially from Jwal a little bit from Karunaya. They seem to have gone a little bit too negative. Washington mentioned they wanted to be one wicket down I think they should have probably given themselves a target of runs but coming in tomorrow exactly as NASA said Kale Rahul and Rishab Pant that partnership if they can get it going and stretch this this this partnership to just beyond 100 for the team that India's back in the driving seat let's run through the wickets that England have taken then the first one Jwell you noted it in the first overy facing Chris woke perhaps a little jittery he was I saw him at short leg just before he had to go in to bat and he was stretching. He seemed a little bit checked out and then he came in and I noticed the first over balls and lengths that he would usually attack. He was letting go and then he was caught in two minds. Try to drag one wide out of off some which he never really tries to do on the leg side and paid the price. I would have liked him to really be much more positive and decisive. Stokes decided to swap the bowlers around really. Archer would normally bowl probably from this end and woke slight the other end. The reason he swapped him around is because of what we saw earlier from Jasper Bmer running in from the nursery end. Lavish bounce really challenging Zack Cwley and Ollie Pope. When Archer came on there was a fascinating passage of play. He went a bit too full. He got driven. He was then asking for the field on the off side to be moved and Stokes just stood there like a statue. Said, \"No, we agreed with the plan. You're not bowling to the plan.\" And in the end he took him off and he took it over the bowling himself. Yeah. You don't set a field for poor bowling and Joffrey didn't quite get it right in that spell. He's not a machine. He's not played a test match for four years. He's played one first class game and he didn't quite get that new ball right. You know, he's supposed to hit that 8 m length. And if you look at his pitch map, he was all over the place. In general, actually, he was full and floaty at 47%. You'd want those the other way round, maybe 20 odd% full just in case it does swing. like cast there at the end got it full and swinging and hitting banging away at that 8 m length all the time where Bumra was hitting. So he didn't get it right. He's that sort of cricketer. We've played with cricketers that when it's not right, they sort of blame something else, the field or something and Stokes managed the situation perfectly. I've set the field. You're not quite getting it right and I'll change it. And he did change it and he brought himself on at that far end. The two Durham lads absolutely delivered and brought England back in this game. cast his spell here and Stokes at the other end. They found the length and bowled beautifully. Well, Car got rid of Karen. I mean, a decent delivery nipping down. Is that just a brain fog from a batsman talking I mean how what goes through a bat's mind when you leave a ball like that at Lords with the slope coming back into I I think it it it is what leads up to that. You saw Karuna start off really positively. Good strides really going at at at the forward defensive with the right intent. might have looked at the clock and thought, \"Well, not much to go here. Let me kind of go back into this and play for time.\" That's where the negativity starts. The moment you do that, you're indecisive. You You're in two minds. And that's the ball he plays 99 times out of 100. The only thing that that I can try and give an explanation for that is saying he's watching the clock. He's thinking about tomorrow. And as Kane Williamson said, he's not present. Big wicketer Shman Gil. Now, even before a ball was bowled, everyone was saying, \"What you got to do with Shman Gill outside off stump? Try and run it into the pads.\" It obviously hasn't worked up to this point. Is it because though this pitch has got just that little bit more and it does it quicker? Does it quicker? That's the key. I mean, I did a lab piece and 603 runs later I was absolutely nip it back in. It got cost him 603, but you're spot on. quicker pitch nipping got it absolutely spot on with the length. Also, let's be honest, he had the England fielders in his ear. If you're going to give it out like last night, you're going to get it as well. He had Brook straight away in his ear. He had Root, he had Ducket. Crawley was in the boundary staying away from it. And he just looked a bit a big booming drive. That was when the theater was building towards the end there. He was a massive wicket obviously because he scores at a good tempo. He's not going to go in his shell. he would have got the board ticking along tomorrow. talking about theater and I know it was the night watchman Akash deep but as you mentioned you and Nash the crowd and I mean this is just what Stokes does isn't he yeah he's he's brilliant I mean when he changed archer he brought himself on he brought cast on two people who really stepped up and I think Stokes feeds off situations like that he needs that pressure he needs things to be on the line to really raise his game we didn't see that with the bat but that's the beauty of Stokes comes back with the ball he adjusted the field just before that delivery as well He's trying to tell Akash deep it could be a short ball. It might be coming at the stumps as well but he held a perfect line and length. Akash deep tail ender. Nothing much you can do. The man that got India going today was the villain as some people call him. I mean he's box office to watch and there's so many box office cricketers in both camps but his emotions his passion. It's sort of a kohi-esque way of playing cricket. It is and that's why he played really well under kohi. Mel Jones asked me on commentary, \"Which cricketers do you have to fire up and which cricketers do you have to calm down?\" He is someone if you fire him up, fire him up and let him go late last night. He loved that one over. That's got him right into this series. He was pumped up. He adjusted his line beautifully for Ollie Pope as well. He was just around about fifth stump and he went straighter. He was fuming with that review because when they showed it on the big screen, he thought that was hitting leg clearly. Look at his reaction there. He it you know he he is someone that you love to have in your team because when it's important when people are you know Indian fans are desperate for their cricketers to show some passion and show some fight no one does that better in this present side than Muhammad Siraj I I thought there were some key moments like you mentioned Siraj firing himself up Washington Sunda spell but for England I just can't for the life of me understand what Harry Brookke did I mean his batting so well the counterattack was so good he was batting positively root put on the other end. If he went for another paddle, I have no problems with it. But that that one crucial moment, although the game is beautifully set up, could have got England so much more and further ahead. And that for me is something that Brook has. I know it's bad ball, I know it's positive, I know it's supportive, but that is not smart. Okay. What India will have tomorrow morning is another heavy roller on the pitch. So that hour, how do they go about it? England with the ball, do you think? It's not time for emotion or it's your best two bowlers. If you thought these two were the best tonight, you go with those two again. Um the one thing about continuously and all the sides have had heavy rollers. If you keep having heavy rollers, then it does open up the cracks a little bit with the heat and everything. That's the one disadvantage as a batting side with heavy rollers. It does make the edge of the cracks start to crumble. So these 135 are going to be hard work unless Rashad Punt starts going. going to be a lot of Indian fans in. It's a sellout tomorrow. Most most the Indian fans are probably at Wimbledon today. They're going to come here. Remember last day Edgebaston and that's going to be coping. It's going to be coping with an atmosphere where every boundary may be cheered and it's going to be controlling your emotions. I cannot tell you how important a day tomorrow is. 2-1 down going to Old Trafford or 2-1 up and all the things that he talks about. If England win, the Harry Brook shot gets put to bed. If England lose, you talk about it. If India lose and you talk about the ball change when it was doing everything if England if India win it's all forgotten so tomorrow is such a vital day leading into the last two tests Mr. Pant yeah Mr. What can we expect? Simple question. What can we expect from Rishad Pant and how he bats tomorrow? He's going to come out and play shots. You can't go into shells when you're chasing this. Washington Sundar said they're going to try and win it after lunch. Immediately they would have had a quick chat in there saying tomorrow we're going to play positive. If they don't, England's going to be further in front, further on top and that's going to be a problem. Come out there bat smartly. Kale Rahul can anchor everyone else got to support him. Score runs. Score runs well. He's been there before. He's done it in Australia. He doesn't bottle it. Rashad Pant, he won't be affected by pressure. He thrives on pressure. The two at the crease, we're presuming Risha Punt will come out tomorrow. Mr. Cool, calm and calculated, who's getting runs for fun in this series and on this ground. And Mr. Chaos Rishad Punt, but in that method that you know there he knows exactly what he's doing. He handles pressure well. That is the partnership you want to break. Could be Mr. confidence that's going to hit the winning runs. Who knows? Or maybe Mr. Siraj will get involved in some way down the line. That is to come. We've had a fabulous day. I mean, that was test cricket at its absolute most dramatic. We are sold out tomorrow as well. It would be a fabulous climax to this third test match.",
        "summary": "As a technical educator, here's a detailed, structured summary of the cricket discussion, designed to highlight key concepts and strategic insights for learners:\n\n---\n\n## Cricket Match Analysis: Day 3 Wrap-up & Day 4 Outlook\n\nThis transcript provides a rich discussion on the tactical nuances, player performances, and psychological aspects of a high-stakes Test match, setting the stage for a dramatic final day. It offers valuable insights into captaincy, bowling strategies, batting approaches, and the impact of pitch conditions.\n\n---\n\n### 1. Overall Match Context & Atmosphere\n\n*   **Current State:** The match is finely poised at the end of Day 3, with India chasing a target of 135 runs. The general sentiment is that it's a \"52-48 sort of game\" in India's favour, but England holds more confidence.\n*   **Significance of Day 4:** Tomorrow is described as \"such a massive day,\" critical for both teams' series aspirations. A win or loss will dramatically shape perceptions of individual performances (e.g., Harry Brook's shot) and team strategies.\n*   **Atmosphere:** The ground's atmosphere has been \"fantastic,\" described as \"electric\" and a \"sellout\" for the final day. The crowd's energy, especially Indian fans, is expected to be a significant factor, potentially creating an \"atmosphere where every boundary may be cheered.\"\n\n---\n\n### 2. England's Performance: Tactical Masterclass & Bowling Strengths\n\nEngland's day was defined by shrewd captaincy and effective bowling, particularly in the latter stages.\n\n*   **Ben Stokes' Captaincy (Masterclass):**\n    *   **Pressure Player:** Stokes is highlighted as a \"brilliant\" captain who \"feeds off situations like that\" and thrives under pressure. He demonstrated remarkable composure and strategic acumen.\n    *   **Bowler Management:** When Jofra Archer struggled (see below), Stokes did not hesitate to take him off and bowl himself, showing decisive leadership. He communicated clearly, stating Archer was \"not bowling to the plan.\"\n    *   **Field Setting:** Stokes' refusal to \"set a field for poor bowling\" and his subsequent adjustment before Akash Deep's dismissal demonstrates a deep understanding of the game and a non-negotiable approach to execution. He was trying to bluff Akash Deep into expecting a short ball, then delivered a perfect line and length.\n*   **Jofra Archer's Struggles:**\n    *   **Rustiness:** Archer, returning to Test cricket after four years, showed signs of rust. He was \"not a machine\" and \"didn't quite get that new ball right.\"\n    *   **Line and Length Issues:** His \"pitch map\" showed he was \"all over the place,\" often \"full and floaty at 47%,\" which is the opposite of what's desired (ideally 20% full). He failed to consistently hit the crucial \"8-meter length\" that yields success.\n    *   **Blaming the Field:** Archer's tendency to \"blame something else, the field or something\" when not performing well was identified and perfectly managed by Stokes.\n*   **Mark Wood (and other pacers like Carse):**\n    *   **\"Durham Lads Delivered\":** The partnership of bowlers like Mark Wood and another pacer (likely Carse, though the transcript has \"cast\") was crucial. They \"found the length and bowled beautifully,\" bringing England back into the game.\n    *   **Key Wicket:** Wood's delivery to get Karunaya was described as a \"decent delivery nipping down,\" highlighting effective use of the pitch.\n\n---\n\n### 3. India's Batting: Challenges & Learning Points\n\nIndia's batting innings was characterized by moments of indecision and tactical misjudgment under pressure.\n\n*   **Jaiswal's Jitters:**\n    *   **Mindset Issues:** He appeared \"jittery\" and \"checked out\" even before batting. This highlights the importance of mental preparation.\n    *   **Indecision:** He was \"caught in two minds,\" letting go of balls he'd normally attack and attempting shots (dragging one wide to leg side) he \"never really tries to do,\" leading to his dismissal. Lesson: Stick to your strengths and game plan under pressure.\n*   **Karunaya's \"Brain Fog\":**\n    *   **Negative Intent:** After a positive start, Karunaya became \"negative,\" seemingly trying to \"play for time\" and \"watching the clock.\"\n    *   **Lack of Presence:** This led to indecision and a \"brain fog\" moment where he left a ball he would typically play \"99 times out of 100.\" As the commentator notes, he was \"not present\" \u2013 a critical mental error in high-pressure situations.\n*   **Shubman Gill's Dismissal:**\n    *   **Pre-Planned Strategy:** Teams were already discussing attacking Gill \"outside off stump, try and run it into the pads.\" The bowler executed this plan perfectly with a \"quicker pitch nipping\" ball.\n    *   **Emotional Factor:** England fielders (Brook, Root, Duckett) were actively \"in his ear,\" likely contributing to a \"big booming drive\" and his eventual dismissal, demonstrating the psychological aspect of Test cricket.\n*   **Akash Deep (Nightwatchman):**\n    *   **Role and Risk:** As a \"nightwatchman\" (a lower-order batsman sent in late in the day to protect a specialist batsman), his dismissal was somewhat expected against high-quality bowling from Stokes at pace, his \"off stump went cartwheeling.\"\n\n---\n\n### 4. Key Player Insights & Decision-Making\n\nBeyond the technical skills, the discussion delves into the psychological make-up and strategic contributions of individual players.\n\n*   **Mohammed Siraj (The \"Box Office\" Performer):**\n    *   **Passion & Emotion:** Described as \"box office\" and \"Kohli-esque,\" Siraj thrives on emotion and passion. He is someone you \"fire him up and let him go.\"\n    *   **Impact:** His energy and fight are contagious, making him a vital player for India, especially when fans are \"desperate for their cricketers to show some passion.\" His fuming reaction to a review shows his intensity.\n*   **Harry Brook's \"Unsmart\" Shot:**\n    *   **Momentum Shift:** Despite a strong \"counterattack,\" Brook's dismissal with a \"paddle\" shot was deemed \"not smart,\" potentially costing England \"so much more and further ahead.\"\n    *   **Lesson in Shot Selection:** This highlights that even positive, attacking cricket needs to be \"smart\" and context-aware, avoiding unnecessary risks that can shift momentum.\n*   **KL Rahul (\"Mr. Cool, Calm & Calculated\"):**\n    *   **Anchor Role:** Expected to be the \"anchor\" tomorrow, building partnerships and steadying the innings. His ability to score \"runs for fun\" in the series is vital.\n*   **Rishabh Pant (\"Mr. Chaos\" with a Method):**\n    *   **Pressure Player:** Pant \"thrives on pressure\" and \"won't be affected by pressure.\" He's known for his aggressive style and ability to play shots without \"going into shells.\"\n    *   **Strategic Intent:** Expected to \"come out there and play shots\" and \"bat smartly\" to win the game after lunch, rather than trying to defend. His past success in Australia underscores his ability to deliver under pressure.\n\n---\n\n### 5. Pitch Conditions & Day 4 Outlook\n\nThe pitch's nature will play a significant role in determining the outcome.\n\n*   **Heavy Roller Effect:** Using a \"heavy roller\" on the pitch can \"open up the cracks a little bit with the heat,\" potentially making the \"edge of the cracks start to crumble.\" This can lead to unpredictable bounce or movement, making batting harder.\n*   **Hard Work Ahead:** The target of 135 runs is predicted to be \"hard work\" unless Rishabh Pant can produce a quickfire innings.\n*   **Tomorrow's Strategy (Bowling):** England is advised to stick with their \"best two bowlers\" who performed well on Day 3, focusing on consistent lines and lengths, especially targeting the crumbling cracks.\n*   **Tomorrow's Strategy (Batting):** India's team talk will likely emphasize playing \"positive\" and \"smartly\" from the start, with Rahul anchoring and Pant leading the charge. Defensive play is seen as a route to defeat.\n\n---\n\nThis match is a microcosm of Test cricket's appeal: a blend of skill, strategy, psychology, and dramatic uncertainty, making Day 4 a must-watch for any cricket enthusiast or learner.",
        "title": "Has the third Test swung in England's favour? | Nasser & Sangakkara analyse day four \ud83d\udd0e"
    },
    {
        "videoId": "iSOfkw_YyOU",
        "timestamp": 1752494960.047153,
        "transcript": "hello everyone my name is abishek and welcome back to my channel this video aims at explaining the networking Concepts right from the fundamentals even if you don't understand what exactly is an IP address you can go ahead and watch this video this is a 3our long video I broke this video down into five chapters and using each chapter you will learn networking con Cs and towards the end of these five chapters you will Implement two networking related projects on AWS so you will not only learn the networking fundamentals you will learn Advanced networking Concepts on AWS and you will be able to implement a two architecture project as well as you will learn Advanced networking Concepts such as security groups and NSS so what are you waiting for IP address IP address is a very simple concept so IP address is used to generate or to provide a unique address to a particular device that is connected to your network don't worry I'll make it simple let's say you have a home network there is a house and within this house there is a Wi-Fi or local network that is created through a router and there are two people or two persons living in this house and each person has two devices so overall there are four devices that are connected to this local network or the Wi-Fi network now what happened was one of the these devices was used to connect to a payments website and made a payment you want to track which of these devices was connected to this payment device and you want to block access in the future probably also uh there's a kid in the house and you might want the device that is used by the kid to access Instagram or you might want one person who is using this particular device to block access to youtube.com now if each of these devices does not have a unique identification number how can you do that right it's practically not possible and you will end up doing all these devices access blocked to these websites So to avoid that each of this devices should should have a unique number and that unique identification is nothing but IP address it's not just about blocking access probably you want to track the activities of a particular device or you want to monitor activities of a particular device in all of these cases IP address comes into handy now your next question should be abishek okay I understood IP address is a unique number that is given to device let's say in my house if there are four devices each device gets a unique IP address but my next question is how does this unique address look like so you said this is a unique address but how exactly does it look like so just like human beings have names or houses have the house number which are unique right of course human beings names are not unique but if you take about houses where you have a unique identification number for each house so this particular representation is done through IP V4 for devices there is also a standard called IPv6 we'll talk about it later but for generating unique address for each device the standard that is followed is ipv4 and using ipv4 you can generate huge number of unique addresses now why is that required you might say that abishek why do I need to follow this particular practice in my house Network there are only four devices so I can simply uh say one device as 1.1 one device as 1.2 third device as 2.1 and fourth device as 2.2 now each of them got unique identification but what if there are 10 more devices that got added or instead of this house let's take example of your school or your University where there can be 10,000 or uh 20 ,000 devices that are connected to a particular number so for each of these things to maintain a proper standard see for your house if you maintain this particular standard so for your school and University you have to create a different standard so to maintain a proper standard the one that is used is IP V4 so using IP V4 what you will do is you will generate a unique identification number in the format such as 172 2.6.3 do4 or 10. 1.2.4 so these are the IP addresses that is if you are using IP V4 standard for your device you will see addresses in this particular format today you can go to your laptop and uh type the command called if config or uh ip config and you will see this kind of representations for your device now these things are called as IP addresses now let's try to understand what is the significance that you get when you assign this kind of numbers so if you watch carefully for each number there is a DOT right so here there is a DOT here there is a DOT here there is a DOT and here there is a DOT what is the advantage so the advantage that you get is each of this number can vary from 0 to 255 so that means the number number of unique identifications that you generate varies from 250 0 to 255 multiplied by 0 to 255 multiplied by 0 to 255 multiplied by 0 to 255 so this is the number of unique IP addresses that you can generate using IP V4 standard now your next question has to be abishek why 255 why can't it be 0 to th000 so with ipv4 standard what you do is of course this is a number that you see but for computer it does not understand numbers it only understands bits so each of this number is one byte or eight bits right so overall what exactly is an IP address IP address is nothing but four bytes or 32 bits and each bite right four bytes each by is separated by a DOT so how does an IP address would look like IP addresses would look like 0 to 255 0 to 255 0 to 255 got it so why 255 that is because in computer language in pv4 standard you have four bytes and one by is nothing but eight bits so four bytes is nothing but 32 bits so overall each of this number that you can see can be varied from 0 to 255 only so this is a standard you cannot change it and this standard is called as ipv4 if I repeat one more time got it so how does IP addresses look like so tomorrow if you create an E2 instance or a virtual machine in Azure what are the numbers that you would usually see you will not see something like 600. 400 do 150 do10 this will not be possible numbers that you would see can probably be 192 do 168 do 12.14 any number in all of these four places that is separated by dot should be varied from 0 to 255 only I know I'm repeating this multiple times but I want to make sure you all understand how the IP address is represented it is only represented between 0 to 255 that is because in computer language ipv4 represents IP addresses in four bytes and each of the bite right each of the four bytes is nothing but eight bits right and how are this eight bits represented if I have to write this IP address I have to write 32 hyphens for example okay this is 1 2 3 4 5 6 7 8 this is one bit 1 2 3 4 5 6 7 8 this is another bit 1 2 3 4 5 6 7 8 this is another bit 1 2 3 4 5 6 7 8 this is another bit so this is bite one bite two by three and bite four so this is how IP addresses are represented now when I say 192 what happens is how does computer understand this 192 computer understands this 192 as 1 1 0 0 0 0 0 now how did I write this particular thing that is very simple for each bite sorry each bit that you see here you start with zero you call it as two 2^ 0 this is 2^ 1 this is 2^ 2 this is 2^ 3 2 power 4 2 power 5 2^ 6 and 2 power 7 because the first number is 192 2^ 7 + 2^ 6 that is 128 + 64 which comes to 192 this is how computer understands your IP address the reason why maximum number that you can see here right if you want to see here even if you put one one 1 1 1 1 1 computer understands this as 2^ 0 + 2^ 1 2^ 2 2^ 3 4 5 6 and 7 which combinely comes to 255 that's why maximum number in an IP address can be 255 only if you see something like this 192 do 600 do 12. uh 254 this is not an IP address because 600 is not possible now I hope it is very clear how IP address is represented in the IP V4 format now if you want to try out few IP addresses you can give it a try like your assignment can be convert this particular IP address that is 172 probably uh dot 32 dot 16 Dot fun convert this IP address into the format that I have explained that is using the bits format how does computer understand this particular IP address so to to do this what you'll simply do you will convert each bite into eight hyphens you will write two hyphens and under two under this eight hyphens you can write 2 power 0 2 power 1 2^ 2 2^ 3 and convert this particular thing let's say if I have to write this by default I know 32 is just 2 power 5 so this is first decimal second decimal third fourth fifth sixth 7th right if I have something like this in the fifth place I'll just put one and in all of the places I'll put zero right so this is how IP address is converted in the decimal sorry hexad decimal or octed format it's octed format it's not hexa so this is how number is converted into octed format and how computer understands the IP address perfect now you will understand later point of time why I'm stressing so much on this particular representation when we try to understand subnets and cadr this comes very very handy so I hope IP address part is clear now let's move to the next concept that is subnet abishek IP address is very clear where let's say if I have a home Network or a school network and within the school network what I'll simply do is I will say I just need some whenever I'm creating uh this particular Network let's say I'm creating a VPC on AWS or I'm creating something on open stack okay so I'm requesting the network provider let's say some 65,000 IP addresses and I got 65,000 IP addresses I know the maximum people that would connect to my network if it is a school network or office Network maximum people that would convert connect to it is only 40K so I have just taken 25k IP addresses as a space so I know okay this amount cannot be exceeded now I'll create a school Wi-Fi or office Wi-Fi right let's call office Wi-Fi and we will call this uh office Wi-Fi as free Wi-Fi because anyone can connect to it so any new employee that is joining this company has started accessing this network right this person this person this person everyone has started accessing this free network now what happened one day was there is this employee who accessed a malicious website okay so this person has accessed a malicious website that was written by a hacker and because this person has accessed it hacker got access into this particular device and now if you understand carefully all of these devices are connected to the same network that means hacker got access to one device but because they are connected to the same network hacker can access all the devices right so technically all the devices are hacked now if this is your office Network there can be some sensitive files or there can be payroll of the employees or there can be some financial data of the company that are stored in some servers such as your bank details or any any such thing so hacker got access to everything so your complete company Network got hacked now this should not happen right of course this should never happen so there is a concept in networking which is called as subnet what exactly is this concept of of subnet very simple subnet is basically if you look at the name itself it says sub networking where if you take the same example you can create a VPC in AWS or you can create a new network on your open stack with 65,000 IP addresses and what you will simply do is you will say out of this 65 IP addresses okay or out of this complete Network I will split it into two parts okay and one network I will use only for finance related things all the sensitive information of the company I will call this network as secure network like sometimes if you go uh to a uh to your company or if you go to your school network you will see some secure networks which you don't have access to and then you'll have some free network or this network can have a password but this will be shared if you are an employee you just go to your admin and you ask for a password and they will share this password with you so I'll call it as a free network and so I will try to split this network into two parts and I will say Network one should be strictly used by Finance team and network 2 can be used by anyone now even if the hacker one of these people tries to access a malicious website and hacker gets access to it all the devices in this particular Network are compromised but still all of these are sensitive this is important for the company this concept is called as subnetting and this particular network is called as subnet and this particular network is called as subnet why are they called as subnets because they are part of a bigger Network and you have tried to split it so that's why you call it as subnet right understood the advantage of creating a sub subnet the advantage of creating a subnet is you get security you have the privacy and you have a proper isolation that's why subnets are used today you can go back and you can create Subnet in your home Wi-Fi network as well right you can say only the uh heavy usage appliances should use a particular subnet and other appliances should use a different subnet that's totally possible you can create subnets in any kind of network right so this will give security privacy as well as isolation now I'll come back to the concept of subnet uh addresses like uh how subnet one gets IP addresses and how subnet 2 gets IP addresses but before that just understand that there are two types of subnets one is private subnet two is public subnet if you ever used the cloud provider you might have uh heard about this terms called private and public subnet but what exactly are these so private subnet is some Network that does not have access to Internet and public subnet is a subnet that has access to Internet that's the only difference it's a very simple difference right so private subnet has access to to uh does not have access to Internet and public subnet has access to Internet how do you do that how can you enable access to Internet don't worry it's a very simple concept uh you know if you already know about Cloud providers like AWS or Azure all that you need to do is you can go to this particular subnets and you can attach route tables to this particular subnet and destination of a particular route of this subnet you can provide as an internet gateway and that will grant access to the internet even if you don't know this particular concept you can ignore FNA because right now we are learning the fundamentals of networking so just understand access to Internet public subnet no access to Internet private subnet now let's go back to the concept that I was explaining right abishek you mentioned in the last slide in a very simple way that you have a office Network right and in the office Network you said you will simply split finance and free subnet where everyone can join you mentioned it as very simple way where there are 65 IP addresses but how do you actually know how many IP addresses this Finance uh subnet gets and how many IP addresses that this particular subnet net gets right overall when you created this VPC in AWS you have requested for 655,000 IP addresses what is VPC VPC is just nothing but a private Network so you have requested for a private network from AWS and you said you need 65,000 IP addresses now you are saying you will create two subnets in the 65,000 IP addresses one is finance and two is a free subnet everyone can access but how will you say that this Finance subnet let's say it only needs 2506 IP addresses and this need rest all IP addresses how will you divide that it's very simple again wherever you are creating this subnet right so if you're creating on private Cloud you have platforms like open stack if you're creating on public Cloud providers like AWS or Azure in any of the cases when ever you are creating this subnet you will be asked to provide a cidr range this is also called as cider so this cider or cidr is a way of explaining how many IP addresses are available in a particular subnet right here I want to say my finance particular thing just needs 256 IP addresses and rest all can be given to this free subnet where anyone will access this particular Network now how will you do cidr is the solution how does cidr work right so concept of cidr is very very simple don't worry I'll explain it in a very simple way people usually get uh find it difficult but the calculation is very simple so what you will do when you create this particular subnet let's use AWS as an example so you will go to AWS and you will say I want to create a subnet of course you need to go to the VPC first right this is the VPC that you have initially created you'll go to this VPC inside the VPC you'll go to the subnet and within the subnet what you will do I mean you are creating the subnet when you are creating you will be asked for a cidr and when you want to create this cadr The Simple Solution is you say 17216 do 3.0 SL 24 so if you require 256 IP addresses you will provide Hyun 24 now how did I do this right this should be your million dollar question that is abishek how did you understand if I need 256 IP addresses I need to provide 176 do uh 16.30 sl24 or 192 16. 4.0 sl24 whatever is the IP address of the VPC followed by sl24 how did you understand this it is very simple again like I mentioned you pick up any IP address in the VPC range okay let's say while creating VPC you have provided the range as 1726 do 0.0 pick up any IP address uh I mean you said the VPC IP address name should be 172.16.0.0 to 17216 do 255.255 that is you have 65,000 IP addresses now pick up any IP address so let's pick up 172 dot 16 do 3.4 let's take this as an example and as I've told you represent this in the uh octet format that is 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 and finally 1 2 3 4 5 6 7 8 right so I'm representing this particular IP address you can pick up any IP address I'm representing this particular IP address in the ipv4 by standard where for each bite I have written the bits so overall if you combine here there are eight bits here there are eight bits here there are eight bits and here there are eight bits so if you combine all of these things what is the output 32 bits right and as I have mentioned you each of this particular thing can represent only between 0 to 255 and I have mentioned I just need 256 IP addresses for the finance domain so only this last thing is enough right so if I just provide 17216 3.0 to 17263 255 I got my required 256 IP address addresses so I don't need this eight this eight this eight that means this eight can be common this eight can be common this eight can be common only this thing will vary so what was my VPC IP address bpc IP address was 172.16.0.0 to 255.255 right so all these three things can be common for my device so this can be the host Network or the common things and this has to vary so that's why I'm providing 17216 do3 do0 sl24 why 24 because I've cancelled 24 that is all this 24 bits can remain static can remain same and only this eight bits have to vary so that's why I said 0o followed by 24 don't worry I'll try to explain it one more time if you did not understand so what was my VPC IP address or the primary uh uh Network address 1721 16. 0.0 to 17216 do 255.255 this was my IP address range right now if I just need 256 IP addresses I can pick up something like 17216 3.0 3.1 3.2 3.4 to 3255 right what are the number of IP addresses here 256 IP addresses right so this solves my problem either I can pick 17216 3.0 to 3255 or 17216 4.0 to 4255 5.0 to 5255 anything solves my problem so what I can simply do is if I need 256 IP addresses from this particular VPC range to provide to the finance Network thing either I can use 17216 Dot 3.24 4.24 6.24 or even 2440 sl24 right I can provide any of these things as the cidr range for my Subnet now let's make it even more simple let's say right this is my office Network and here here same I want a finance domain uh sorry Finance subnet and this is my free subnet and in the finance subnet I just want two addresses nothing more than that I just want two addresses and here right here I want to provide rest all addresses for the devices that are getting connected so if I just need two addresses what I can simply do is I can provide one 72.6 do 3.0 /31 why 31 so if you remember the uh hyphens thing there are 32 hyphens right out of which I have cancelled 31 hyphens so there is only one hyphen left and one value of a bit is nothing but it can be either one or it can be zero so there are two IP addresses that are generated when you are calculating the cidr thing right you should not calculate from 2^ 0 so if you have 32 and out of which 31 are stried out so there is only one that is left so always try to calculate 32 minus the number that is provided here if 31 is provided 32 minus 31 and 2 power the number that is left so here 2^ 1 is left so the answer is two so similarly if we are taking 24 here 30 2 - 24 what is left 8 what is 2 8 256 IP addresses right so if you just want uh let's say 32 IP addresses what you will do 32 minus 27 is 5 and 2^ 5 is 32 so I can simply provide here 27 so depending on this particular calculation you will take the cidr range so if there is a IP address like this right so if there is some IP address like 10 do sorry subnet with cidr range like 10.0.0.0 sl8 what does that mean 32 - 8 which is 24 so the number of IP addresses is 2 power 24 which is almost like I think a very huge number right 256 into 256 into 256 so that's a very huge number 2^ 24 so whenever you are seeing something with sl8 which is nothing but it is also called as class a IP addresses and if you are seeing something with sl6 which are called as Class B and you see sl24 which is called slash C here you just have 255 or sorry 256 IP addresses here you have 256 into 256 and here you have 256 into 256 into 256 this is just a common representation don't worry most of the times you will see sl8 sl6 and sl24 but along with that you might see sl27 you might see sl28 you might see /29 as well whenever you are seeing something like this don't get panicked just simple thing that has to come to your mind is 32 minus 27 which is five so number of IP addresses is 32 when you see 29 32 - 2 9 which is three so number of IP addresses 2^ 3 which is 8 very simple calculation and straightforward right so this is the thing that has to be in your back of mind so next time when you create a VPC observe these things carefully or you are on open stack you are on Azure anywhere whenever you are creating a new network okay it will immediately ask you for the range of that particular Network so let's say when I'm creating VPC it will initially ask me for a range and range I can provide like I told you either 10 172 198 anything so I'll just say 19260 do0 sl16 that is I will have 65,000 IP address and now when I create subnets inside this VPC depending on the requirement from the development team or depending on the requirement from the team if they are asking 32 IP addresses only what I'll simply do I'll say 19216 do3 do3 or four anything here 3.0 sl27 this is what I'll give you and why I'm doing it very quickly because I'm very used to this particular calculation so you can take few calculations as example in Internet you just search for cidr calculators you will get hundreds of it and verify if your calculation is right so tomorrow if someone is asking me that abishek this is my VPC and I want to create a new VPC uh sorry subnet with 64 IP addresses I just want subnet with 64 IP addresses immediately what I'll do is I'll just okay look at this IP address randomly I'll say 19216 whatever is my uh favorite number so I'll just say 5.0 SL okay 64 this is 2^ 6 so 32 minus 28 sorry 26 is six so 2^ 6 is 64 so I'll just say SL 26 right now why I'm only modifying this particular number because they are asking below 256 So Below 266 I'll just modify this particular thing this will be staed I'll just put this as zero if they're asking more than 256 let's say someone is asking me 65,000 IP addresses so what I'll do 19216 do0 /6 this is the cadr range that I would provide I hope these examples made the concept of cadr clear to you and what you will do is go to the cidr calculator on internet and take random numbers like this and try to estimate what will be the cidr number for it or what will be the number of IP addresses for it I can give you some assignments what is the number of IP addresses for this particular C IDR range okay let me write in a new one what are the number of IP addresses for this particular cidr that you see 172 do 168 dot 3.0 sl30 and what is the cadr for 10.0.0 sl8 so in the comment section you can mention what will be the number of IP addresses for both of this cidr range let me see how many of you can get this right and before we move to the next topic there is one more thing that you have to remember most of the times when we talk about private subnets you will see this numbers only 192 172 10 right why you would uh see mostly this numbers is because these are the ones that are used for private subnets okay so this just like a practice so even with Amazon or wherever you are creating you would see these numbers as the starting ones when you create private subnets because these are the numbers that are used for private subnets right so when you are creating a private subnet your cidr range cannot be 3.4. 5.6 or 5.0 SL 24 this cannot be your thing because probably this public one of these public IP addresses has been taken by some particular website for example 8.8.8.8 is a public IP address that is taken by google.com this is the DNS for the google.com so if you keep this for your application there will be a conflict so anytime private IP addresses should be starting with this only right so I hope this concept is clear so what did we cover till now till now we covered the concept of IP address we covered the concept of uh cidr we covered the concept of subnet within subnet we try to cover what is private subnet we try to cover what is public subnet as well these are the things that we have covered till now and the next concept is about ports ports is again a very very simple concept you don't have to get confused with it so whenever we provide IP addresses to connect to websites we provide some ports right uh colon 80 colon 443 or we say uh colon 36 or colon 808 what exactly are these things so basically to bind an application so let's take a virtual machine okay on this virtual machine I have deployed an application okay now on this virtual machine there can be number of applications okay so what you you can do is for application you can provide a unique Port again ports are also in a particular range only you cannot create port with number one lakh right so and there are some ports that are taken also right for example um there is a specific for port for MySQL better not to take that particular port or uh there can be a uh port for Jenkins usually Jenkins starts on 8080 so better not to take that particular port so whenever you are starting your application start with unique ports such as 9,000 right or you can take 9191 these kind of ports right for example better not to take ports like 3306 which are taken by other kind of applications so these things comes with practice and uh you know what if you go back to the topic that we are discussing whenever you create application on a virtual machine to ask access this application from the internet you need two things one is the IP address of this virtual machine of course if it's a public IP address only you can access it from the internet if it's a private IP address you can access only within your network let's say this is a public IP address so to access this particular application if this application has Port 9191 so you will say 172 do 16 sorry public IP address let's say 3. 4. 5. 8 colum 991 91 now if this application has Port uh 5162 so you will access using 5162 so what is a Port Port is a unique number for your application in an instance there can be hundreds of application and using Port you can distinguish the request has to go to which particular application let's quickly recap what we have learned as part of episode one in episode one we learned about IP address what exactly is a subnet what are the different types of subnets we learned about cidr how to read write and perform calculations on cidr blocks if you are not aware of this networking Basics I'll highly recommend you to watch episode one and the link to episode one is in the description now let move towards the Whiteboard and start understanding the concept for today that is OSI model we all interact with servers on a day-to-day basis let's take a simple example let's say I open my laptop open one of my favorite browsers and search for google.com within a fraction of seconds I receive a response back which is nothing but Google homepage a HTML page basically but how does this thing happen how is my request from my personal laptop sent over the internet to one of these Google servers and how is the response received back to my personal laptop what are the different components that are involved and what are the different layers in which the Journey of data takes place I'm calling it as journey of data because whatever you are trying to request and whatever you are trying to send is all data so OSI model is one of the popular models that helps you understand the Journey of data across the internet what are the different layers that are involved and OSI model explains this entire thing in seven layers layer 7 to layer 1 sometimes it can be layer 1 to layer seven if you are at the receiving end of the data but what exactly each of these layers do what happens to my data in each of these layers when I say layer seven layer 6 layer five layer 4 layer 3 2 and 1 what what each of these layers actually do how my data gets transformed in each of these layers and finally how does it reach one of these Google servers let's try to understand the same thing in today's video we will use the same example that is request to google.com and let's try to understand the complete workflow here before we understand the Journey of data that is even before your browser initiates a request to the Google server there are two things that happens that is even before the OSI model comes into picture they are number one is DNS resolution it is also good to understand these Concepts before you understand the OSI model and what each and every layer does because this is also part of the entire workflow that you are trying to learn the networking Concepts DNS resolution and the second thing that happens is the TCP handshake now why these things happens even before your request is initiated right even before the request starts or even before the request is sent why these things happens because let's say you are searching for or you are trying to make a request to https www.google.com first of all what your r router that is your home router or your browser tries to do is they will try to verify if www.google.com is mapped to any particular IP address so there is a system called DNS which is nothing but domain naming service you can understand it as a simple database right just try to understand it as simple database where records are maintained right if I have to explain this in a very simple way just try to understand that every router has this information which is records of domain name mapping with IP address right so when you are saying www.google.com in this domain naming service there is an IP address against the google.com domain name now when you search for this www.google.com router verifies this initially in the local cache right just like your laptop has a local cache or you know you have uh the local memory similarly your router initially searches this for in the uh local cache where previously if your router has made a request to this particular website it might have maintained it record its records in the local cache if this information is not available in the local gache then it goes to your internet service provider and verifies this particular mapping if there is any mapping for the domain name in your internet service providers DNS every internet service provider maintains a DNS where the complete records are available right so google.com is usually mapped to the IP address 8.8.8.8 so if this mapping happens if this domain that you're trying to reach is actually valid only then you will move to the second stage why this needs to be performed because let's say I'm trying to access something like the www. abishek vera.com now this is something that does not even exist I did not create this domain I did not map it to particular IP address now what's the point of you know starting with this entire data Journey sometimes you might be uploading some 10gb file or you might be sending a huge amount of data now if the DNS itself is not resolved what's the point of even starting the dat Journey or what's the point of even initiating the request why should your browser or router initiate a request right so this is the first step that happens DNS resolution if the DNS resolution happens then comes your second stage which is nothing but TCP handshake now what is the TCP handshake again it's a very simple concept let's say this is your laptop and this is the Google server now you are trying to send a request to it you are ready to send a request but is the server that you're trying to send is ready to accept your request what if it denies your request even after sending this entire thing right what if you cannot make a handshake with it hand check is nothing but you are just trying to say hi and it says hi I'm okay to accept your request so there is a TCP handshake which is usually called as a three-way handshake that gets performed even before your request initiation or even before your OSI model comes into picture now how does this three-way handshake perform So when you say www.google.com what your router or browser what they try to do is they initially send a high to the server like let's say this is your laptop and this is your server so initially router tries to send a high in networking terminology we call it as sync and if this server is ready to accept if it says that okay I'm good then it says hi which is in the networking terminology sync acknowledged and finally your laptop says acknowledged so so this happens in three steps that's why we call it as a three-way handshake now you might ask me but abishek why can't it be very simple as you say sync and it says sync techn knowled and it's done so it's a very uh detailed explanation let's not go into it but if you are interested there is also something called two-way handshake and there's also something called four-way handshake so if you are interested you can go through these things you can read about difference between two way 3way and four-way handshake 3-way is the most popular and which is mostly used so that's why uh I'm talking about the three-way handshake but you can also read about these things so these are the two you can consider as prerequisites even before your data uh request initiation takes place one is DNS resolution two is the TCP handshake now that you understand both of these Concepts that is if DNS resolu ution is done and TCP hand check is also done then your data initiation or your data request initiation starts now let's try to understand that in terms of the OSI model now I hope you understood why did I explain this even before the OSI model because when you are learning networking it's important to understand the end to endend part of it now to start with OSI model let's take same example when you search https www.g google.com and I'm assuming DNS resolution is successful and TCP handshake is successful so what happens after that is your are browser right understand this carefully your browser initiates a HTTP or https request to the server that is this one right so you are searching for google.com in the browser so your browser is initiating a request now it did not send the request but the initial process has started now when the initial process has started it says use HTTP based request why because you have asked for it let's say you're asking for FTP then your browser initiates a FTP based request right so this particular stage is called as layer 7even which is the initial stage and also called AS application layer in this particular layer you can pass some headers right and you can also provide information for the authentication or whatever is required but this happens in the layer 7 then comes like once you or browser has initiated the HTTP request right if I go back to this particular thing once the request is initiated now what should be the next step if you think about it the next step should be data encryption right because no one in the entire process like from your laptop to the server data goes through multiple routers I'll talk about it what are these routers but basically data first goes to your home router then it goes to your internet service Provider from them it goes to a different router different router and finally it goes to the Google server so if your data is encrypted then even if someone tries to hack your data they don't understand what exactly it is and that's where we use https right so when you use https The Next Step that has to happen after the HTTP request initiation is data encryption which is also called as data formatting and this layer in OSI model is called as layer six which is also called as presentation layer now all of these layers are virtual just for you to understand so Osa model is trying to just explain you that okay this is the first step that happens this is the second step that happens and once HTTP request is made data is encrypted what should be the next step The Next Step should be your browser should create a session what exactly this is very simple let's say today you can go to facebook.com and probably search for facebook.com/ abishek vamala 20 minutes later take a different tab or probably take a different window as well and search for facebook.com/ Raju or John your browser will not ask you to authenticate one more time initially when you search for facebook.com/ abisheka it will ask you to log into Facebook but even after 20 minutes if you search for slj orxy Z facebook.com does not ask you to authenticate because there is a session that is maintained and this session is very very important because sometimes let's take example of your banking transaction okay so initially you log to your bank and probably you try to send someone uh some amount or you know you try to make a transaction what if your session gets disconnected in one minute you just have one minute you complete your transaction or you make a transaction and you want to make one more transaction right unless you log out your bank server should not ask you to log in one more time and that happens only if your browser creates a session we all use sessions on a day-to-day basis whether you're using Instagram Facebook or anything there are session that are maintained so once the HTTP request is initiated second thing that happens is encryption which is nothing but data formatting in the presentation layer and then a session is created with you and the server so so that the server does not ask you to authenticate multiple times and this particular thing is called as session layer and one interesting thing about all these three layers right layer seven layer six and layer five is that all of these three layers are maintained by your browser right so all these three layers happen at your browser level what does that mean let's say you're using Chrome or you're using Firefox or you're using any particular browser layer 7 layer six and layer five are taken care by those browsers itself right your browser initiates a request then your browser depending upon https and what kind of certificates that you're providing it takes care of encryption and your browser only takes care of session if you want an example just go to your browser go to your browser settings and try to clear cash as well as cookies and then try to authenticate with the facebook.com let's say you authenticated facebook.com 2 minutes later if you delete the cookies and cash it will ask you to authenticate one more time because you have deleted the session session is basically stored in cookies and cash right so layer 7 six and five are taken care by your browser itself we did not even come to the router part if this is my laptop and this is my server laptop is connected to my home router till now whatever I discussed happening is happening in the browser itself my request even did not reach this particular browser if I'm talking about only layer 6 5 and four now the next thing that happens is once the session is also created to transmit the data right if we are taking this example some cases the data that can transmitting can be of 10 GB also right probably you are trying to upload a movie or you are trying to do something so if if you are trying to do data in one at a time right if 10 GB if you're trying to upload at once or you know even simple things such as requesting to google.com what usually happens is your data is segmented right that is the data that you are trying to send or the data that you are trying to receive is segmented and split into parts so this particular particular thing is called as segmentation and this happens in layer four along with the segmentation in this particular layer the protocol is also defined whether you want to use TCP or UDP right so there are only two protocols which are like these are the widely used TCP as well as UDP and in layer four once all of these things happen data segmentation takes place and the particular protocol is also identified in this layer how is the protocol identified mostly these are standardized let's say if you're using HTTP the protocol is TCP if you're using something like DNS or something else the protocol is UDP so these Protocols are standardized and whether I mean if you are using HTTP PCP is the protocol that is used to transmit the data from here on that is the segments of data that got split are transmitted using the TCP protocol and this layer is called as transport layer now once the data segmentation is also done now it's time to send the data and to send the data the first thing that happens is the data that send is received by your router and what your router does is router does two particular things one is to the segmented data let's say say you want to travel from Delhi to Mumbai here you know what is your destination right you know that you want to travel from Delhi to Mumbai and second thing is what is the shortest path probably you can travel from T to Mumbai in 20 different ways but you will only pick up the shortest path similarly even in this example data from your personal laptop to google.com that is from India to the US there will be multiple hops for your data that is there are multiple routers that are involved probably your home router then your internet service provider from your internet service provider to etc etc XYZ and finally it reaches the Google server so which routers or which hops should your data take to reach the Google server in the fastest way to understand that what we will do in the layer three is that we will add Source IP address as well as destination IP address to each segment right and here once you add the source IP address and the destination IP address we call this data as packets right so packets have a clear information of how to travel which path to take and who takes this decision this decision is taken by your router and this layer is called as networking layer just try to take this example Delhi to Mumbai if you have to travel you need to know what is your source and what is your destination if you want to send 100 people from Delhi to Mumbai to each and every person you will tell them that okay this is the path that you have to take similarly in layer three your router comes into picture and your router tells each and every packet that okay this is the source this is the destination IP address and you need to travel through these hops right and this layer is called as Network layer then comes your Layer Two and in Layer Two like okay now your router received the information but if you understand it carefully usually these routers are connected to switches right even if you look at your uh home network your router is connected to a ethernet port and from the ethernet port your router is connected to the cables right so if you're talking about this e ports or if you're talking about this switches right now data has to be sent some or the other time to these ethernet uh ports or to the switches right so at this point of time when the data has to be sent to the ethernet uh ports the data has to be transformed from the packets to frames depending upon the medium that you're using if you're using router the data is converted into packets then if you're using these switches the data is converted into frames and in this frames along with this IP address that you have provided Mac information is also added which is nothing but Mac information tells these switches what are the other components within your network right so here Mac components are also added now you might be thinking but why can't I use packets because your medium has changed from router this request is being sent to switches and switches only understands how data can be transmitted in the frames so this layer is called as data link layer finally you have layer one that is your data end of the day or your router switches end of the day are connected to optical cables and guess what language this optical cables understand the language these optical cable understands is electronic signals right so here your data is transmitted into electronic signals and using optical cables data is transmitted very fast right and this is how your data is transform right if if I have to explain this in one single slide one more time you have layer seven layer 6 layer five layer 4 3 2 and 1 so if you take take your personal laptop and this is your Google server for example now when you initiate a request to www.google.com so initially what your browser does starts with layer 7even I mean the process starts with layer 7 where first of all it defines what is the type of request so mostly we use HTTP type request so HTTP or https now once it identifies that okay I have to initiate a HTTP based request second thing that it does in the layer six is encrypt the data once the data is also encrypted the next thing that happens is create a session with the server now once the session is created with the server because data has to be transmitted within fraction of seconds segmentation of data is important and that happens in layer four and here along with the segmentation the protocol is also identified depending upon the request that you are making if it is HTTP TCP request is most widely used once the segmentation is also done right now the data is divided into small parts after that data reaches your routers and within the routers the path to transmit the data is identified because in this layer we add the IP address for each and every segment and here we call them as packets once packets are also created then the next thing is from the routers you will move towards the switches and here data is converted into frames where you add the MAC address of each and every component that is there so that you know like you add the MAC address so that within the network this switch can understand what are the other components to transmit the data and finally layer one is a physical layer where these switches are connected to optical cables and here the data is converted to electric signals or electronic whatever you would like to call right so when you initiated the request from layer seven layer 6 5 4 3 2 1 what happened from your laptop it went to your router from router it went to switch and it connected to the cable now it goes through multiple hops over the internet and finally it will reach one of these Google servers right from layer one which is let's say this is one of the cables then it is connected to other routers other switch and again cable and this entire thing happens and the data is transmitted and finally it reaches one of these Google servers Now understand carefully what happens here when this data received by one of these Google servers again the OSI model comes into picture where initially data is received by one of these Google physical servers so one of these Google physical servers is connected to a optical cable so L1 from there it will go to one of the switch boards then it identifies which router to use from there the data which you have been using as uh packets like whether it's a TCP or UDP right let's say we are using the TCP protocol so here the TCP data is taken into place from there once you have the TCP data here then session is validated after that de encryption takes place from there it will go to one of the Google's applications and this Google application will say hey okay this is your request so let me give you a HTML page as a response P here this application is a microservice or any particular monolith application which has the source code and it understands depending upon your request okay I want to generate a HTML p now again this HTML page is sent back to your personal laptop and same thing happens right before it reaches your personal laptop your router is connected to a physical cable so it has to move from L1 L2 L3 L4 5 6 and finally reach your particular laptop so this is the entire OSI model and you know basically when you are sending the request L7 to L1 data is transmitted right data is transmitted through L7 to L1 layers now when you are at the receiving end data is received from layer one to layer seven so these are all the these are not the physical layers right these are just a way of explanation of OSI model for you to understand the data transmission in a better way OSI model explains it in this particular P of course Osa model is not the latest model there are models like TCP IP model right which is again based on OSI model but the thing is in TCP IP model L7 L6 L5 is combined because you know more or less these are our performed by the same component that is your browser so L7 L6 and L5 are combined and called as a single layer in the TCP IP model so why people usually prefer OSI model because this is a standard and if you understand this standard you typically understand the entire transmission of data whether it's TCP IP model or any new model they are all based on the OSI model right so don't get confused if you see TCP IP model it's exactly the same only difference is L7 L6 and L5 are combined and usually they are called as one single layer in the tcpip moduel right so this is the video for today and how this I model is helpful for the devops engineers I will tell you that it is not completely must have thing right this knowledge is kind of a good to have thing I'll tell you the reason why because these days all of these things is standardized and this entire thing is completely automated like you won't see anyone using wi Shack these days unless you are working in the core networking company and probably if you're dealing with the layer three layer two or you know if you are involved in any sort of uh networking companies only then you see people using these kind of things right the wire Shar or any tools to decrypt the packets understand these things but if you have a high level knowledge of OSI model that is more than enough you don't have to dig dive and try to understand what are the different protocols in layer three right you don't have to understand what are the different types of uh techniques that are available in layer 2 because there are this is a ocean and whatever I taught in this particular video it's more than enough if you are trying to understand networking as a devops engineer so today's class let's learn what is virtual private Cloud let's understand why you need virtual private Cloud what are the different components of virtual private cloud and also how do they interact with each other and finally there is a fear amongst a lot of people that virtual private cloud is a difficult topic you might say that yes abishek we have that concern because VPC involves a lot of networking there are multiple components and it's difficult to understand why and how each of these component is interacting what purpose are they solving so don't worry if you have any such fear or even if you are completely new to networking and VPC after today's video you along with me will say that virtual private cloud is an easy concept and I understand this concept end to end so we will do practicals as well but practical will not be of today's video because it is very very important to have strong foundational knowledge on VPC so today's video I will try to add that foundational knowledge on VPC for you and in tomorrow's video we will Deep dive into slightly more complicated topic of uh VPC that is related to security and in day after video we will try to do a practical video deploy application inside VPC and explore the complete flow of VPC so that will be the entire thing but how will I make this concept easy for you you might ask me what I'll do is I will take a real life scenario okay so we will take a very much real life scenario that um all of us understand and we will use the same real life scenario to understand the concept of VPC so this real life scenario has nothing to do with AWS but still I will correlate and make this concept very very easy for you in the lines of AWS so let's quickly start the video and understand virtual private Cloud so to start with the example the scenario that I was talking about let's say that there is a huge land okay so let's call this as a village okay so we have a village and there are lot of people in this Village but there are specific set of lazy people in this Village okay so these lazy people are lazy enough that they can't construct their own houses why they can't construct their own houses because they don't want to maintain their houses they don't want to manage their houses and also so they don't want to go through all the construction process and everything related to their houses so these people they are actually looking for some people and at the same time there is one wise person in this Village okay let's say that this wise person is called ABC so there is a wise person called ABC in the village what he has done is he saw this opportunity of lazy people and he said that okay let me grab this opportunity and make money out of it so this wise person ABC what he has done is he went to this Village and he acquired a huge land in this Village okay so he acquired this entire land in this Village and he said that okay hey lazy people what I'm going to do is I'm going to construct the houses for you but for that you have to send me a request with your entire requirements like what are the resources that you require in this house what is the capacity uh that you require in this house and fill me all the details and give it to me and in return I'll give you house and I'll take money from you right so this person has started the process and what this uh ABC or the vice person has done is he has constructed this house in the land that he has acquired so he purchased this entire land and he uh constructed a house and he said that I'll maintain this house for you so this person felt happy for it and then seeing this other person also approach and he has constructed house for the other person as well so in future there is one more person and he has constructed house for that one more person so it is going well and all of these people their problem is solved that they don't want to maintain the entire construction and maintenance this person the vice person ABC is taking care of it and they are all happy but there are other bunch of people what they have done is they saw this they were happy about it but they realized that these bunch of people they have realized that there is some kind of security issue here there is some kind of privacy Beach that means for the purpose of uh saving a lot of money money what this person has done is he has constructed the houses but they are very nearby and there is some kind of security bridge that anyone who access this house can has can hack hack or access the other house and can also access the other house so that way there is some kind of security bridge that is happening here so these people said that okay we are also looking for houses from you but we want it in a much better way not in this way where if one of these houses compromised that the other houses are also compromised so this person to not to lose the business and you know uh because he want to construct more and more uh these things what he said is okay give me some time and he came back with A New Concept called secure land okay so what he said is okay I have this entire property but if you come as a group where if you want uh a group housing or something what I can do is I can build a secure property for you people so inside this entire land what this person ABC has done is he has buil a secure property for these people and this security uh secure property what he said is only people who have access to the secure property like he said that I'll construct a gate here which will act as a Gateway right so this gate will act as a Gateway and only this people or the people's relatives or people who know these people only can enter this secure property right so what happens is let's say someone wants to meet this person okay and this person has one property here the other person this person has one property here and this person has a property here right so now there is one of the relatives of this person right let's call this person as a now the relative of a wants to meet this person so what the secure property does is this person can definitely meet him but he first has to pass through this gate so there can be a security guard or someone who will guard this property and once he enters this so this is a property that all of these people own right so the secure property this is all the public thing where everybody owns this but if this person this XY Z person let's say he wants to go and meet this person called a now he need to understand how to go to this property right now because I'm drawing this picture you know how to go from here to here but eventually to go from here to here there has to be another guide here so this guide basically explains okay uh move from here to here and finally you'll reach the person a again once you reach this person a there will be another security guard or there will be some kind of person here who will validate okay is this the right person you entered the Gateway you moved into the secure property but again do you want to meet a do you want to meet B do you want to meet C so if you are a valid person who wants to meet a then only I will accept your request to meet the person a so this way what this ABC the wise person has done is he has solved the problem of security breach and solve the problem of security and now everybody is enjoying the secure property looking at these people there are another bunch of people who came to him and he has built another secure property another secure property another secure property this way he started making his business now why did I explain this story to you people you might be thinking has abishek gone mad why is he saying all of these things and how is how is it going to be related to AWS okay now let's try to relate the same thing to AWS in in the last class I showed you how to create an easy2 instance how to deploy an application and how to access that application right so now let's go to the same scenario where there is this big region okay so let's call this region as Mumbai or Ohio or whatever you would like to what happened was there are people okay so there are companies let's say this company as example.com this company as example 1.com then there is another company called example 2.com so these people initially they were maintaining their own data centers but it was becoming too hectic for them so they said that okay uh if there is someone who is going to help us with this data center we will offboard that to them or there is a new startup now they said that okay we can't affort to build a new data center we can't affort to maintain it now we are also looking for someone so AWS saw this opportunity right so in this case who is the vice person AWS is the vice person what aw said is hey okay what I can do is inside this region let's say there is region called Mumbai or Ohio or throughout the world what AWS did is AWS started building their own data centers in the previous example that person has acquired a lot of land right the vice person here what AWS did is AWS started purchasing or building their own data center so AWS built this entire data center in Mumbai region for example or Ohio region or Frank uh Frankfurt anywhere AWS started building their own data centers now what AWS said is hey lazy companies or hey startup hey example one example two example three so we can host your applications inside our data centers or you can request us saying that hey uh I want one virtual machine I want 10 virtual machines I want 20 virtual machines and we will take your request you give me money and I will create the instances for you so this was going by uh I mean this exact same thing what we did uh previously and this was going very fine where example.com 1 they requested and what AWS did so this is the story of 2013 2014 okay so during this time what AWS did is if example.com is requesting it uh 10 ec2 instances so it went to the data center in region Mumbai or whichever region it requested and it created 10 E2 instances okay so again inside the data centers there will be multiple physical servers let's say this is a phys phical server and example.com requested 2 E2 instances so it gave 2 E2 instances inside This Server let's call it server one so then there is example two and what AWS did is it created again request of example two in the server one only then there is uh example three and what AWS did is again it went to here and it created server let's say it requested for one server it created in server one only here what I'm talking about virtual machine one virtu two virtual machine Mach and two virtual machines so this is fine all of this problem is solved and AWS is also giving them virtual machines inside the data center inside data center like I told you there can be multiple servers for one server AWS choose one server and soled all of their problem but what happened is let's say so this is a uh Theory what happened what can happen is that in this case so if there is a hacker and let's say this is startup okay let's say this is startup if there is a hacker here because they were not maintaining proper security he entered into this server he tried to make some false request to this application inside uh the physical virtual machine and he was able to hack this particular server now because all of them are in the same physical server inside the AWS data center hacker can easily come here hacker can easily come here hacker can easily come here and he can hack the entire thing right so just because of example three or just because of startup what happened is all of the servers are hacked because AWS was creating entire instances or AWS was creating all the things in same things or it can also create in different things but what happens if it creates in the same server right so till 2013 2014 this was happening so to solve this problem like I told you even in the previous example to solve the security breach what AWS said is okay we will come up with a new Concept in that case it was Secure community in AWS terms it is called as VPC right so what AWS said Is We will build a VPC for you or in the other way around like in the previous case the Builder will build the entire secur community and give it to you but in the AWS terms AWS will give you documentation AWS will give you examples who is the one who builds this entire VPC and maintains the VPC it is devops or AWS devops Engineers okay so AWS devops engineer looking at the documentation of AWS they will go to the AWS portal and they will request for the VPC and they will configure everything inside the VPC now let's see what is inside the VPC using the same example itself in the same example what I told you I'll try to convert that into AWS example right so this is AWS data center again so there is uh let's take example of one company here so let's say there is TCS and inside TCS there is uh one project and this project is owned by a devops engineer so what this devops engineer does is he will go to AWS and let's say there is a region called Mumbai he'll go to AWS and say hey AWS give me a VPC now how do you define the size of a VPC in the previous case because it was a land you can Define the size of land in some Acres or in some hectars or something but how do you define the size of a VPC so for defining the size of VPC there is something called as IP address range okay so what is this IP address range so IP address range whenever a devops engineer creates a VPC what AWS asks is what is the IP address range so if you say for example the IP address range is 172.16.0.0 sl16 that means AWS will allocate 255 into 255 now don't worry about this calculation what is abish calculating and what is happening here you will understand this in future for Now understand that AWS will give you 6536 IP addresses okay so how do you define the size of a VPC you can Define the size of VPC using the IP address ranges so devops engineer will say hey for our project in TCS I need this particular IP address range for the VPC which means I need 6536 IP addresses so technically you can assign these IP addresses to 6536 applications instances whatever you would like to call to right so this much IP addresses can be allocated using the VPC but what devops engineer will do is PC can be one project but inside the project there will be multiple sub projects right so there can be one project related to payments one project related to transactions one project related to XYZ so what this devops engineer will do is for One internal project let's say there is one internal project here there is one internal project here one internal project here so this devops engineer will split the IP address ranges right so what he will say is okay for this particular uh sub project this particular sub project and this particular sub projects divide the IP addresses so here he'll say 17216 1.024 2.0 sl24 3.04 so this particular thing will get some 255 IP addresses this particular thing will get some 255 and this particular will get some 255 okay now again don't worry about the calculation how abishek is calculating and all that is irrelevant to this discussion for now just understand that for TCS there will be internal projects and depending upon the internal projects this devops engineer will split saying that okay even the VPC might have one IP address range but I will split them to three different IP address ranges for three projects and this particular concept is called subnet okay the name itself says that it is sub Network right so it is subnet what you are doing here the VPC is created with a particular IP address range you are splitting the IP address for your sub projects devops engineer is splitting this for the sub projects calling it subnet and inside this sub project let's say this sub project has only one application so you deploy an E2 instance and you deploy that application here there can be two instances here can be three instances depending upon the sub project it is up to the sub project development leads how many applications do they want to deploy inside a subnet they will they will contact devops engineer as well but for now let's say this is the configuration so there is sub project sub project sub project one application two application three applications so now what devops engineer will do after this once he creates the subnet and all now devops engineer like we discussed in the previous example he will create a Gateway right why is this Gateway required if there is no Gateway nobody can access this particular uh uh VPC right so without a gate nobody can enter the Gated Community or the secure Community similarly without this gate nobody can access or enter to this entire uh property itself let's say there is a relative uh in previous case here let's say there is a customer who wants to access application in this ec2 instance there is customer who wants to access application in this ec2 instance so he has to definitely come through this uh uh Gateway itself to the VPC so firstly there will be a Gateway and what this Gateway will do is Gateway is just like a pass for someone to enter to this VPC now once they enter what will happen like I told you this is one subnet this is one subnet and this is one subnet but this is all a free space right inside the VP see these are the subnets which we call as private subnets because they don't have access to Internet but then there is something called as public subnet right so what is public subnet public subnet is the one that a user first access inside the VPC so how does public subnet connect to the user or how does public subnet connect to the internet basically is using the internet gateway so this particular thing this particular Gateway here we call it as internal Gateway sorry internet gateway through the internet gateway there is a public subnet from which the user enters to this okay once the user enters basically there let's say there is a load balancer here okay what is load balancer load balancer is the one uh that forwards the request depending up the load for example there is this application and the user from the internet want to access this application now how will the request from this public subnet from the load balancer goes to this application right so there has to be a particular path so for this public subnet what you will do is you will create something called as a router in AWS terminology you will call it as a route table so you will create a route table this route table defines how should application or how should the request go to the application right so you have a load balancer load balancer can send the request depending upon the target group we will discuss about load balancer in detail later but for now just understand that there is a internet gateway once from the internet gate there is a public subnet from the sub subnet request goes to the load balancer load balancer is assigned with the public subnet and for the load balancer what you will do is you will create a Target group of this application okay so now what happens is if the request from the load balancer has to go to the subnet load balancer does not know how to go to this particular thing so what you'll do is for the subnet you will create something called as a route table okay so this route table defines and tells the load balancer okay go through this particular part so that you'll reach me and once the request from internet now reached here but still at the E2 instance like I told you in the previous example there can be a security guard here who can block your request similarly for the E2 instance there can be something called as Security Group okay so there will be something called as Security Group where this Security Group can say okay so on which Port do you want uh do you want to access me or from which which IP address are you coming from what is your IP address then the security group can say okay only if it is coming from this particular IP address in the Internet only if you're trying to access me on a particular IP address then I will allow the access and that way you will finally reach your application okay so what is happening on the whole if someone from the Internet is trying to access an application in the private subnet here first of all he has to the request has to go through the internet gateway once it reach the internet gateway it will go to the public Subnet in the VPC right what is public subnet it's a common subnet across the VPC once it goes to the public subnet there is a load balancer here right what is load balancer doing load balancer is attached with the public subnet and load balancer has a Target group so when request goes to the load balancer load balancer is the one that takes the request to the private subnet and to the application right so to reach I mean for the load balancer to understand how to go go to this application the subnet is here but what is the route path for the subnet so there is something called as route table right so route table is the one which defines the path once the path is defined it can go here but still there is something that can block that is called as a security group once the security group also allows you your request is finally reached now let's try to write these things okay so that you understand what exactly has happened first of all from the internet right so let's say this is internet there is person in the internet here he is using a laptop and what he is trying to do is he's trying to let's say access an application which is called for example example.com or okay let's leave the uh a domain and let's call he is trying to reach uh something called 172 do 16. 3.1 SL something okay in General he'll not use the private IP address he will use the load balancer IP address to reach there but for your understanding let's say he's trying to reach one application in the private submit okay so to reach what I have told you first is there is a devops engineer who has created a VPC right inside the VPC what is there here there is basically an internet gateway okay so let's say this is the internet gateway and then inside this VPC right if you call this entire thing as VPC which has has a basic IP address block or the IP address range this entire thing has a IP address range inside this what he has done is for project a he has created a subnet Project G Project B has created a subnet and project C has created a subnet again what is a subnet this is the entire IP address range so for each project you can divide the IP address range if this IP address range is 172.16.0.0 sl16 this can be 17216 1.0 24 2.04 3.02 24 that way you can split now let's say there is one application inside the subnet one easy to instance and end goal is to reach from here from the internet which is completely outside of VPC you want to reach from here to here okay so what is the process firstly it will come here it will see that there is an internet gateway here it will pass through the internet gateway and once you pass through the internet gateway like I told you there is a public subnet here okay what is public subnet public subnet is the one that can be accessed to the public outside the VPC but they have to pass through the uh internet gateway once they pass through the internet gateway in the public subnet there is something called as load balancer in AWS world you will call it as elastic load balancer okay so you have elastic load balancer it can be application load balancer Network load balancer we'll talk about that later now you have an elastic load balancer here so what this elastic load balancer does is your request Quest from the external world has reached the elastic load balancer from the elastic load balancer now it has to go here but how will the load balancer understand that it has to go here this is a private subnet right but for elastic load balancer or for the load balancer to send your request to the private subnet there has to be a proper route okay so who will Define this route there is like I told you something called as a route table okay okay now you need to have a route table and using this particular path your request has to flow right so what you will do is for elastic load balancer one thing you will attach the private subnet and you will attach the target group okay so what is Target group you will understand when you learn more about the elastic load balancer but for the load balancer to understand request has to go here firstly you need to create something called as Target group and you have to assign this instance to the Target group and at the same time the subnet should have the route so that the traffic is Flowing here okay so and here like I told you there is a Security Group which can block the request or accept the request so these are the entire flow what are the components that we talked till now one is internet gateway then we talked about public subnet then we talked about load balancer then we talked about route tables which are basically routers in uh other terminology so routers are the one which forwards a request even from your home laptop if you're trying to access a particular website there are routers and there are multiple hops using multiple hops different routers connect to each other and forward your request and finally your request goes to a particular server even in AWS that's the same but you call it as route table here so you have route table that you understood then you understood about security groups right so these are the critical components of VPC additionally there are few things which I'll cover very quickly but in future class that is in tomorrow's class we will talk about them in detail one is like let's say you have one Security Group here you have another Security Group here you have another Security Group here okay but within a subnet okay if you want to define the same Security Group to multiple applications multiple E2 instances or you want to repeat the security group configuration there is something called as NSS okay NSS are basically automation for security groups where instead of defining the same thing again and again you can Define that as part of NSS okay and finally there is one more concept that I will explain and I'll conclude today's class that is the concept of NAD gateways now what is n Gateway I'll try to explain in this scenario itself instead of directly explaining you the example and see uh the concept of NAD Gateway now the request from here has reached this application which is well and good but now what if the request I mean sorry now what if this particular application here tries to access something from the internet okay now till now we talked about this workflow right workflow flowing from here to here but what if the application tries to access something from the internet from google.com let's say this application wants to download a package which is quite common right U you might want to download something to your server so there is a server here and you want to download something from internet so what is a problem here you might say that okay again it can use the public subnet and it can flow and it can access it the only problem is that okay uh okay the request can flow how the request will flow is a different challenge we will discuss that but the only thing is when someone tries to access some resource from the internet one of the critical things that you have to remember is if this is in the private subnet that means that there is some sort of security here so it is a bad practice to expose your applications or to expose your servers IP address to the internet okay so whoever is receiving this in the internet so let's say you are trying to download something from xyz.com okay so xyz.com should not know the IP address of your uh server or it should not know the IP address of your application anything it should not know uh sorry it should not know the IP address of your private subnets applications or the servers easy to instances it is going to be both Theory and practical and theoretically practically we will understand one of the most important concepts of VPC which adds additional security that is security groups as well as n right so this is what we are going to learn in detail today both theoretically and practically so please watch the video till the end so that even you can perform this act uh activity like even you can take this as an assessment and you can try this at your end by watching this entire video got it so now without wasting any time let's quickly see what is this Security Group what is this nccl and what exactly they are going to do within the VPC so if we go back uh to yesterday's video very quickly what I told you is if you consider this as VPC right let's say this is a virtual private cloud and I have explained that okay in the world of virtual private Cloud the very first thing what you will do this is a real life example right how organizations use VPC as of today this is the same thing so initially you will have something called as an internet gateway right so usually what happens is within the VPC there will be a public subnet so and there will be private subnet right so what is a subnet basically I explained in the last class when you create VPC you will Define what should be the I i p range or what should be the IP address range of your VPC it is just in the similar terms you can consider how many IP addresses you want inside this VPC right so usually uh let's say that when I create VPC if I say that the IP range has to be 10. 1.0.0 hyph 16 then that technically means you will have within the VPC 6536 IP addresses okay so this defines the size of VPC how big a VPC or how small a VPC so when you have 6536 VPC uh IP addresses that means technically you can use 6536 applications components within this VPC okay so let's not go into the detail of this but just understand that when you create VPC then you define an IP address range which defines what should be the size of your VPC now within that you can divide subnets so subnets are basically like within VPC if you have multiple projects okay so as a devops engineer you have created an AWS account then you have created a VPC for one of the project within the project also there will be internal projects right so for each internal project for group of applications you can Define private subnets okay so this private subnets basically will not have access to Internet that's why they are called private subnets so by default these applications cannot be accessed from internet so that's where VPC comes comes into picture and what VPC does is whenever let's say this is a user okay I know that I'm taking time here but this is very very important for everyone to understand when user eventually want to reach here like you know user want to access an application here what happens is user firstly tries to go through this internet gateway okay and this is a public subnet where anyone can access this P public subnet okay saying that you can add few conditions here through the internet gateway and through the apaa gateways you can add conditions and validate few things is this user good user bad user hacker or whatever it is we will talk about that later so through the internet gateway initially what happens is within this public subnet devops Engineers usually place a load balancer okay so load balancer can be directly accessed from outside world and what happens is this load balancer will talk to this private subnet okay so load balancer is the one that has access to private subnet and this user has the access to load balancer and within the load balancer also you can do lot of configurations you can restrict traffic you can say that okay uh etc etc and you can make this more secure with the load balancer as well but today's topic is about once the load balancer forwards the request to the private subnet okay so there are two things one is you can add additional security at the layer of the subnet okay for each subnet AWS says that hey you can add more SEC security at the subnet level okay so if you add more security at the subnets level what we will do is we will start using using nscls whereas even if you bypass this let's say you have not added anything at the uh sublet level you can as a devops engineer or the application owner what you can do is you can add more security at the ec2 instance level okay the level where your application is actually deployed so at the ec2 instance level if you add security then we call it as security groups so using security groups you can add more security at the ec2 instance level whereas if you want to add more security here at the subnet level then we will call I mean we will use NSS okay don't worry even if you haven't understood exactly what is happening here when I deep dive and explain it further you will understand but for now here what I wanted to explain is in AWS there are multiple layers where you can add security because security is one of the key components for any organization if they want to move to public Cloud what they will first see is is that public Cloud secure enough or not you can talk about costing you can talk about any level of things but for any public Cloud tomorrow if there is new public Cloud what you will first see is how secure this public cloud is because you are going to put all of your application in the public Cloud right so if you if your organization is using AWS it will have your organizational user details you will have uh details like the database and everything is hosted on AWS so first thing organizations look at is how secure is AWS and that's where AWS says that okay we will do our part so the fundamental thing that AWS says is in AWS security is a shared responsibility understand this sentence very keen in AWS security is always a shared responsibility what does this mean so AWS says that okay from our side we will try to add as much security as possible like we will tell you that you can use VPC we will tell you that hey uh you can add uh security groups you can add nsel's you can add uh API Gateway you can add a lot of security but along with that we will need that means AWS is saying along with the things that we are adding we need help of devops Engineers or AWS admins whoever it is like right most of the times AWS admins are also devops Engineers these days or network admins system admins whatever your company calls it but AWS says that security should be a shared responsibility we will do our part as devops Engineers AWS admins or network admins you also have to do your part and that where I mean that's exactly where all of these component comes into picture whether adding load balancer for your organization uh do you want to use default load balancer different load balancer do you want to use API Gateway or not and how exactly you want to configure security groups and NSS all of these things devops engineer play a very critical role in today's topic we will debug and deep dive into security groups and NSS which comes at the last point of security right which comes at the last point of security but this is very critical because if you are not taking necessary actions at the last point then your application will be easily hacked right nccls and security groups are very critical because of this very reason that is they act as a last point of security in your AWS account just before the user request reach the application right if this is user and this is where the application is it has to travel a long path right the request has to travel a long path within the VPC it goes through multiple components but when it reach here before this point the last point of security is served by security groups and nscls okay so that's why this is very important and we will try to focus I will explain you like what and when to use Security Group and what and when to use nscls so firstly let's start with this concept on understanding what is the primary difference between security groups and nsls so Security Group basically serves at the instance level okay now what do I mean by this on day three so whoever has followed our day three's video or in general what we have done on day three is firstly we created an AWS account right in the AWS account in the default VPC like whenever you create AWS account itself AWS will give you a default VPC without VPC AWS will not deal with anything if you want to create custom vpcs you can do as we will do today but by default AWS will give you a VPC and what we have noticed is we created an E2 instance and within this ec2 instance we tried to deploy a Jenkins application and and as a user we tried to access this genkins application what happened we were not able to access this application by default because as I showed you in that video there was something that was blocking this one right so by default as I told you in AWS security is a shared responsibility and by default what AWS says is we will not allow anything to your instance directly okay so if you want to do that you need to configure your Security Group okay so if we want to configure anything at the instance level that we do within the security group Concept in AWS and that's exactly where to access the genkins application deployed in the ec2 instance we have allowed port 8080 or you can also allow all traffic okay so this is where devops engineers and the applications owners have to be very careful instead of opening port 8080 let's say in your organization you have opened some other Port let's say you have opened Port 9,000 10,000 and multiple other ports and there was one hacker who was trying to access this and they were luckily able to access these ports and they tried to hack something okay or additionally whenever you open ports that are not used or you keep opening a lot of ports then there can be some kind of unwanted traffic that can flow to your ec2 instance and reach the application so AWS is doing the part of by default not accepting any traffic whereas as devops engineer you have to do your part by allowing only specify traffic into your ec2 instance and into your application that thing is done by security groups Concept in AWS I hope this is clear if you still feel that this is not clear when we do the Practical you will understand we will deploy a python application and I'll show you the same thing again using the security groups now you might ask me okay this is a very easy topic so it is an easy topic but there are few things to understand in the security group so within the security group basically there are two things one is there is inborn traffic and there is outborn traffic what is inborn and what is outborn basically when you deploy your application inside the ec2 instance right so if this is an ec2 instance and this is your application there are two kind of activities right what is the first activity as a user user will try to access your application and as an application probably you will try to access google.com or you will try to access something to get some kind of information or you want to access some third party application so this traffic that is flowing into your application okay this is called as inbound traffic and this traffic which is Flowing outside of your application is called as an outbound traffic for example let's say uh this application that we are talking about is amazon.com this is just a blunt example okay so user today I can be an user who will try to access amazon.com and amazon.com probably can access any thirdparty application for example Razer pay or amazon.com can access other application like Amazon pay to access Amazon pay amazon.com this request has to move out of uh this one right go out of your ec2 instance and access some other application that is Amazon pay and user will try to access the application so this access where user is trying to access amazon.com is called as inbone traffic whereas amazon.com trying to access the razor pay or the Amazon pay is called as a outbound traffic okay so as part of security groups you can manage both inbound traffic and you can manage the outbound traffic that is traffic coming into your E2 instance and traffic going out of your E2 instance again like I told you you should be very careful because AWS by default what it does okay by default when you create a security group or by default when you create an e to instance AWS will assign a default Security Group because AWS will also take care of your security right so in this default Security Group what AWS says is hey what I'll do is by default I will allow all the outbound traffic okay all the outbound traffic I will allow like using your application you can access anything in the internet except for Port 25 I will tell you why uh there is a story behind this port 25 but except for Port 25 I'll allow you all the outbound traffic and what AWS says is but anything that is coming inside okay anyone who is trying to access your application as inborn traffic I will deny everything by default okay in the security groups what aw is doing is for the inborn traffic and for the outbound traffic okay for the inbound traffic what AWS is saying is okay I have lot of restrictions here I don't want to allow anything if you want to allow anything then you can use the concept of security group inbone Traffic rules and you can add the specific port number or the specific IP address where the traffic has to be accepted but as part of outb traffic I'll allow everything and if you want to restrict okay or if you want to do any uh specific configuration you can do in the outboard traffic okay so this is the thing related to inbound and outbound traffic we will see in the practical video how we can use this inbound traffic outbound traffic now talking about the story of Port 25 so what happens with Port 25 is that so in this story AWS does not allow the outbound traffic default on the port 25 because Port 25 is basically a mailing service okay so AWS does not want you to uh you know there AWS does not want any kind of spam activity or you know AWS does not want to record the IP address of this ec2 instance or record the IP address where your application is hosted so AWS by default blocks the port 25 only outbound traffic there can be a lot of spamming activities or something like that okay so now let us see if Security Group is doing all of these things now why I need NL first of all what is NL right you might have this question abishek you have been talking about NL you keep saying NL NL what exactly is this NL so NL basically stands for Network Access Control list okay so this is a very complicated name that's why in the world of AWS people just call it as n okay so NSL network access control list basically goes a level Beyond okay so security groups is applied at the ec2 instance level whereas NSL is applied at the subnet level okay NSL is applied at the subnet level now with NSL devops Engineers can play even critical role because let's say you gave an E2 instance for development team one and what this development team did is you know just to make their process quicker what they have done is they have deployed a Jenkins application or any other application they know that they should not allow all the traffic but to make it easy what they have done is they just said that okay I'll have all the traffic inside my E2 instance instead they could have just opened Port 88 but what they've done is they said okay accept everything allow all the traffic to my E2 instance and allow from all the IP address in the world it can be IP address from Turkey it can be IP address from India all of everywhere everything and you know I'm fine with it so the problem is that this particular instance owner or this particular development GP Development Group they have for some reason they have ignored the security aspect in the AWS right because security groups are meant to open only specific ports and allow only specific traffic into the ec2 instance so how and what you can do as devops Engineers is instead of okay so this is fine every instance owner they can open specific ports whatever the traffic they would like to but additionally what AWS says is if you go back to the previous diagram where I said uh this is VPC and and if this is public subnet this is private subnet right similarly there will be lot of private subnets so what aw said is we will give you more access instead of instance owner defining what kind of ports or what kind of traffic is allowed if they do some kind of mistake there and if they say that allow all kind of traffic as devops Engineers or administrators you will have one more layer where at the subnet level at each of the subnet level what you can do is you can Define what can of traffic do you want to deny understand this carefully here I'm saying what kind of traffic do you want to deny so if you deny some traffic at the subnet layer even if you try to do that or even if you try to accept that at the security group layer there will not be any advantage so devops engineer Network Engineers or AWS administrators what they can do is they can make advantage of nscls to Define their organizational Network traffic okay so if something is applied at the subnet level then it is by default applied to all the instances within the subnet okay so if there are 50 ec2 instances sorry if there are yeah if there are 50 ec2 instances inside this private subnet let's say you give this private subnet to development team three this is just an example you gave them to development team three and what they have done is they have created 50 E2 instances in inside this private subnet and for some reason they said allow all traffic but because you have applied NSL configuration and you said by default deny all the traffic and allow only specific things if you say that at the NSL layer what will happen is AWS will deny all the traffic that is flowing in and only allow the configuration that is defined in the NSL so NSL will add as an additional layer of security got it so instead of applying the configuration for E each and every E2 instance also you can use NSL for automation so maybe in your organization devops Engineers are the one who is taking care of the instance level security that is security groups as well as nsls then what you can do effectively is instead of adding security groups for each and every E2 instance if you just apply NSL to the subnet if there are 10,000 E2 instances also this NSL configuration is directly applied to all the 10,000 ec2 inst es okay so that you can automate this manual activity of assigning the rules to each and every instance using Security Group so it is up to you whether you want to use Security Group whether you want to use NSL or you want to use the combination of Security Group plus NSL okay so NSL basically the primary purpose is you can deny what kind of traffic that you would want to and you can allow what kind of traffic you want want to whereas security groups is only for allowing Security Group does not have deny uh thing in Security Group you will only configure the rules for allowing so now we discussed so much here uh probably for some people there might be some kind of confusion so what we will do is we will try to deal this same thing practically and make it understandable for people practical part that I going to demonstrate on the AWS account today so if you watch this carefully what I'm going to do today is this dotted lines represents virtual private Cloud so firstly I'll create a custom virtual private Cloud you haven't seen how to create virtual private Cloud so that's going to be interesting so firstly I'll show you how to create this virtual private Cloud on AWS and when you create virtual private Cloud as I told you security is a shared responsibility so as I create virtual private cloud and provide the IP address range AWS by default will create an internet gateway for me AWS by default will create NL with default configuration and AWS will create a route table additionally what I'll do is I will create an ec2 instance and attach a security group to this ec2 instance now once we do this we will play with this Security Group configuration and NSL and we will see how traffic flows into this ec2 application sorry application inside the ec2 instance what happens if I block something in the security group what happens if I allow something in NL and block that thing in uh sorry uh allow something in Security Group and block that in NL right all love all love block all love we will try a lot of things so let's see how does this configuration go and you can also try the same thing as your assignment so because I'll share you all the steps right now you can watch the video and you can follow the steps as I do so let's quickly go on to the A s Management console here so this is the AWS Management console right so what I'll do now is move to the service called VPC so this is the VPC service some people might be watching this for the first time they might have not seen VPC before so what we will do here is once you go here you have lot of things within the VPC don't get scared some of these things you have already learned about uh in the previous classes what is VPC what is subnets Route tables internet gateway security groups today we learned about Nel's as well if there are few things that you don't understand don't be scared eventually you will understand all of those things so click on VPC and let's click on create VPC okay so there is two options here I mean there are two options here VPC only VPC and more go for VPC and more because AWS will take care of creating the resources for you I mean the default resources for you for example when I create this VPC using VPC and more what AWS is going to do AWS is going to create public subnet for me as well as private subnet for me in both the availability zones right I'm in the region North Virginia in North Virginia there are two availability zones Us East 1 a Us East 1B so AWS will create four subnets the public subnet and private subnet for me so that I don't have to create a lot of configuration in realtime example like I told you public private subnet comb is used more then AWS is also going to create route tables for me finally it creates internet gateway igw stands for internet gateway and it also creates some VPC endpoints for S3 bucket which I'll tell you later right now S3 buckets is not taught in this series so don't worry perfect now what I'll do after this is you can provide any name or you can use the random name so I can provide some name here for this VPC let me call this name name as demo VPC okay and this is exactly what I've been talking about so here if you see if I provide this configuration sl16 what AWS is saying me that you'll get 65,536 IPS if I want to modify if I just say this as 1.0/4 see what happened AWS said that you'll get only 256 IP address so in future if you require more than 256 IP address then you have to modify this VPC or you have to add additional uh configuration so whenever you create the VPC if you have an understanding of what is this going to be what is the size then that's better and you can modify these things as well uh for example instead of 10.0.0 do uh something you can also change this to 192 series or whatever series that you would like to so let me just change this to uh whatever AWS was giving me by default and now what I'll do is here you can configure number of availability zones that you want technically I want only one availability Zone because this is just a demo thing but let it be you can go with the defaults at this point of time because you are just learning number of public subnets I want public subnet because if you go back to the diagram I'm going to do this entire thing using public subnet only perfect uh all the configuration is provided now I can simply like this VPC endpoint uh right now you can put it as n as well but if you want to go with default let it be as S3 Gateway click on create VPC now the VPC configuration will be created see what exactly VPC is doing firstly it created the VPC then it is configuring the DNS we will learn about DNS tomorrow don't worry then VPC creation is done it has created subnet for me these are combinations of public and private subnet then it created internet gateway that's why I keep saying security is the shared responsibility a s will create configurations for you Additionally you have to look at those configurations you have to modify those configurations if required add additional security if required AWS has then created route tables create route associate the route tables with the public private subnets and finally you know AWS has also done the route table creation okay so this is uh all the things uh that are created as part of this VPC workflow now let me go back to the VPC that is created for me and here you can also look at the rough diagram of how does this VPC look like just go to the resource map and here you'll get a total picture of how your newly created VPC is looking like so just spend some time here and try to understand what is happening here this is exactly what I've taught you on day four so you can understand if you don't understand you can ask me any specific question as well perfect now this part is done I've created the VPC AWS has created internet gateway for me see this is the default internet gateway in this picture you can see if you scroll down so this is the default Ingress greatway that AWS has created for me and AWS also creates NL and security groups which you will see so let me go back and create the ec2 instance now because I want to place the E2 instance in the public subnet of the VPC and demonstrate security groups as well as NL go to inst instances click on launch instance what we will do different in today's class from the previous class where we created E2 instance is let me call this as demo instance let me choose the flavor as ubben to T2 micro is fine for this class no problem provide the key value pair whatever you would like to use uh I'll use this one called AWS login for example here in the network configuration you have to edit this right so edit the network configuration and in the network configuration don't go with the default VPC default VPC is the one AWS creates for you so that you can start working with it if someone does not know the concept of VPC they just entered or they just started with uh AWS they can use this default VPC and they can play with it that's what we did on class three on the day three we use the default VPC which comes up with all the default configurations of AWS but now I want to play with my custom VPC in your organizations you will always use the custom VPC only so demo VPC let me click on this one and see what subnet is getting allocated to your instance right in this case AWS is saying that I will allocate the private subnet for you which is a General Industry practice by default your ec2 instance and the application should be in the private subnet but because today we are trying to understand this concept and not going in detail load balancer and all the other configurations so I will change this to the public subnet how do I do that just go back here and change the subnet demo subnet public one Us East one okay so because I'm in the availability Z I want to play with only one availability Zone I've choose the US East one availability Zone assign public IP address yes I need a public IP address for my E2 instance create security group or select any existing Security Group like I told you if you are not aware of these Concepts then you can ask AWS you know uh to select any existing security groups but in this tutorial I'm going to play with the AWS use custom VPC so let me let AWS create Security Group for me and then let me tweak it rest all the configuration looks okay to me let me create on launch instance now what we will do is inside this AWS cc2 instance that is getting created for me I will install a python application okay I will just use a very simple python application on this E2 instance and what I'll do inside this uh python application is I'll run the application on Port 8,000 and now I'll try to access by default you will I mean I think most of you already know that when you try to do this the security group will block because the default security group that AWS created for you will not allow the traffic directly you need to all the traffic explicitly so I will enable all traffic I will enable Port 8,000 and show you what happens and then we will try to block the traffic we will say block all traffic in NL and see what is going to happen Okay so this is going to be interesting now the ec2 instance is also running let's get the IP address of the ec2 instance and log into the ec2 instance using my terminal so this is my terminal what I'll do is SSH uh I hope I have that pem key AWS login pem perfect change the IP address also great so let's see uh log to the E2 instance and what is the first thing that you would do after log to the ec2 instance you need to update the packages right because I want to run the python application so firstly I want to update the packages see python is installed and then I will run a simple HTP server so firstly do sudo appt update it will not take much time all the packages are getting updated it will not take much time here perfect all the packages are updated now now let me see see if python is installed or not so let me just run Python 3 command perfect python is also installed now I will just run a very simple HTTP server on python for that you don't even write you don't even have to write a single uh command or you don't even have to write a single file just say Python 3 minus M HTTP do server and provide the port whatever you would like to so a simple HTTP server is now running on Port 8,000 I ideally if you open this instance on this IP address and access the port 8,000 you should be able to access but let's see what would happen so we saw the same thing uh in the last class as well on day three there have used genkins and I start I tried to access the IP address on Port 8,000 which failed and then I enabled right so let's try to do the same thing now this is my instance copy the IP address just say HTTP Callum IP address colon Port 8,000 the application will not be accessed because what is happening let's go and see this is the ec2 instance this ec2 instance is attached with a security group this is the default security group that is attached and what is happening in this default Security Group is that you know what AWS is saying is I will allow only Port 22 this is the port 22 that is only allowed the reason is also because because AWS want only you I mean AWS only wants to allow the SSH traffic if this is also not allowed then we will not be able to even log to the E2 instance so only SSH is allowed and let us see what is the NSL configuration as well right so if we look at the NSL configuration just go uh take I'll take a different tab here and open VPC Network ACS so what is the network ACL attached to rvpc this is rvpc right demo VPC so let go to that specific thing and see what are the inbound rules why are we looking at inbound rules because inbound rules is the one that is coming to our uh application so it says all the traffic is currently alled so Network ACL which is the first layer of Defense okay which is the first layer of defense for the end ire subnet what is configured here it said that okay allow all the traffic I don't have any problem you can say that there is also deny rule here but deny rule will only be triggered if this condition is not met okay so order goes with the order of priority whatever is here in the first row so it depends upon the number so least number will be here first let's say I'll configure one more rule uh and the rule number is 200 so first rule 100 will be verified then rule 200 will be verified and finally it will go to rule star okay star is the last thing here this condition is met right if this condition meets that is if all the traffic is allowed then there is no problem okay let's say here instead of all traffic I can also say only all Uh custom TCP traffic of Port 8,000 or I can say all love only custom uh TCP traffic of Port 9,000 which we will see but for now all traffic is enabled so no problem from NSL what AWS did here is from the internet gateway you got entry into the subnet okay because NSL is allowing all the traffic so internet gateway can forward the request to the route table if there is a load balancer here then the request would have gone to the load balancer from the route table request goes to the load balancer but because there is no load balancer here from internet gateway if NSL accepts then the traffic goes to the route table right so let me add an arrow here so that it is much clear so this is what has happened here from the NSL Now traffic goes directly to the route table and Route table would forward the request to the ec2 instance let's see what is happening and why if NSL is accepting why the request is not going to the E2 instance that is because Security Group is blocking it first layer of defense is cleared but the second layer or the last layer of the defense which is Security Group is blocking the request now let's try to unblock it so for that go to the security group edit the inbound configuration add one more Rule and inside this say allow Port 8,000 so custom DP TCP means the custom Port 8,000 of TCP you're allowing Port 8,000 from any IP address for now you can say uh any IP address anywhere in the world uh the uh the resource that is using ipv4 anywhere in the world just save the rules and as soon as it is saved you will see that the configuration will be applied to ec2 instance and if I just refresh this page you will see the output what is this output basically I'm just running a simple HTTP server right I'm not running anything more than that this is a very simple HTTP server so that's why what is happening here is the request that you have sent is listing out the directories that are available in that folder anyways you got the request output you can verify or validate this request output here as well so you got 200 response that means the request is successfully sent and you have got the response back now what if okay what if you have opened this port right so where is this in the security inbound rules you have opened this port let's say you are the instance owner you are not the devops engineer and as part of the instance ownership what you have done is you have enabled Port 8,000 but your organization has a very strict rule that the port 8,000 should not be enabled so as devops engineer you can configure this you can control this thing at the NSL level itself because you cannot monitor each and every ec2 instance and you cannot control every ec2 instance you can say that for my organization or for this particular VPC I will block Port 8,000 in the naacl right or the network ACL configuration see the what would happen so the power of devops engineer using NSS this is Security Group where is NSL let's go to the NSL Yep this is NSL this is the inbo rule what I will do is edit inbo rules instead of allowing all traffic just remove this add new rule and what you will say is in this specific rule let me call this rule as 100 custom TCP Port range 8,000 I want to block anything that is coming from internet just deny now let's save this change okay and let's try to copy this URL take in a different browser and just try to enter it and see now you will see that the request will not reach the application that is because devops engineering team has blocked that specific IP range or devops engineer has denied that particular traffic in the NL so it will be blocked for the entire subnet whether you are e to instance owner whether you can be anything because the network configuration is blocked you cannot access now again let's try to edit this inbound traffic Rule and say that for 100 I will just say all traffic Al Love Save changes edit inbound rule I will add one more inbound rule okay and that inbound rule number is 200 okay custom TCP Port 8,000 any guesses on what would happen I will deny this let's see how much you have understood here I have added this rule number as 200 and I've have denied the traffic let's see if I save changes and refresh this you will see that the traffic is sent even you have denied the configuration in the NSL because NSL goes in this specific order right so the order would be first you will verify the I mean AWS will verify the first lowest number the lowest number is 100 or it goes with the specific order that is mentioned here it is mentioned in the ascending order itself so firstly it will check rule number 100 which says all traffic is allowed so this rule is met AWS will forward the request the request goes to Security Group in the security group also you have allowed that Port 8,000 so it directly reach the specific application what happens if I simply rename this one like if I make this as 200 and if I make this as 100 save changes now okay so let's save one by one so let's keep this as uh 200 let keep this as 110 okay you can keep any number it just has to be the order that's it now try to save the changes so it will reflect and 110 goes above of so this is the first first rule what it says is if the port number is 8,000 block if the port number is not 8,000 then it will go to the next rule what does the next rule say all love all the traffic now let us see what would happen just copy this URL take this in a different Tab and try to refresh again the traffic will not reach the E2 instance the thing is now the application developer will be confused hey what happened I have allowed the port 8,000 here but still my application is not getting the traffic because devops engineer or the network Engineers have blocked this specific Port now this port they might have blocked because there might be some security reasons there might be some concerns it's not just about the port but with the NSS you can play around with different things you can play around with IP addresses let's say you are worried about some IP address that is coming from XYZ country and you know the IP address range you can say block the IP address that is coming from 3.4. 5.6 range or if you know the specific IP address you can provide the specific IP address if you know this if you don't know the specific IP address you can provide the range as well 3.4. 0.016 right you can say anything you can block on the IP address you can block on the port range you can play around with NCL and Security Group configuration so what you have understood at the end even if you allow the configuration in the security groups NCL acts as the first layer of defense and devops Engineers as well as system Engineers Network configurations or the admins who has access to nscls they can overall block the configuration at the subnet level right so today we understood the top topic of NSL and security groups with respect to allowing the access and blocking the access please take this as an assessment you can watch this practical part one more time and you can try this at your end this is really really important today we just tried with one public Subnet in future we will enhance the scope we will add the private subnets we will add load balancers we will add API gateways We'll add a lot of configuration multiple availability zones so there this concept you will understand even more in detailed way when we play with other components of AWS but for that today you have have to practice this and let me know in the comment section were you able to try the assessment were you able to solve this do you have any questions please let me know in the comment section and I'll more than happy to reply to it so firstly let me give you a brief overview and you know what is this project about what are we going to implement what are some things that we need to understand before the implementation of the project firstly this project demonstrates how to create VPC and how to secure your applications within a VPC in a production environment right so if you look at the architecture diagram here there is a VPC and inside the private subnet we have deployed the applications and there is a public subnet where you have load balancer and the N Gateway users would access this application through the internet gateway to the load balancer and from the load balancer it will reach the applications in the private subnet right so this is the entire architecture that even I'm trying to explain here so you can pause the video here and you can also just R read through this entire thing so that you can explain this same thing when interviewers ask you in the interview process but just a overview like in four lines first we will create a VPC that has public subnets as well as private subnets in two availability zones now why I need two availability zones you can ask me like AB when you explain the theory part you were just using one uh VPC in that VPC you are only using one public subnet and one one private subnet and that to in one single availability Zone but when you do this in production just you need to create two availability zones instead of one so that for some reason if the data center of AWS in a specific region or the availability zone of AWS in a specific region goes down you will still have the other availability Zone that is serving the traffic for you so that's the only purpose we create a VPC with public subnet and private Subnet in two availability zes and in the public subnet what we will do is we'll deploy a ad Gateway and a load balancer why you need load balancer I'll explain you but why you need an ad Gateway let's say this applications here if they want to access something from the internet probably they want to get some uh you know Json information or they want to get some kind of information from other apis in the the internet so it is better to mask the IP address of this applications when they send a request so that's exactly what n Gateway does so let's say the IP address of this one is 17216 1.0 or 17263 do4 when the application or the when the ec2 instance tries to access something from internet what this n Gateway says is okay let me change your IP address to the public IP address of mine and send the so that your information will be hidden even if that third party application is a false application or even if it is hacked there will not be any problem because you don't know I mean that person does not know the IP address of this server that is about the public subnet and in the private subnet you will launch these applications using an autoscaling group okay what is Autos scaling group I'll just explain you in a minute finally we will see how to reach this application through the internet traffic flows and we will also I mean I'll also tell you how this applications access internet using the NAD Gateway so this is going to be the complete overview and before I jump onto the video you should know about these four things because I covered these things in the day Zero to D6 but I did not cover them to an extent that it is registered for everyone basan is something that I did not cover at all but the other three things I covered but not to the fullest so let me give you an overview Autos scaling group load balancer they have dedicated sessions in this AWS Z2 hero series but in this project I'll use them so that you should understand the summary of it firstly what is an autoscaling group so autoscaling group is you know you can just understand it as a concept let's say you want to deploy your application in two availability zones instead of creating your ec2 instance two times what you can do is you can just tell the aut scaling group that okay so create minimum of two replicas and in case in case my application receives more requests and if two servers are not enough to load this uh uh two servers sorry uh two servers are not enough to accept the traffic incoming traffic let's say two servers can only accept 100 requests but there are 200 requests that are coming at you what autoscaling group can do is it immediately can take a decision and scale your servers to four in number five in number six in number right so this is a very basic example of Autos scaling group in future we will discuss about Auto scaling group in detail so don't worry about it for now then you have load balancer what is load balancer the name itself says what is load balancer it is something that balance the load let's say there are 100 requests now we have two servers right there is server one here and there is server two here hosting the same application so load balancer takes the responsibility of sending 50 requests here and 50 requests here or if you have this instance very powerful right so you can say send 60 request here 40 requests here 80 here 20 here and load balancer will exactly balance the load right apart from that using load balancer you can also do path based routing host based routing different kinds of routing mechanisms which we'll cover in depth when we come to the load balancer Topic in the AWS Z to Hero series and one final topic is what is basan or a gum server this is a very very simple concept ccept so these ec2 instances right that we are creating are created in a private subnet so they don't have a public IP address or we cannot SSH into these instances directly so what we will do is because we want to keep them secure we will not create any IP addresses public IP address for them but we will create a basan host or just a jump host in this public subnet and through that basan or jump host we will connect to this E2 instance and there are multiple advice ages of using basan host the one mechanism is the same thing that I've just told you if you use basan host instead of directly connecting to the server you can connect from the Bastion so that there will be a proper logging mechanism you can do proper auditing of who is accessing this private subnet you can configure bunch of rules in that Bastion host where the traffic actually moves or the SSH traffic actually goes to this application in the private subnet through the basan host right so we will discuss about advantage of basan host again because this is not the topic where we want to discuss about Bastion but this is a topic where we want to implement this entire architecture right so I think we have enough information now we can get started with the project and let's see how to implement this end to end on an AWS account okay so I have logged into my AWS console and let's start with the creation of VPC so go to the search bar type uh VPC so you will see an option for isolated Cloud resources VPC click on that so you'll find an option called create VPC click on create VPC and there will be two options here one is to create VPC only or create VPC and more if you click on VPC only then you have to create all the things by yourself like for example uh you need to create all the subnet configuration ipv4 IPv6 and a lot of other configuration but the better thing is to go with VPC and more so when you click on VPC and more you will see in the preview diagram what AWS creates for you so AWS creates a public private subnet uh in US East 1A availability Zone and it will also create a public public subnet and private Subnet in Us East 1B as well so if we go back to our diagram which we have been talking about right so as I told you uh you can search that in the AWS black blogs as well just search for AWS uh public private subnet architecture so this is the blog that I was talking about uh which I wanted to share in the description this blog also has the information but you don't have the complete details in this blog like they are very high level probably uh you know someone might not be able to follow this and execute this so I'll explain in much better way so just use this diagram as reference and we can Implement that in our own style the style that we have learned from day 0 to day six so see what we are trying to do here when you click on VPC and more what AWS is creating one public Subnet in Us East one a one public Subnet in Us East 1B which is what we want right one public subnet here one public subnet here and then you have one private subnet uh here in Us East 1 a and one private Subnet in 1B exactly same one private subnet here and one private subnet here so VPC is getting created subnets are getting created and what all so along with the subnets of course the subnets have to be attached with the route table when I explained the VPC architecture I told you route table is the one which defines how to route the traffic within the subnet so if the subnet is not attached to Route table technically nothing happens with the subnet so if you look at the public subnet it is attached with a route table that has a destination as internet gateway awesome that is exactly what we want this public subnet should have a route table with internet gateway attached to it so that traffic flows into the public subnet so that is also done by the VPC here great and finally you have two private subnets which have two different route tables expected because these two route tables are different and it is attached to a VPC endpoint uh for S3 bucket now I don't want this one uh because this project has nothing to do with VPC endpoint and when we talk about S3 buckets uh you will understand about what exactly this is for now just ignore this and I'll also remove this from the configuration so okay VP and more is what I want then uh select the name of the project let me call this as uh AWS prod example use the same subnet uh there is no problem here because uh in this VPC you have 6536 IPS which is too much uh so that's totally fine I don't want to touch it no IPv6 I don't want to play with IPv6 I'm just using ipv4 configuration number of availability zones it is two that I require perfect number of public subnets two if I change this one the reference diagram also gets changed number of private subnets two n gateways yes I want one n Gateway in one particular availability zone so let's say one per availability Zone and this is the VPC endpoint that I talking about just click on none you will notice that this diagram also changed and now there is no VPC endpoint for S3 bucket perfect now let's click on create VPC here and see that AWS creates bunch of resources for you so take a pause here and read what are the resources that are getting created because this is very very important okay maximum number of addresses have been reached the elastic IP addresses that's not a problem uh let me go back and see uh how do you delete that you can just go to the ECT console this is again something uh useful information while you do demos this is the advantage so then there is an issue here while I was creating the VPC so the number of public uh elastic IP addresses is reached so let me go here and release some elastic IP addresses okay so these are some previous projects that I was doing so let me release those IP addresses if the IP address is released then AWS will allocate this elastic IP address to the other things that that I using so now if I go to the VPC console let me click on retry here so you will see at this point of time the elastic IP addresses will also be created what is elastic IP address elastic IP address in AWS is nothing but an IP address that will remain the same even if the instance is deleted let's say you terminate an ec2 instance I'm just giving an example if you assign ec2 instance with an elastic IP address what will happen even if the ec2 instance goes down and comes back the IP address will remain the same in general it does not remain the same but if you're using elastic IP address then the IP address will remain same in very simple language if I have to explain in a plain English elastic IP address can be called as a static IP address because it never changes okay so in this case the elastic IP address in my example it will get attached to the N Gateway because like I explained you what is n Gateway it will mask the IP address of my ec2 instance or the applications with the public IP address of the N Gateway so this is where the VPC is using the elastic IP address perfect let's see the VPC creation so VPC creation takes a little time uh at this stage creation of NAD gateway to activate uh I mean for the NAD gateway to activate uh it will take couple of minutes so you need to wait patiently here uh for the N gateway to be activated so now all of this is created but even then you need to wait for a couple of minutes sometimes okay so sometimes your VPC is created but sometimes AWS takes one or two minutes to reflect this in the VPC tab so if in case in when you are performing the demo when you click on these vpcs and you see that the VPC is not visible here or it is taking time please be patient sometimes it takes time okay now the VPC is created let's go to the VPC and see if the resource architecture that we have created till now okay till now whatever we have created is available or not okay till now what we have created we created a VPC with public subnets private subnets public subnets attached to the route table and the internet gateway and the private subnet is attached to different route tables perfect everything everything is looking fine till now now what are the other things needed so the other things that are needed is the E2 instances where your applications are deployed we will do them with autoscaling group as I've mentioned and the load balancer is also missing now once these things are deployed we will try the traffic from outside but there is a thing that you need to do additionally will'll get there so please watch it carefully okay now let's go to the console again and create the auto scaling group so search for ec2 in the ec2 you can go for auto scaling option just scroll down you'll see this option called autoscaling groups click on create autoscaling group autoscaling group in AWS cannot be created directly you can use launch template so why you need launch template because you can use this launch template across multiple uh autoscaling groups or this template acts as a reference like tomorrow if you want to understand how your autoscaling group is behaving is your autoscaling group scaling one instance 10 instances 100 instances you can use this launch template as reference so let me click on this launch template firstly create that launch template will not have much configuration uh you just have to mention what is the name of this launch template uh let me just call this name as the name of the launch template as uh AWS prodad example let me give the same and uh what will be the name what are you trying to do here basically uh deploying okay verification or proof of concept for AWS private subnet for app deploy in AWS private subnet perfect now it is asking what should be the operating system of your or the you know what should be the Ami not operating system what should be the Ami which image do you want to use ubo SOS let me just pick up what I've been using that is the uento configuration click on the recently launched and uh this is the uento configuration if you want to browse more you can click here what should be your instant type always pick the free instances when you're are doing this proof of Concepts whenever it is required to use an additional configuration I'll let you know in the key value pair pick up the key value pair that you want to use do not touch anything with respect to this uh specific option here the subnet configuration uh let it be uh create a new subnet sorry create a new Security Group okay and uh provide the name of the security group again let me give the same thing AWS prodad example just say Al love SSH accent this is just a description you can give anything and here comes the important thing in which VPC do you want to launch I want to launch this autoscaling group The instances in the VPC that I just created and here you can add any inbound Security Group rules for the E2 instances that are getting created so my application that I'm going to deploy I know that uh it is going to be deployed and access using Port 8,000 but what you can do is you can just open all of the things as well if you're not sure about your application configuration but it is always um you know suggested to keep the port that you are deploying I mean that your application requires so you know what you can also do like I'm saying is just open all traffic but if you open all traffic then you are not following the security standards so what I'll do is I'll open the ports that I require one is the SSH Port that is Port 22 that is the port 22 from which source anywhere uh SSH Port I want to access from anywhere so you know this is one and now add another Security Group for the application that is deployed inside these instances so I'm going to deploy a python application very simple one so I'm will open port 8,000 so custom TCP Port range 8,000 in your case you can change this according to the application that you want to deploy and again let me put accessible from anywhere and I don't want to add any EBS volumes or any other configuration so let me click on launch template you must be feeling abishek this is exactly similar to E2 instance configuration yes it will be exactly same because here you are using the autoscaling group to scale ec2 instances right so once you create that just refresh the page so that autoscaling group will find the launch template Autos scaling group name again let me say AWS example uh prod or AWS prod example what is a launch template AWS prodad example is a launch template and now let me click on the next button here you need to choose the VPC okay which VPC do you want to choose you want to choose the VPC that you have created with right what is it saying for more applications you can use multiple Ava a ability zones okay but in my case I want to choose the VPC that I've have just created availability zones and subnets in which you want this ec2 instances to be available where this ec2 instances should be there if you go to the diagram they should be there in the private subnet so let's put them in the private subnet this is the private subnet right private one and this is the private Subnet in Us East B so I'm choosing two private subnets perfect now all the things are available let me click on next do you want to attach a load balancer now or do you want to attach a load balancer later so I'll not attach any load balancer here uh let's just not go with any load balancer I'll attach load balancer in a different context I'll just create this load balancer okay so I'll create an application load balancer in the public subnet but in the autoscaling group configuration for this project I'm not creating any load balancer so okay health checks everything is good let's click on the next button desired capacity specify the size of Autos scaling Group by changing the desired capacity okay so let's say you want two E2 instances select this as two what is the minimum and how much it can go up to maximum what is maximum like you are saying to start with two E2 instances but let's say during Christmas or during uh Diwali or some other festival occasions if if you receive lot of traffic then autoc scaling group when the scaling when the traffic goes to 80% 90% depending upon the CPU monitoring what you can do with auto scaling is you can tell Auto scaling to automatically go up to maximum capacity of three instances and four instances so you don't have to worry about this right now you will learn this uh when we go to the in-depth topic analysis of autoscaling groups so you can configure the scaling options here that's what I was talking about like when do you want to Define when the E2 instances have to scale up and scale down so for now just keep it none and click on the next button add notifications you can add notifications through SNS topics when an E2 instance is added or terminated for now I don't want any of those things so let me click on next button and launch configuration perfect now my autoscaling group is also getting created again this will take 1 minute so just wait here before you move to the next so after a minute you will see that the desired capacity is two and the instances are two now let's go to the AWS ec2 and see if this autoscaling group indeed has created two instances for us or not and I'll also verify if one of the instance is created in Us East 1A and the other instance is created in Us East 1B or not okay for that let me go to uh ec2 here perfect and uh click on the instances yes there are two instances perfect click on one of the instance where is this created uh see this is created in Us East 1A now let's click on the other instance go back here click on the other instance and see where is it created uh see this is created in Us East 1 B perfect so you have this configuration also done so Autos scaling group is created instance is created here and here now before going to create the application load balancer you have to do one thing that is install the application inside this servers right I want to install applications inside This Server so now what I'll do for that is just go to those instances so here comes one thing that you have to watch carefully and this is very important so let's go to the instance and try log to this instance so you will notice that this instance does not have a public IP address and that is expected right so I have not given any public IP address here because these instances have to be secure now you might ask me but abishek how do I log into this so that's where our basan or the jum host comes to the picture so as I explained in the theory part what does a Bastion or gy host does so it enables it act as a mediator between your private subnet and it acts as a mediator like between your private subnet and the external persons or the public subnet so I'll create a basan host here and access the private subnet from there okay so what we will do is go back to the ec2 console click on the instances and start creating a Bastian host launch instance so this is I think all of us already know just call it as uh Bastian host and uh choose uento as the image you can choose anything then choose T2 micro provide key pair AWS login and what you will do here is make sure you add a Security Group which has access to SSH right so because you need to SSH to the ec2 instance that is the basan host and from there you have to s s to this private subnets and install the application so perfect SSH is added uh sorry where is it yeah so SSH is already there so I don't have any problem now there is one specific thing here called network settings edit and make sure this basan host is created in the same VPC okay so if that is not in the same VPC then it will not be able to access inside the VPC so I changed that configuration and added it to the VPC perfect Auto assign public IP address yes enable without public IP address it will not be of no use now just go and launch this instance okay I will launch the instance and once the instance is launched what I'll do I'll SSH to this particular instance from there I'll SSH to the private host okay so what I'm doing for my personal laptop I'll SSH to basan from basan I'll SSH to the private subnet but to SSH to the private subnet again you want the key value pair which is present on my laptop so what I'll do firstly I'll show you okay so I'll copy my key value pair to the Bashan host as well why because if the Bashan host does not have the key value pair let's say Bashan host is here it does not have the key value pair how can it log to the private host key value pair is in my personal instance so I will use a shell command called SCP don't worry I'll show you all of these things practically so let me refresh the instances now you will see three instances this is Bash and host and these are instances in the private subnet perfect so let me get my terminal this is my terminal and uh what I'll do is I know my SSH key is here uh I've used an SSH key called uh AWS so just search for GP AWS log I think I've used this pem file so firstly I want to copy the PM file definitely I log into this Bashan host but along with log to this Bashan host I also need to log to these instances for that this Bashan host should have the SSH access for these instances right it should have the pem file so firstly let's copy the pem file so open this instance get the public IP address of this instance okay now I'll use this command I'll also try to paste this command in the description so that you can use this is called as a SCP command secure copy so what I am securely copying I am copying this specific thing called AWS login pem which is the pem file for my private subnet instances as well from my personal laptop to where am I copying it I am copying uh sorry yeah I'm copying it to the UB to E2 instance okay so here I'm using the identity file and then I want to copy this file to the instance okay which instance is this this should be my Bastian post instance so let me change the IP address of it it is very clear right what does SCP do SCP securely copies file from one host machine to the different host machine now from where I'm trying to copy I'm trying to copy from my personal laptop to AWS ec2 instance that is the Bashan host so let remove this IP address and copy the IP address of the basan host so you will see what will happen let me just copy say yes and you'll notice that the pem file is copied to the basan host perfect it is copied now let's go to the bastan host and see if it is copied or not how do I log to the bastan host just copy this IP address again say SSH minus I your key value pair pem file location UB to at theate this IP address just do LS you will notice that the AWS PM file is available here if the pem file is not available here you will not be able to perform the next command which is very very important that is I will not be able to log into any of these instances I need to log into one of these instance because I want to install the python application in one of the instance first okay so take the private IP address SSH minus I AWS pem file Ubuntu at theate private IP address see I am able to log in to this specific instance with the private IP address 10 014109 1014 0109 perfect so that means now I'm able to log to the private instance as well I mean what is a private instance easy to instance in the private subnet awesome now all that I'll do is install the python application here so very simple python application uh let me do one thing let me create a HTML page as well index.html so let's pick up a very simple HTML P W3 School tools HTML basic I'll pick up a very random example let's take this one copy this entire thing and put this in my file so that I just want to host this file let me say that my first AWS project to demonstrate apps in private subnet okay let me remove this and now save this file and just run the python server using Python 3 minus M HTTP do server on Port 8,000 okay let me run it on Port 8,000 so my application is running in one of the instance that is 10 014109 so what I've done till now is I've used the autoscaling group perfect let's go back to the diagram I've used the autoscaling group created the ec2 instance created a Bastion host logged into only one of the instance so see why I've logged into only one of the instance because while using load balancer I want to demonstrate that traffic is going to one particular instance it is hitting and giving you back the response whereas when it goes to the other uh particular particular application in a different subnet it is giving you a error response because this page is not available or the application is not available okay so for that purpose I've installed python application in one E2 instance and I did not install in other E2 instance so this is a effective way of explaining you the load balancing concept okay we will see 50% of the traffic should go to one instance you should get the response and 50% of the times it should go to other and it should not get the traffic perfect now what I'll try to do is create the load balancer and attach these instances as Target groups that will be our final stage so again let's go here search for load balancer or go to E2 that's a easy way and in the easy to you have load balancers so there are three different load balancers now you don't have to worry about it what is application load balancer what is Network load balancer classic load balancer Gateway load balancer we will come to that when we discuss about load balancer Topic in detail that will come after day 20 or something so don't worry about it right now let's go with the default uh that is the application load balancer not default I mean let's go with this one click on the create button and what is an application load balancer on a high level it does the HTTP and https which is L7 load balancer layer 7 load balancer so provide the load balancer name let's call it as AWS prod uh example the load balancer should always be internet facing it should be in the public subnet right so we have discussed this multiple times load balancer should be in the public subnet and should have access directly from the internet gateway ipv4 makes sense what is the VPC that you want to provide so you know what should be done provide the VPC that you have just created in which of the availability zones just pick up both the availability zones no problem and it should be with the public subnet right so change to the public subnet even if you put that in a private subnet AWS will give you an error so put both the subnets both the availability zones and put the public uh subnet range now the security group uh you can select any Security Group does not matter like you know or you can create a new security group as well what you are trying to do in a security group is for the load balancer are you allowing all the traffic or not or in the VPC where the load balancer is there are you providing access where in the security group you can Define okay open port 80 open port 8080 whatever it is so whatever I attached here is the default VPC you can remove this one uh the VPC for AWS prod example and launch wizard so if it does not work what we can do is we can create another Security Group and we can attach that another Security Group here so that what will happen is load balancer which is listening on Port 880 should be accessed from the internet what is this Security Group doing it is allowing SSH traffic and Port 8,000 traffic what is this doing uh I'm not sure I need to look into this Security Group so let me remove this as well for now just add this one and if it does not work I'll go to the security group and add Port 80 as well listeners and routing okay so you need to create a Target group where you will Define which instance should be accessible so create a Target group first just like launch template firstly we'll create a Target group and uh what is the target group ec2 instances so which E2 instances do you want to access Target group name just provide AWS prodad example perfect and uh what you're trying to do is you are trying to use the HTTP protocol only to the instances this is the VPC perfect heal check HTTP perfect move to the next tab select the instances this is one instance and this is the other instance one instance has the application other instance does not have the application but it is perfect for the demo later you can also add the instance uh add the application to that instance and see that the traffic is Flowing you can try different things in one you can say hi this is abishek in one you can say hi this is vamala and see how the load balancing is happening targets perfect include as spending now create Target group okay so if you have noticed here what I'm trying to do is I have misconfigured I said that the target group on Port 80 so I need to just go back there is no problem go back to your application and just change the port 80 right so the application that is running I can just simply go here and run the application on Port 80 the reason is that I have misconfigured or just keep it like this and create the target group just modify the target group this is the one right so what you will do is go back to the Target group and here where you have option for Targets this is the port I'm saying on Port 80 but actually the port is 8080 so let me delete the target group no problem and recreate create Target group instances let me do it very fast Target group name AWS Brad example just say put 8,000 where I'm trying to access http1 VPC fraud VPC health check is fine these are the instances which I'm trying to access on Port 8,000 perfect click on create Target group oh sorry just include as spending below and click on the create Target group now the target group is getting created so just wait uh for one minute here as well and what is this target group doing basically it has two E2 instances which it is uh verifying on Port 8,000 now I'll just go back and add this target group to the load balance answer you might not find it at this point of time just wait for a minute or do the configuration one more time okay go to the load balancers create load balancer go to application load balancer create AWS prod example internet facing yes click on the VPC prad example both the subnets and both the subnets using the public thing right and then the security groups like I told you firstly let let me put one Security Group if it does not work I know what to do uh because I'm accessing the load balancer on Port 80 I can simply open the port 80 in the security group because I'm accessing the load balancer on Port 80 Brad example perfect or I can access the load balancer on Port 8,000 because I know this particular Security Group is exposing Port 8,000 but okay for the purpose of video uh just to show you how this thing exactly work I'll just use the port 80 which is default addon Services load balancer tags you have used the uh Target group as well everything is looking fine create the load balancer load balancer is created view the load balancer it will take again a minute so wait for a minute here for the load balancer to provision and once the load balancer is provisioned let's try to access it from the outside world the expectation is when you access the load balancer from outside world you should see that load balancer gives you a response of the application so let's wait for a minute here so now the load balancer is in active state right so all the configuration is created at this point of time and what I've done intentionally I did not create application in one of the uh servers right but all the other configuration is completely created I have load balancer internet gateway public private all the things are available now let me go back to the load balancer and try to access the load balancer you will see that the load balancer is not accessible because the subnet that you have attached to the load balancer does not expose Port 80 I've already told you so let's see that in practice so even if you go to the load balancer you'll notice an error so just scroll down down you will see that okay it says that the port 80 is not reachable why just click on this you it will say security groups for your load balancer don't allow traffic on the listener Port so what you need to do is either go to the security group so this is a security group right click on this Security Group open it and allow the HTTP traffic on it how do you do that just go to the inborn traffic rules edit inborn traffic rule add a new rule called HTTP open port 80 perfect anywhere from the Internet is fine save rules after a while once this configuration is reflected you should see this error will disappear what is this error The Listener where you'll see this should disappear okay so it might take a minute perfect now it is reflected now let us access this particular IP address from the internet and see if I'm able to access the my first AWS project to demonstrate app in private subnet congratulations you have implemented your first AWS project and you have demonstrated a very complicated project the project that is used by most of the engineers as devops I mean most of the devops engineers in their organizations we have implemented this entire concept again like I told you I have intentionally not deployed in one of the instances so sometimes you'll get an error okay so if you see here carefully what is happening is the page is not getting reflected let me do it in two different types okay so Okay the reason why it is going is uh there is a target group here uh which is actively monitoring the healthy Parts uh so for example this is a target group right so what is happening here this target Group which I've have created has a heal check okay and it is only forwarding the traffic to the healthiest ec2 instance so one is healthy and one is unhealthy so what is happening is the load is always going to the healthiest ec2 instance okay so in future videos I'll show you don't worry where I will configure this heal check in a way where it should send irrespective of the heal check like you know you can disable this heal check and send that uh send traffic to the both is instances but for now perfect you don't have to bother about it the project is demonstrated and what you can do to see the load balancing concept is go to the other E2 instance right so we have created two E2 instances right in one of the E2 instance I created python application saying this is my first AWS project go to the other E2 instance in the same way that I've explained and call this as this is my second AWS project okay deploy the application deploy the HTML page with my second AWS project and now when you hit the load balancer once you should see my first AWS project and second time when you hit you should see my second AWS project okay because that time both your E2 instances will be in a healthy State and Target groups will forward one request to one E2 instance other request to other E2 instance so this is your assignment try to do it and let me know in the comment section if you were able to perform the assignment or not thank you so much we'll meet in one more interesting episode of AWS in episode 8 thank you so much see you take care bye",
        "summary": "This summary aims to break down core networking and cloud concepts, specifically focusing on AWS, into an easy-to-understand format for learners. We'll cover fundamental definitions, their practical applications, and how they build up to complex architectures.\n\n---\n\n## Detailed Structured Summary: Networking & AWS Fundamentals\n\nThis transcript provides a foundational understanding of networking concepts and their practical application within Amazon Web Services (AWS), culminating in the implementation of a secure application architecture. The content is structured into distinct, logical sections to facilitate clear learning.\n\n### I. Core Networking Fundamentals\n\nThis section lays the groundwork by explaining essential networking concepts.\n\n#### 1. IP Address (Internet Protocol Address)\n*   **What it is:** A unique numerical label assigned to every device connected to a computer network (e.g., laptop, phone, server).\n*   **Purpose:** To uniquely identify a device on a network, enabling communication, tracking activity, and controlling access (e.g., blocking a specific device from a website).\n*   **Analogy:** Like a house number for a building, or a unique identifier for a person in a large organization.\n*   **IPv4 Standard:** The most common format today, represented as four sets of numbers (octets) separated by dots (e.g., `192.168.1.10`).\n    *   **Value Range:** Each number in an IPv4 address can range from `0` to `255`.\n    *   **Why 0-255?** An IPv4 address is 32 bits long, divided into four 8-bit sections (bytes or octets). An 8-bit binary number can represent 256 unique values (from `00000000` to `11111111`), which corresponds to decimal numbers `0` to `255`.\n    *   **Binary Representation:** Computers understand IP addresses in binary (0s and 1s). For example, `192` in binary is `11000000` (which is `2^7 + 2^6`).\n\n#### 2. Subnet (Sub-Network)\n*   **The Problem:** Connecting all devices in a large organization (e.g., office, university) to a single, flat network poses a significant security risk. If one device is compromised, a hacker could potentially access all other devices on the same network.\n*   **The Solution:** **Subnetting** involves dividing a large network (like an office network or an AWS VPC) into smaller, more manageable, and isolated logical segments called subnets.\n*   **Benefits:**\n    *   **Security:** Contains security breaches to a specific subnet, preventing them from spreading across the entire network.\n    *   **Privacy:** Isolates sensitive data and applications.\n    *   **Isolation:** Improves network performance by reducing broadcast traffic and localizing issues.\n*   **Types of Subnets:**\n    *   **Private Subnet:** A subnet that **does not have direct access to the Internet**. Applications and data requiring high security (e.g., databases, backend servers) are typically placed here.\n    *   **Public Subnet:** A subnet that **has direct access to the Internet**. It's used for resources that need to be directly reachable from the internet (e.g., load balancers, web servers for public websites, Bastion hosts). Internet access is usually enabled via an **Internet Gateway** and **Route Tables**.\n\n#### 3. CIDR (Classless Inter-Domain Routing)\n*   **Purpose:** CIDR is a method for efficiently allocating and identifying blocks of IP addresses. It tells you exactly how many IP addresses are available within a given network or subnet.\n*   **Format:** An IP address range followed by a slash and a number (e.g., `192.168.1.0/24`).\n*   **The Number (`/Prefix_Length`):** This number (e.g., `24`) represents the \"prefix length\" or the number of *fixed* bits in the IP address, starting from the left. The remaining bits are \"host bits\" which can vary, defining the number of unique IP addresses in that block.\n*   **Calculating Available IPs:** To find the number of IP addresses in a CIDR block, use the formula: `2 ^ (32 - Prefix_Length)`.\n    *   `/24`: `2 ^ (32 - 24) = 2^8 = 256` IP addresses.\n    *   `/31`: `2 ^ (32 - 31) = 2^1 = 2` IP addresses.\n    *   `/27`: `2 ^ (32 - 27) = 2^5 = 32` IP addresses.\n    *   `/16`: `2 ^ (32 - 16) = 2^16 = 65,536` IP addresses.\n*   **Common Private IP Ranges:** Certain IP address ranges are reserved for private networks and are not routable on the public internet. This prevents conflicts and enhances security.\n    *   `10.0.0.0/8` (Class A)\n    *   `172.16.0.0/16` through `172.31.0.0/16` (Class B)\n    *   `192.168.0.0/16` (Class C)\n    *   **Educational Value:** Understanding CIDR is crucial for designing and sizing networks, especially in cloud environments where you explicitly define VPC and subnet sizes.\n\n#### 4. Ports\n*   **What they are:** A unique numerical identifier (0-65535) assigned to a specific **application or service** running on a device.\n*   **Purpose:** While an IP address identifies the device, a port identifies *which application* on that device a network request should go to. A single server can run multiple applications, and ports allow distinguishing traffic for each.\n*   **Analogy:** An IP address is like an apartment building's address, and the port is the specific apartment number within that building.\n*   **Common Port Examples:**\n    *   `80`: HTTP (unencrypted web traffic)\n    *   `443`: HTTPS (encrypted web traffic)\n    *   `22`: SSH (secure shell for remote access)\n    *   `3306`: MySQL (database)\n    *   `8080`: Jenkins (common for web applications)\n*   **Access Format:** To access an application, you combine the IP address and port (e.g., `http://192.168.1.10:8000`).\n\n### II. The Journey of Data: OSI Model\n\nThe Open Systems Interconnection (OSI) model provides a conceptual framework for how data travels across a network.\n\n#### Prerequisites for Data Transmission (Before OSI Layers)\n1.  **DNS Resolution (Domain Name System):**\n    *   **Purpose:** Translates human-readable domain names (e.g., `www.google.com`) into machine-readable IP addresses (e.g., `8.8.8.8`).\n    *   **Process:** Your browser/router first checks local cache, then queries your Internet Service Provider's (ISP) DNS servers.\n    *   **Why it's first:** No point in sending data if the destination IP address is unknown or invalid.\n2.  **TCP Handshake (Three-Way Handshake):**\n    *   **Purpose:** Establishes a reliable, connection-oriented session between the client (your laptop) and the server (e.g., Google's server) *before* data transfer begins.\n    *   **Process:**\n        1.  **Client -> Server: SYN (Synchronize):** \"Hello, I want to connect.\"\n        2.  **Server -> Client: SYN-ACK (Synchronize-Acknowledge):** \"Hello, I acknowledge your request and I'm ready.\"\n        3.  **Client -> Server: ACK (Acknowledge):** \"Okay, I acknowledge, let's start.\"\n    *   **Why it's first:** Ensures the server is available and ready to receive data, preventing wasted data transmission to an unresponsive server.\n\n#### OSI Model Layers (7 Layers Explained)\nData flows from Layer 7 to Layer 1 when *sending* and from Layer 1 to Layer 7 when *receiving*.\n\n1.  **Layer 7: Application Layer**\n    *   **Function:** User interaction; defines what kind of request is being made (e.g., HTTP, HTTPS, FTP). Handles authentication and high-level protocols.\n    *   **Example:** When you type `google.com` into your browser, the browser initiates an HTTP/HTTPS request.\n    *   **Managed by:** Your web browser (Chrome, Firefox, etc.).\n\n2.  **Layer 6: Presentation Layer**\n    *   **Function:** Data encryption, decryption, and formatting (e.g., converting data into a standard format for transfer).\n    *   **Example:** When using `HTTPS`, this layer handles the encryption of your data.\n    *   **Managed by:** Your web browser.\n\n3.  **Layer 5: Session Layer**\n    *   **Function:** Establishes, manages, and terminates communication sessions between applications. Ensures continuous communication without re-authentication.\n    *   **Example:** Staying logged in to Facebook or your banking website across multiple tabs or after a brief period. Session information is often stored in cookies.\n    *   **Managed by:** Your web browser.\n    *   **Educational Note:** Layers 7, 6, and 5 are primarily handled by the client application (like your browser) *before* the data even leaves your machine.\n\n4.  **Layer 4: Transport Layer**\n    *   **Function:** Segments data into smaller, manageable chunks for transmission. Defines the data transfer protocol (TCP or UDP).\n    *   **Protocols:**\n        *   **TCP (Transmission Control Protocol):** Connection-oriented, reliable, ensures data delivery, flow control, error checking (used for HTTP, FTP).\n        *   **UDP (User Datagram Protocol):** Connectionless, faster, less reliable (used for DNS, streaming).\n    *   **Output:** Data segments.\n\n5.  **Layer 3: Network Layer**\n    *   **Function:** Adds **Source and Destination IP Addresses** to each segment, transforming them into **packets**. Determines the best path (routing) for packets across different networks using routers.\n    *   **Analogy:** Knowing the exact start and end addresses for a journey, and deciding the most efficient route.\n    *   **Managed by:** Routers.\n\n6.  **Layer 2: Data Link Layer**\n    *   **Function:** Transforms packets into **frames** for transmission within the local network segment. Adds **MAC (Media Access Control) addresses** to identify devices on the local network (e.g., connected to a switch).\n    *   **Managed by:** Network Interface Cards (NICs) and Switches.\n\n7.  **Layer 1: Physical Layer**\n    *   **Function:** Converts frames into **electrical signals, light pulses (for fiber optics), or radio waves** for actual transmission over physical media (cables, Wi-Fi).\n    *   **Managed by:** Physical cabling, connectors, and network interface hardware.\n\n#### OSI vs. TCP/IP Model\n*   The **TCP/IP model** is a more practical, simplified version of the OSI model, primarily used in real-world internet communications.\n*   **Key Difference:** In the TCP/IP model, Layers 7, 6, and 5 of the OSI model are often combined into a single **Application Layer**.\n*   **Educational Value:** Understanding OSI provides a comprehensive, standardized view of network communication, which helps in debugging and designing systems, even if you primarily work with the TCP/IP model.\n\n### III. Virtual Private Cloud (VPC) on AWS\n\nVPC is a fundamental concept in AWS that provides a logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define.\n\n#### The Need for VPC\n*   **Past Problem (Pre-2013/14):** AWS initially launched instances in a shared network space. If one customer's instance was compromised, it could potentially affect others on the same underlying physical infrastructure, raising security concerns.\n*   **VPC Solution:** AWS introduced VPC to provide dedicated, isolated virtual networks for each customer or project.\n*   **Analogy:** AWS acts as a \"wise builder\" creating \"secure gated communities\" (VPCs) within its vast \"land\" (AWS regions/data centers) for different \"companies\" (AWS customers).\n\n#### Core Components of an AWS VPC\nA DevOps or AWS engineer defines and configures these components to build a secure and functional network.\n\n1.  **VPC (Virtual Private Cloud):**\n    *   **Definition:** Your isolated virtual network in AWS. You define its size using a **CIDR block** (e.g., `10.0.0.0/16` for 65,536 IPs).\n    *   **Analogy:** The entire gated community with its own address range.\n\n2.  **Subnets:**\n    *   **Definition:** Subdivisions of your VPC's IP address range, placed within specific **Availability Zones (AZs)** for high availability.\n    *   **Types:**\n        *   **Public Subnet:** Has a direct route to the **Internet Gateway**. Hosts resources needing internet access (Load Balancers, Bastion Hosts).\n        *   **Private Subnet:** No direct route to the Internet Gateway. Hosts sensitive applications and databases.\n    *   **Why Multiple AZs:** Deploying resources across multiple AZs (isolated data centers within a region) ensures that if one AZ experiences an outage, your application remains available in another.\n    *   **Analogy:** Different residential blocks or clusters of houses within the gated community.\n\n3.  **Internet Gateway (IGW):**\n    *   **Definition:** A horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet.\n    *   **Purpose:** Enables internet access for resources in public subnets.\n    *   **Analogy:** The main security gate for the entire gated community.\n\n4.  **Route Table:**\n    *   **Definition:** A set of rules (routes) that determine where network traffic from your subnets or gateways is directed. Each subnet must be associated with a route table.\n    *   **Example:** A public subnet's route table will have a route pointing `0.0.0.0/0` (all internet traffic) to the Internet Gateway. A private subnet's route table might point `0.0.0.0/0` to a NAT Gateway or not have an internet route at all.\n    *   **Analogy:** Internal road maps and navigation instructions within the gated community, guiding traffic between different areas or towards the main gate.\n\n5.  **Load Balancer (Elastic Load Balancer - ELB):**\n    *   **Definition:** Automatically distributes incoming application traffic across multiple targets (e.g., EC2 instances) in multiple Availability Zones.\n    *   **Purpose:** Improves application availability, fault tolerance, and scalability.\n    *   **Location:** Typically placed in a **public subnet** to receive internet traffic.\n    *   **Types (briefly mentioned):** Application Load Balancer (ALB - Layer 7), Network Load Balancer (NLB - Layer 4), Gateway Load Balancer, Classic Load Balancer (older).\n    *   **Analogy:** A traffic director at the entrance of a major area, distributing cars evenly to different parking lots.\n\n6.  **NAT Gateway (Network Address Translation Gateway):**\n    *   **Definition:** A highly available AWS managed service that enables instances in a **private subnet** to connect to the Internet or other AWS services *outside* the VPC, while preventing the Internet from initiating a connection to those instances.\n    *   **Purpose:** Allows private instances to download updates, access external APIs, etc., without exposing their private IP addresses. It translates their private IP to the NAT Gateway's public IP.\n    *   **Location:** Always deployed in a **public subnet**.\n    *   **Analogy:** A special post office that can send mail out from your private house but prevents anyone from sending mail directly to your private house's address; they only see the post office's address.\n\n7.  **Bastion Host (Jump Host / Jump Box):**\n    *   **Definition:** A hardened EC2 instance located in a **public subnet**, used as a secure intermediary for SSH/RDP access to instances in **private subnets**.\n    *   **Purpose:** Enhances security by minimizing the attack surface for private instances. Only the Bastion host needs a public IP and open SSH port from the internet. All access to private instances is then routed through the Bastion.\n    *   **Analogy:** A designated, guarded reception area at the entrance of a secure facility, through which all visitors must pass before being escorted to internal offices.\n\n### IV. Security Layers: Security Groups vs. Network ACLs\n\nAWS follows a **Shared Responsibility Model** for security: AWS secures the *cloud itself* (infrastructure, hardware), while *you* (as a DevOps/AWS engineer) are responsible for security *in the cloud* (your configurations, data, applications). Security Groups and NACLs are key tools for your part of this responsibility.\n\n#### 1. Security Groups (SG)\n*   **Level of Application:** **Instance Level** (applied directly to individual EC2 instances, Load Balancers, etc.).\n*   **Stateful:** This is a crucial characteristic. If you allow *inbound* traffic, the *outbound return traffic* for that session is automatically allowed, and vice versa. You only need to define one direction.\n*   **Rule Type:** **Allow Rules Only**. You specify what traffic is allowed, and anything not explicitly allowed is implicitly denied.\n*   **Default Behavior:**\n    *   **Inbound:** Denies all inbound traffic by default.\n    *   **Outbound:** Allows all outbound traffic by default (except for specific ports like Port 25 for SMTP, which AWS blocks by default to prevent spam).\n*   **Analogy:** A \"personal firewall\" or \"security guard\" right at the door of each house (EC2 instance), checking who enters and exits that specific house.\n\n#### 2. Network Access Control Lists (NACLs)\n*   **Level of Application:** **Subnet Level** (applied to entire subnets).\n*   **Stateless:** This is the key difference from Security Groups. You must explicitly define rules for *both inbound and outbound traffic*. If you allow inbound, you *must also* explicitly allow the outbound return traffic for the session to work.\n*   **Rule Type:** **Allow and Deny Rules**. You can specify both what is allowed and what is explicitly denied.\n*   **Rule Processing Order:** Rules are processed in order by number (lowest rule number processed first). The first matching rule is applied.\n*   **Default Behavior:** The default NACL associated with a new VPC allows all inbound and outbound traffic.\n*   **Use Cases:**\n    *   As an **additional layer of defense** that can override Security Group rules (NACL is evaluated first).\n    *   To implement broad, network-level traffic control (e.g., denying traffic from specific malicious IP ranges to an entire subnet).\n    *   To automate security configurations for all instances within a subnet, reducing manual effort compared to configuring SGs for each instance.\n*   **Analogy:** A \"community-level firewall\" or \"security guard\" at the entrance of a specific block of houses (subnet), checking all traffic before it even reaches individual houses.\n\n#### Key Differences & Interaction:\n*   **Order of Evaluation:** NACLs are evaluated *before* Security Groups for inbound traffic, and Security Groups are evaluated *before* NACLs for outbound traffic.\n*   **Granularity:** SGs are finer-grained (instance-level); NACLs are coarser (subnet-level).\n*   **Rule Types:** SGs are allow-only; NACLs can allow or deny.\n*   **Statefulness:** SGs are stateful; NACLs are stateless.\n\n### V. Project 1: Secure VPC Architecture Implementation\n\nThe project demonstrates building a production-ready, secure application architecture on AWS.\n\n#### Architecture Overview\n*   **Goal:** Deploy applications securely in a VPC, making them accessible to users via a Load Balancer, while allowing private instances to access the internet securely.\n*   **Components:**\n    *   **VPC:** Custom-created with a specified CIDR block.\n    *   **Availability Zones (AZs):** Resources deployed across *two* AZs for high availability and fault tolerance.\n    *   **Subnets:**\n        *   **Public Subnets (2):** One in each AZ, hosting the Load Balancer and NAT Gateway.\n        *   **Private Subnets (2):** One in each AZ, hosting the application EC2 instances.\n    *   **Internet Gateway (IGW):** Attached to the VPC for internet connectivity.\n    *   **Application Load Balancer (ALB):** Placed in the public subnets, internet-facing, distributes incoming user traffic to the application instances.\n    *   **NAT Gateway:** Placed in a public subnet, provides outbound internet access to instances in private subnets.\n    *   **Auto Scaling Group (ASG):** Manages the application EC2 instances in the private subnets. Automatically scales instances up/down based on demand and replaces unhealthy instances.\n    *   **Bastion Host:** An EC2 instance in a public subnet, used as a secure jump point to SSH into the private EC2 instances for management and deployment.\n\n#### Traffic Flow\n*   **Inbound (User to Application):**\n    1.  User's request from the Internet.\n    2.  Internet Gateway.\n    3.  Load Balancer (in Public Subnet).\n    4.  Load Balancer forwards traffic to instances in Private Subnets (via Target Group).\n    5.  Application (on EC2 instances).\n*   **Outbound (Application to Internet, from Private Subnet):**\n    1.  Application on Private EC2 instance.\n    2.  Traffic routed to NAT Gateway (in Public Subnet).\n    3.  NAT Gateway translates private IP to its public IP.\n    4.  Internet.\n*   **Management (SSH to Private Instances):**\n    1.  User SSH from local machine.\n    2.  Bastion Host (in Public Subnet).\n    3.  Bastion Host SSH to Private EC2 instances (using the SSH key copied to the Bastion).\n\n#### Key Supporting Concepts for Project Implementation\n*   **Auto Scaling Group (ASG):** Automates the scaling (adding/removing) of EC2 instances based on demand, ensuring applications can handle varying loads and maintaining high availability.\n*   **Load Balancer (ALB):** Distributes incoming traffic across multiple instances, ensuring no single instance is overloaded and improving overall responsiveness.\n*   **Bastion Host:** A hardened server acting as a secure intermediary for accessing private resources. This centralizes access control and logging.\n\n---\n\nThis structured summary provides a comprehensive overview of the discussed networking and AWS concepts, emphasizing their purpose, how they work, and their relevance in building secure and scalable cloud infrastructure. Learners can use this as a guide for deeper exploration and practical implementation.",
        "title": "Learn Networking in 3 Hours | Networking Fundamentals + AWS VPC Networking"
    },
    {
        "videoId": "M9Kex1ID7GY",
        "timestamp": 1752537010.786812,
        "transcript": "networking is really important for you to know as a devops engineer you will be working and creating systems that should be secured should be scalable with low latency and you can do all of this only through networking so in this video I'm going to explain all the important networking Concepts you should know as a devops engineer not just this I'm also going to share some learning resources to help you cleared your Concepts so without wasting any time let's start with learning networking for called devops engus all right so I'm here on my screen and I have a notion document created with all networking Concepts that every devops engineer should know along with some learning resources so if you want me to share this document do let me know in the comment section if most of you want it I will share this as a PDF on LinkedIn so without wasting any time let's start with networking Concepts that every devops engineer should know starting with OSI model the First Fundamental networking concept that every devops engineer should know is the OSI model and if you are a computer sized student I'm pretty sure you have heard about the OSI or Open System interconnection model which is a set of standards that defines how different computer systems communicate over a network in this OSI model the data that is flowed all the information transferred is broken down into seven different layers starting from physical layer to data link layer to network layer to transport layer session layer presentation and lastly as an application layer to the end user so it starts from physical layer or the physical connection which is the layer one to end user Services which is layer seven and each layer has its own specific role in managing aspects like Hardware addressing routing and application Level interactions so you can read about this here in this Learning Resource which has information about each layer so you can read about it here the physical layer what it does what does the datering layer does and similarly for the other layers to understand different layers in networking this is very important for devops engineer to know because you will be able to understand and troubleshoot Network processes only when you have your ideas cleared about OSI model but there is still a problem this OSI model is great to understand how data is flowed or to have a theoretical understanding of network but it's very challenging to use in practice this is why we use PCP IP model in which the three top layers of OI model are combined into one layer which is the application layer and these two bottommost layer which data link and physical layer are network access in TCP IP layer so now instead of having seven layers we have four layers in TCP I model and it's quite easy to practice and as a devops engineer you should also know TCP IP model to troubleshoot some networking issues because we are using this but OSI is great to understand how is a data transferred or are different things happening in the back end now most of you might have a question what is TCP what is IP so TCP stands for transmation control protocol and IP stands for Internet Protocol and as a devops engineer it is very important for you to understand protocols so what are protocols a protocol is a set of rules that defines how data is transmitted and received between devices in the network for example I am reading this from an ocean block on my browser which is using https protocol it is https because it has SSL certificate or else it might be HTTP so HTTP and https web browser protocols similarly if you send a mail to your friend it uses SMTP protocol so protocol is a set of rules that Define how data is transmitted and received between different devices the two main Protocols are TCP and UDP TCP transmission control protocol and UDP user datagram protocol these two protocols Define how the data should be transferred and these two work on the fourth layer which is a transport layer so if you read here it says transmits data using transmission protocols including TCP and UDP so let's read about them so TCP as we know stands for transmission control protocol and it operates at the transport layer it establishes connection between two devices before data exchanges and it ensures reliable and ordered delivery of information so it will First Connect and the connection is initiated only then the data should be transferred so it breaks data into packages assign sequin numbers and uses acknowledgement messages to guarantee delivery it is connection oriented meaning it sets up maintains and terminates a connection for data exchange so remember connection oriented TCP will first initiate a connection and only send the data after the connection has been set up and it will guarantee delivery next is UDP which is user datagram protocol and it also operates on layer 4 as TCP which is a transport layer and it is connectionless protocol so it's like fire and forget it will send the data but does not care if the data is transferred or if the data has received completely or not so it sells data without establishing a connection providing low latency communication however it doesn't guarantee delivery or order making it suitable for realtime applications like video streaming or online gaming so remember whenever you have an application where the data is very critical and has to be transferred in ordered manner you will be using TCP some examples could be be web browsing email database connections and on the other hand if the data is not so critical and you can handle some data loss you could use UDP which can be running on low latency as well some examples could be online gaming or video calls Etc and I also have an example here so TCP as we all know is connection oriented it is connectionless slow fast reporting errors non- reporting errors and this image here defines that this girl is drinking your water which is going directly in the mouth this is why it is TCP whereas UDP here she's drinking the water but it does not go in the mouth which means it's UDP and if you don't get this joke you might be UDP as well another kind of protoc is IP or Internet Protocol which works on layer three the network layer and it handles addressing and routing to ensure the data packets or information should read their intendent destinations the function of this IP or Internet Protocol is to assign IP addresses to the devices and uses routing table to direct data across networks so we all know that every computer has its own IP addresses and we share information through IP addresses and this is done using Internet Protocol to learn more about TCP UDP or IP you can go through a Learning Resource and if you want me to share this document do let me know in the comment section next networking concept that every devop engineer should know is ports so ports are communication endpoints that allow different services or a device to send and receive data for example we know https is a protocol that is used to access web pages on the browser and https is a protocol but it has a port number which is 443 similarly we have done many projects on this channel and we use different ports for different Services we all know Jenkins run on port 8080 whereas MySQL runs on Port 3306 MB runs on 27017 and similarly other services or other protocols run on specific port number SSH runs on Port 22 similarly DNS runs on 53 HTTP while Z on 80 so a doops engineer should know about vots because they are important in configuring networking settings defining rules for firewalls managing container communication orchestrating services and troubleshooting network issues you don't have to remember all the numbers but you still have to know the most common ones where and if you're not sure you can always serve the internet so port number are communication endpoints for a particular service or a protocol and you can read more about it here on this Learning Resource which is in the document next very important networking topic for devops engineer is IP submitting and side range so this is a process where you divide the used Network into smaller sub networks and we use this a lot when managing networks on the cloud a deop engineer should understand IP addressing subnetting and side notation this knowledge is crucial for Designing and managing IP address spaces effectively for example if I show you I'm here in the AWS dashboard and I'm inside VPC service if I click on create VPC this is the process where I need to allocate the ipv4 side range and you will able to understand or you will be able to assign this IP Ider only when you understand subnetting so subnetting is a technique used in computer network to divide a larger IP network into smaller more manageable sub networks or subnets or you can see this picture here which defines that if you use SL 17 you will have 32 K IP addresses to use similarly if you use SL 18 SL 16 K IPS can be used sl9 will have 8K so when you create side range here let's say 10.0.0.0 sl17 means you get 32,000 IP addresses to use in this particular Network so CER block size must be 16 /6 and sl28 sl28 here means you will least a 16 IP addresses and I have explained this very easily in this YouTube video so you can go check it out understand IP subnetting and side arrange now answer of this how do you think an information is transferred from one place to another on the internet through routing so routing is a process of directing data packets from a source to destination across a network and routers uses routing tables and protocols to side the bath for data transmission ensuring efficient and reliable communication between devices so you can see your routers are using routing tables and and it defines where should the data be transferred from one source to another and there's a diagram here where you have computer a and the router is defining how should the data be transferred to computer B using different networks if I show you this in the cloud let's go back to our AWS dashboard in our AWS inside VPC we have something known as route tables in this route tables would Define where should the data go so if you look at this particular route table which is RDS private route table I have routes created which is saying this particular range should have a target of local but I can create more routes to it and I can click on ADD route option here in this I can Define the uh IP range so let's say all IPS in this particular range should go either to internet gateway or to n Gateway or to Gateway balancer endpoint and this is how you can Define the path where the data should be flowed and this process is called as routing I also have a very nice documentation by AWS which explains what is routing why is it important what is router and also other information on different types of routing so you can go check it out another important concept is DNS and every devops engineer should know what is DNS how DNS Works what are different DNS records so to understand DNS or domain name system translates easy to remember domain names to computer friendly IP addresses but it's not just limited to that you can also help find mail servers balance web traffic among servers youir request perform reverse lookups and speed up responses through caching using DNS for example whenever you buy a domain name which canb website.com it is actually connected to a server's IP address and you as a human do not remember the IP address which is why we use DNS to map our IP addresses with our domain names for example let me show you this is IP address which is actually Google's IP address if I ask you to access google.com you will go ahead and do it by running google.com but I can also do it by IP address and you will not remember this IP address maybe after watching this video so using DNS you can map an IP address with a domain name and it's not just limited to IP to domain you can also do that mapping a domain name to IP V6 address or mapping a domain name to a domain name mapping all subdomains to another domain name and similarly there are other records as well to show you an example in AWS we have a service named as Route 53 which is a DNS service in this I have a hosted Zone created named as my website.com this is my website if I want to create records I can goe click on create records option here in this I can map my domain name to an IP address using a record or I can go ahead and choose different records to do different task as mentioned here in this part so you can translate domain name to IP address find mail servers balance traffic redirect request and a lot more using DNS and this is very very crucial for a devops engineer to know because you will be managing DNS records you'll be creating hosted Zone a lot in your role so to understand this I have excellent resource attached here which is by AWS again and in this you will understand what is DNS or DNS Basics different types of DNS service and lot more so you can also learn more about DNS from here till now we have covered a lot of important networking Concepts that you should know let's go ahead and understand the next important concept which is VPN or virtual private network security is a very important factor these days when deploying application and we use VPN on the networking site to make sure things are very secure so VPN or virtual private network is like a secure tunnel for your internet connection it encrypts your data and Roots it through a server making your online activities more private and secure and this is a diagram here which defines how VPN works if you look here this is the part where VPN is used and this is the part where VPN is not used when you use VPN you have VPN client and a VPN server and the data transferred from the VPN client is encrypted so the data is secure whereas when you don't use VPN it's not so encrypted not so secure so vpns allow professionals to securely connect to a remote server access Cloud resources and perform maintenance task without compromising data security it ensures a private an encrypted connection crucial when dealing with sensitive configurations deployments or infrastructure management Dusk to know more on why you should be using vpns as a devops engineer you can check out this resource which explains five different reasons on why devops needs to use a VPN for stronger security protocol to control who sees what you're doing and for work regardless of location Limitless internet and all the other reasons so you can go and check out this resource not just this if you look at the aw us VPC dashboard we have a VPN section where there are different vpns which are used by companies to ensure security now we are almost at the end of our networking for devops and genius video but along with this different concepts I also have a section which is networking tools and I have listed down important networking tools that you should know as a devops engineer because these are very very helpful tools when you're working or troubleshooting your networking tasks the first is Ping tool or the Ping command and this is is used to check if the website or the server is reachable or not let's say you have an application if it is not working fine and you need to check it out if it is reachable you can simply run the Ping command let me show you an example if I do ping google.com and if Google is responsive or is rebled it should give me back some response so when I do this it gives me back responses like this which means Google is recabled so using pin command you can check out if the service or the website is reachable or not next you have is trace route using test route you can see the different path your data takes to reach a destination so you can see what different paths and also how much time it takes so let's try to run it I'm going to say trace route and H to google.com again so let's see and using Trace rout google.com you can find the different path it took for the data to be transferred also how much time it took so this is another great networking tool apart from this you have netstat using netstat you can list all the network connections on your computer you can also find different processes running so I'm going to open up netstat uh hyphen a is the command you can also add use another attributes different attributes to get the response so when I do net start iph you can see different connections running in my computer and these are all the process ID for them so if I want to stop them I can also stop it I use netstat usually to check different ports that I used and like you can do that by running net stat hyph Tu LNP using this you can find out that the port 80 is used Port 631 is used Port 543 is used so if I'm running an application and if 8 is already used I can simply go and delete the process so this this port is available for me to use next time similar to this we have next which is nmap that is used to discover host and services they offer on your network next we have TCP D to capture analyze Network traffic then we have ip config for Windows or if config for Linux to check different networking information of your computer deck command is used to query DNS main servers for information about host address mail exchanges so if you have domain name and you want to find more information about it you can use the dick command similar to dick is NS lookup which is again used to P DNS servers for domain information then we have wi wi Shack and wire Shack is a very good tool for Network protocol analyzer for troubleshooting and Analysis of interactions between Network component it captures analyzes packets of specific n network interface last you have is I this is used to measure how much time it takes for the data to be transferred so it measures the TCP and UDP performance of a network so these are all the different networking Concepts and networking tools you should know as a devops engineer so this is the end of our networking for devops video I hope this video was informative if you found this video helpful do like this video subscribe to Cloud champ and let me know if you have any questions any doubt in the comment section thank you and have a good day",
        "summary": "As a technical educator, here is a structured and detailed summary of essential networking concepts for DevOps engineers, designed for clear understanding and practical application.\n\n---\n\n## Networking Essentials for DevOps Engineers: A Comprehensive Guide\n\n### Executive Summary\n\nNetworking is a foundational skill for DevOps engineers. To build and manage secure, scalable, and low-latency systems, a deep understanding of how data travels across networks is crucial. This guide covers the core networking concepts and essential tools that every DevOps professional should master, complete with explanations and practical relevance.\n\n---\n\n### 1. The Importance of Networking for DevOps\n\nFor DevOps engineers, networking is not just an add-on; it's fundamental. You are responsible for designing, deploying, and maintaining systems that must be:\n*   **Secure:** Protecting data in transit and at rest.\n*   **Scalable:** Handling increasing loads without performance degradation.\n*   **Low Latency:** Ensuring quick response times for applications.\n\nAll these objectives are achievable only by leveraging proper networking principles and configurations.\n\n---\n\n### 2. Key Networking Concepts\n\nLet's dive into the core concepts you need to know.\n\n#### 2.1. OSI Model (Open Systems Interconnection Model)\n\n*   **What it is:** A theoretical framework comprising seven layers that defines how different computer systems communicate over a network. It's a standard model for understanding network interactions.\n*   **The Seven Layers (Bottom to Top):**\n    1.  **Physical Layer (Layer 1):** Deals with the physical connection (cables, Wi-Fi signals, hardware).\n    2.  **Data Link Layer (Layer 2):** Manages data transfer between network entities on the same local network, handles physical addressing (MAC addresses).\n    3.  **Network Layer (Layer 3):** Handles logical addressing (IP addresses) and routing of data packets across different networks.\n    4.  **Transport Layer (Layer 4):** Ensures reliable data transfer between end systems, managing segmenting, reassembly, and error control (e.g., TCP, UDP).\n    5.  **Session Layer (Layer 5):** Manages communication sessions between applications, including establishment, management, and termination.\n    6.  **Presentation Layer (Layer 6):** Translates, encrypts, and compresses data for the application layer, ensuring data is in a usable format.\n    7.  **Application Layer (Layer 7):** Provides network services directly to end-user applications (e.g., HTTP, FTP, SMTP).\n*   **Educational Value for DevOps:**\n    *   Provides a strong theoretical foundation for understanding network processes.\n    *   Essential for conceptualizing data flow and troubleshooting network issues by isolating problems to specific layers.\n*   **Limitation:** While great for theory, it can be challenging to directly map to practical implementations.\n\n#### 2.2. TCP/IP Model (Transmission Control Protocol/Internet Protocol Model)\n\n*   **What it is:** A more practical and widely used networking model compared to the OSI model. It's less complex and directly reflects how modern networks operate.\n*   **Layers:** It simplifies the seven OSI layers into four:\n    1.  **Network Access Layer:** Combines OSI's Physical and Data Link layers. Deals with hardware addressing and physical media.\n    2.  **Internet Layer (Network Layer):** Corresponds to OSI's Network Layer. Handles IP addressing and routing.\n    3.  **Transport Layer:** Corresponds to OSI's Transport Layer. Manages end-to-end communication (TCP, UDP).\n    4.  **Application Layer:** Combines OSI's Session, Presentation, and Application layers. Provides application-level services.\n*   **DevOps Relevance:**\n    *   Easier to use for practical network configuration and troubleshooting.\n    *   The model used for real-world network implementation.\n\n#### 2.3. Protocols\n\n*   **What they are:** A set of rules that define how data is transmitted and received between devices in a network. They ensure consistent and understandable communication.\n*   **Examples:**\n    *   **HTTP/HTTPS:** For web browsing. HTTPS adds an SSL certificate for encryption.\n    *   **SMTP:** For sending emails.\n*   **DevOps Relevance:** Understanding protocols is key to configuring applications, firewalls, and network policies correctly.\n\n#### 2.4. TCP (Transmission Control Protocol)\n\n*   **Layer:** Transport Layer (Layer 4).\n*   **Connection-Oriented:** Establishes a connection (a \"handshake\") between two devices *before* data exchange begins.\n*   **Reliable & Ordered Delivery:**\n    *   Breaks data into numbered packets.\n    *   Ensures packets are delivered in the correct order.\n    *   Uses acknowledgment messages to guarantee delivery.\n    *   If a packet is lost, it will be retransmitted.\n*   **Analogy:** Like a phone call where you confirm the other person is on the line before speaking.\n*   **Use Cases:**\n    *   Web Browsing (HTTP/HTTPS)\n    *   Email (SMTP)\n    *   File Transfer (FTP)\n    *   Database Connections\n    *   Any application where data integrity and order are critical.\n\n#### 2.5. UDP (User Datagram Protocol)\n\n*   **Layer:** Transport Layer (Layer 4).\n*   **Connectionless:** Sends data without establishing a prior connection (\"fire and forget\").\n*   **Fast & Low Latency:** Does not guarantee delivery or order. No acknowledgments or retransmissions.\n*   **Analogy:** Like shouting across a room; you send the message, but don't know if it was heard or understood.\n*   **Use Cases:**\n    *   Video Streaming\n    *   Online Gaming\n    *   VoIP (Voice over IP)\n    *   DNS lookups\n    *   Applications where some data loss is acceptable for the sake of speed and real-time performance.\n\n#### 2.6. IP (Internet Protocol)\n\n*   **Layer:** Network Layer (Layer 3).\n*   **Function:** Handles addressing and routing of data packets (also called datagrams) to ensure they reach their intended destinations.\n*   **Key Mechanisms:**\n    *   **IP Addressing:** Assigns unique numerical addresses to devices on a network (e.g., 192.168.1.1).\n    *   **Routing Tables:** Used by routers to determine the best path for data to travel across networks.\n*   **DevOps Relevance:** Understanding IP is fundamental to designing network layouts, configuring routing, and isolating network segments.\n\n#### 2.7. Ports\n\n*   **What they are:** Communication endpoints that allow different services or applications on a device to send and receive data. They are numerical identifiers.\n*   **Function:** When data arrives at an IP address, the port number directs it to the correct application or service running on that device.\n*   **Common Port Examples:**\n    *   **80:** HTTP (standard web traffic)\n    *   **443:** HTTPS (secure web traffic)\n    *   **22:** SSH (Secure Shell for remote access)\n    *   **53:** DNS (Domain Name System)\n    *   **8080:** Common for web servers like Jenkins, Tomcat.\n    *   **3306:** MySQL database.\n    *   **27017:** MongoDB database.\n*   **DevOps Relevance:**\n    *   **Configuring Network Settings:** Defining which ports applications listen on.\n    *   **Firewall Rules:** Opening/closing specific ports for security.\n    *   **Container Communication:** Exposing and mapping container ports.\n    *   **Service Orchestration:** Ensuring services can communicate on their designated ports.\n    *   **Troubleshooting:** Identifying port conflicts or blocked connections.\n\n#### 2.8. IP Subnetting and CIDR (Classless Inter-Domain Routing)\n\n*   **What it is:**\n    *   **Subnetting:** The process of dividing a larger IP network into smaller, more manageable sub-networks (subnets). This conserves IP addresses and improves network performance.\n    *   **CIDR:** A notation (e.g., `10.0.0.0/17`) that specifies the network address and the number of bits in the network portion of the IP address, effectively defining the size of the network and the number of available IP addresses.\n*   **Why it's important:** Allows for efficient allocation and management of IP address spaces.\n*   **DevOps Relevance:**\n    *   **Cloud Network Design:** Crucial for designing Virtual Private Clouds (VPCs) and allocating IP address ranges for subnets (e.g., in AWS, you define CIDR blocks for VPCs and subnets).\n    *   **Resource Management:** Understanding how many IP addresses are available in a given CIDR block (e.g., a `/17` block yields ~32,000 IPs, a `/28` yields ~16 IPs).\n\n#### 2.9. Routing\n\n*   **What it is:** The process of directing data packets from a source to a destination across different networks.\n*   **Mechanism:**\n    *   **Routers:** Network devices that perform routing.\n    *   **Routing Tables:** Databases within routers that contain information about network paths and destinations, helping routers decide the optimal path for data.\n*   **DevOps Relevance:**\n    *   **Network Flow Control:** Defining how traffic flows between different subnets, VPCs, and the internet.\n    *   **Cloud Routing:** In cloud environments (like AWS VPCs), you configure **Route Tables** to define target destinations for different IP ranges (e.g., directing traffic to an Internet Gateway, NAT Gateway, or other VPCs).\n    *   **Troubleshooting Connectivity:** Understanding routing helps diagnose why traffic isn't reaching its intended destination.\n\n#### 2.10. DNS (Domain Name System)\n\n*   **What it is:** A hierarchical and decentralized naming system that translates human-readable domain names (e.g., `www.example.com`) into machine-readable IP addresses (e.g., `192.0.2.1`).\n*   **Beyond Name-to-IP:**\n    *   **Finding Mail Servers:** Locating mail exchange (MX) records.\n    *   **Load Balancing:** Distributing web traffic among multiple servers.\n    *   **Reverse Lookups:** Translating IP addresses back to domain names.\n    *   **Caching:** Speeding up responses by storing frequently accessed translations.\n*   **DNS Records:** Different types of records (A, AAAA, CNAME, MX, TXT, etc.) serve various purposes for mapping.\n*   **DevOps Relevance:**\n    *   **Domain Management:** Mapping application domain names to server IP addresses or load balancers.\n    *   **Cloud DNS Services:** Managing DNS records using services like AWS Route 53 to control traffic flow for your applications (e.g., creating A records to map a domain to an EC2 instance's IP).\n    *   **Service Discovery:** Often integrated into container orchestration platforms for internal service resolution.\n\n#### 2.11. VPN (Virtual Private Network)\n\n*   **What it is:** A secure tunnel for your internet connection. It encrypts your data and routes it through a server, making your online activities more private and secure.\n*   **How it Works:** Your device (VPN client) establishes an encrypted connection to a VPN server. All your internet traffic then goes through this secure tunnel, appearing to originate from the VPN server's location.\n*   **DevOps Relevance:**\n    *   **Secure Remote Access:** Allows DevOps engineers to securely connect to remote servers, cloud resources, and internal networks from anywhere without compromising data security.\n    *   **Data Protection:** Ensures sensitive configurations, deployments, and infrastructure management tasks are performed over an encrypted connection.\n    *   **Access Control:** Controls who sees your activities and restricts access to specific resources.\n    *   **Cloud VPN Services:** Cloud providers (like AWS) offer VPN services to create secure connections between your on-premises data centers and your cloud VPCs.\n\n---\n\n### 3. Essential Networking Tools for DevOps\n\nThese command-line tools and applications are invaluable for troubleshooting, monitoring, and analyzing network issues.\n\n1.  **`ping`:**\n    *   **Purpose:** Checks if a host (server or website) is reachable and measures the round-trip time for packets.\n    *   **Usage Example:** `ping google.com`\n    *   **DevOps Use:** Quickly verify network connectivity to servers, services, or public endpoints.\n\n2.  **`traceroute` (or `tracert` on Windows):**\n    *   **Purpose:** Shows the path (hops) that data packets take to reach a destination, including the time taken for each hop.\n    *   **Usage Example:** `traceroute google.com`\n    *   **DevOps Use:** Diagnose routing issues, identify network bottlenecks, and understand network latency paths.\n\n3.  **`netstat`:**\n    *   **Purpose:** Displays active network connections, routing tables, interface statistics, and open ports on a system.\n    *   **Usage Example:**\n        *   `netstat -a`: Shows all active connections and listening ports.\n        *   `netstat -tulnp`: (Linux) Shows TCP/UDP listening ports with process IDs and names.\n    *   **DevOps Use:** Identify which services are listening on which ports, troubleshoot port conflicts, and monitor active connections.\n\n4.  **`nmap`:**\n    *   **Purpose:** A powerful network scanner used for discovering hosts and services on a network. It can identify open ports, operating systems, and service versions.\n    *   **DevOps Use:** Security auditing, network inventory, and discovering unmanaged services.\n\n5.  **`tcpdump`:**\n    *   **Purpose:** A command-line packet analyzer that captures and displays network traffic passing through a specific network interface.\n    *   **DevOps Use:** Deep dive into network traffic for detailed troubleshooting, debugging application communication, and analyzing protocol interactions.\n\n6.  **`ipconfig` (Windows) / `ifconfig` (Linux/macOS - often replaced by `ip` command on modern Linux):**\n    *   **Purpose:** Displays network configuration information for a computer's network interfaces (IP addresses, subnet masks, default gateways).\n    *   **Usage Example:** `ipconfig` (Windows), `ifconfig` (Linux), `ip a` (Linux)\n    *   **DevOps Use:** Quickly check a server's network configuration, verify assigned IP addresses, and troubleshoot local network connectivity.\n\n7.  **`dig`:**\n    *   **Purpose:** A flexible command-line tool for querying DNS name servers to retrieve information about host addresses, mail exchanges, and other DNS records.\n    *   **Usage Example:** `dig google.com`\n    *   **DevOps Use:** Verify DNS resolution, troubleshoot DNS propagation issues, and inspect DNS records.\n\n8.  **`nslookup`:**\n    *   **Purpose:** Similar to `dig`, used to query DNS servers for domain name information.\n    *   **Usage Example:** `nslookup google.com`\n    *   **DevOps Use:** Basic DNS troubleshooting and checking domain-to-IP mappings.\n\n9.  **Wireshark:**\n    *   **Purpose:** A graphical network protocol analyzer. It captures packets and provides a detailed, user-friendly interface for analyzing network traffic at various layers.\n    *   **DevOps Use:** In-depth troubleshooting of complex network issues, debugging application layer protocols, and understanding network interactions in detail.\n\n10. **`iperf`:**\n    *   **Purpose:** A network performance measurement tool that can measure the maximum TCP and UDP bandwidth performance between two hosts.\n    *   **DevOps Use:** Benchmark network throughput, identify network performance bottlenecks between servers or cloud regions, and test network capacity.\n\n---\n\nBy mastering these concepts and tools, you will significantly enhance your ability to build, secure, and troubleshoot resilient and high-performing systems as a DevOps engineer.",
        "title": "Networking Fundamentals for DevOps Engineers | DevOps Networking"
    },
    {
        "videoId": "RqTEHSBrYFw",
        "timestamp": 1752564318.765127,
        "transcript": "hey team welcome to this complete course on Docker and containers my goal with this course is for it to be the most efficient way for you to ramp up on your Docker and container skills starting from the basics and progressing through to intermediate and advanced concepts by the end of the course you'll have the Knowledge and Skills to be productive within a team using containers for both development and deployment of their applications you'll understand why you would want to use containers as well as how to integrate them seamlessly into your own workflow this course is split between the and practice for each section I'll cover the concepts that you need to know using helpful visuals and diagrams before diving into our code editor and solidifying that knowledge with real world examples I've provided a companion GitHub repo Linked In the description containing all of the code and configuration that will be shown throughout the course feel free to clone or Fork the repo to follow along as we go before diving in I do want to highlight a couple of prerequisites that will help you get the most out of this course as the image shows it can be tempting to jump straight into learning about containers and skipping some of the foundational Concepts that they build upon I'll do my best to explain everything in such a way that even an absolute beginner would understand but if you have some familiarity with web application development and how all the different components fit together that will be helpful and make it easier to understand what aspects are container specific and what aspects are just General application architecture concerns also having some understanding of basics L shell commands will be useful as we'll use such commands throughout the course if you don't have these I think you can still get value from the course but some things that I do or say might not make sense so it could be worthwhile to go and brush up on those topics first I've broken down the course into the following learning modules each building progressively upon the last I provided timestamps in the description to make it easy to jump between sections if you're already familiar with the topic or if you plan to revisit this video and watch it across multiple sessions in the first portion of the course sections 1 through 4 we'll explore why containers exist in the first place the problems that they solve and the underlying Linux kernel features that make Docker and containers possible we'll also install and configure the docker desktop application and get our first hands-on experience running some containers for the remainder of the course we'll be focused on building out and containerizing our own web application from development all the way through to deploying it to the cloud this is where the rubber meets the road and you'll learn exactly how software companies use Docker throughout the application development life cycle so what is this demo application we're working with I'll go into it in much more detail during section five and throughout the rest of the course but in summary it's a minimal three- tier web application meaning it has a front end in this case built out using react an API layer we actually have two implementation of this one in nodejs and one in goang to highlight how different languages are handled with containers and finally a postgres database each service is designed to be as simple as possible while still showcasing how they all fit together within a real world application when we get to the deployment section of the course we'll learn how to deploy our containerized application to the cloud within the course I'll be using Docker swarm which is a container orchestrator built into Docker and I've also filmed two bonus videos to highlight additional methods of deploying containers one using Railway and another using kubernetes these are separate from the course and information about where you can find them can be found in the description this should give you a taste of the variety of options that we have for deploying containerized applications before we jump into the course content I want to pause for a moment to thank the sponsor of this video Shipyard because of their generous support I can provide this course to you completely free of charge I'm excited about them as a sponsor because Shipyard is highly relevant to everyone working in the container World their platform offers the easiest way to generate on demand ephemeral environments and other words automatically generating fully functional isolated environments for every pull request you connect your G Hub repo add a couple of lines of code to your configuration files and you can have test environments running in just a few minutes later in the course I'll demonstrate how to use the shipyard platform with the course sample application as a bonus for all the students of this course the team at Shipyard has created an exclusive coupon code for you the first 300 people to create a shipyard account and use the promo code devops directive will get 30 extra free days on either the startup or business tier plans links and instructions are in the description below you should go check out the shipyard platform and consider how it can help your teams deliver better software faster without further Ado let's get into the course now in this first section of the course I really want to uh ground our understanding of why containers exist and why a product like Docker has been so successful uh we're going to start with kind of the motivation and then we'll walk through the history of different virtualization Technologies from bare metal to Virtual machines to containers uh and try to understand how that Evolution has occurred and why containers have become the dominant uh kind of development and deployment mechanism for software applications today and this meme on the right is one that I've seen shared a number of times uh it's this idea that I'm building a software application I get it running locally but then something goes wrong in the process of Translating that local system to the production one and so one thing that Docker does really well is help to make it such that our local environment is as close as possible to the production environment that we're going to deploy to and so that's what this is this is hinting at is we're taking that local environment and just shipping it directly to production now there's two main aspects uh of the software development life cycle that Docker and containers are trying to address uh the first one is on the development side so up front as we're building this application maybe historically there was this long process proc you you join a company and they tell you here's your new laptop uh go ahead and and install all these things run these scripts and get everything configured and now you can run the application locally and do your development uh it was very complex lots of opportunity for human error uh and so then Docker came along and Now setting up your development environment is as easy as running a single command Docker compose up we're going to go more into what that is doing later in the course but it took this whole complex sequence of of events that you needed to uh run and turned that into a singular system and command that everyone can run and be up and running uh in the first hour of their their day and so making all of the different combinations of Windows and Mac and Linux development environments all compatible uh is one thing that Docker does really well and containers in general do really well so that's kind of the the de vment side of things on the deployment side of things uh similarly let's say you would build your application and then you need to to put it onto some server somewhere to run it and there was this uh idea that there's a number of steps that need to happen let's say okay we need to create a server running auntu then we're going to run some configuration software like anible to get all the dependencies configured and then finally we're going to copy our application code onto the server and run it with these configuration options again lots of steps lots of complexity uh and much harder to ensure that it's going to be right every single time what containers do is they bring a single standardized package such that as long as my server has a container runtime and there's a number of those available Docker provides one but there's a number out there all you need to do is take this standard container image and run it with these specific options and that's it so now instead of worrying about the underl operating system and the underlying dependencies as long as we have our system and a container runtime and our container image we can run it and get repeatably reliable results uh over and over and so these two aspects are really why containers exist and you can see how much simpler it gets when we use containers going from this historical system with lots of steps and and potential for error into a much more simplified uh unified approach using a system like container and like docker and so that hints at what a container is I've talked about kind of the standardized package uh but from the docker website a Docker container image is a lightweight Standalone executable package of software that includes everything needed to run an application so you can think of it as this box that contains the underlying sort of os dependencies let's say runtime dependency so if I was writing an app in Python it would have the python runtime with the version that is needed uh it would have any of the Python libraries that I'm using so maybe I'm using SQL Alchemy to talk to a database or fast API to build out an API and then it's also going to have my application code so the specific application code that I'm writing all of that gets bundled into this container image and then the beauty is that we have this standard interface and can run multiple copies of it so let's say I need two copies of this application I can take that image with a container runtime and run a container run two copies run three copies Etc and so I've used the term container and container image it's important to know the difference there a container image is this artifact that has all of the all of these dependencies within it a container is what you run from that image so you take an image and if you think of object oriented programming a container image is like the class and then the container itself that you're actually running is like an instance of that class an instantiation of that class and so a container allows us to have this standardized Packaging that we can then create one or more copies of uh and those copies are going to be the same every single time so far I've been using the term Docker container uh but I want at this point to call out what is known as the oci or the open container initiative and effectively there were a few early players including Docker building out the early implementations of containers and as they became more popular companies realized that rather than having a whole bunch of slightly different but similar things that we're calling containers it would make much more sense to come together and create one standard that can be used across different implementations that's exactly what the cont the open container initiative is a number of companies including Docker Google VMware Microsoft Dell IBM Oracle the list goes on came together uh and created open industry standards around what a container is and how it should be run and so there's three primary specifications that the oci defines the runtime specification the image specification and the distribution specification so the image specification defines what should be included in the image in terms of metadata and the format that it should contain so a serializable file system the runtime specification defines how you would take an image that adheres to that image specification and run it in a container and then the distribution specification talks about how those images should be distributed so think about Registries and pushing and pulling images and so these three things together make up the open container initiative and Docker has a specific implementation of this standard so when I talk about Docker images or Docker container images what I'm referring to specifically is the docker implementation of this oci specification if Docker is the only container tool that you're using it's not all that important to know this I wanted to provide this context in case you start using other tools and know that there is this standard format that can be shared across different Tooling in the ecosystem now now let's walk back in time a little while and understand how we got to this idea of containers hopefully from my description of the motivation uh you understand why containers are useful but it's I think it's also important to understand the history and the context of how we got to where we are today and so before there were containers before there were even virtual machines uh we just had computers uh we now call this bare metal Computing so if you're running your applications directly on the Hardware system itself uh another term for that is bare metal but back long enough ago this was just called Computing right so in this case you have your your physical host your server um the physical Hardware layer at the bottom you've installed whatever operating system you want on top of it then you've installed whatever binaries and libraries are necessary to run your application and then you would have your application one or more applications installed directly uh onto this host machine now what are some challenges with this setup uh first of all you can see that shared layer of binaries and libraries uh it's shared across both of those applications and so if your applications for example need different versions of the same dependency or have some dependencies that are somehow incompatible with each other now we've got we've gotten ourselves into what is known as dependency hell so uh we somehow need to either modify our application such that those dependencies are not compatible or figure out a way to to carve up the system such that we can have both versions of the the libraries available doing things like setting uh your your path environment variables differently for the different processes Etc so that can be quite a challenge um you also because of this dependency management challenge you're more likely to have le fewer applications on each host machine uh and so it makes it harder and harder to achieve High utilization so let's say application one takes one CPU core application 2 also takes one CPU core but we have this server-based uh system that has 64 cores um it could be very hard to sort of utilize that compute capacity effectively because of the challenges uh associated with managing dependencies for for applications you also now if you are installing multiple applications onto the system if you ever have an issue with one of those applications or do something that will mess up uh some underlying dependency now you have this large blast radius meaning uh changes or or issues with one application can directly impact uh the performance of another application running on that same host machine so uh it just makes it much more dangerous to to modify things also the startup and shutdown speed of a physical system is on the order of minutes so uh to spin down that server and reboot it for for some reason uh that can take a number of minutes and so so it's just relatively slow if you want to try to achieve high uptime and and eliminate downtime for your applications in addition to that creating new physical machines uh sort of provisioning a new system is very very slow so depending on my setup if I have my own Data Center and I need to actually order the hardware and get it installed and hookup networking that could be on the order of days or weeks if I'm working with a co- location facility or a provider that offers bare metal systems that provisioning might be on the order of hours um but still quite slow relative to what we'll see as we evolve and and sort of add in this concept of virtualization and so that's bare metal I think most people are familiar with working with bare metal um even if they don't realize it because when you run things on your laptop or your desktop you're installing them directly on the host and so that is this concept of of bare metal however that's not the only way to do things and so these challenges really prompted Engineers to come up with this concept of virtualization and adding virtual machines such that we can eliminate some of those challenges and so in the case of a virtual machine now this outer boundary is still the host the physical system the server uh we've got our Hardware um we may or may not have an operating system running on that Hardware I'll talk about that more in a second but then we have what's known as a hypervisor and so a hypervisor is some combination of software and Hardware that allows us to carve up the physical resources so think CPU cores memory access to storage or gpus other devices it allows us to carve up that pool of resources into isolated smaller pools that we can then install our our systems onto and so this hypervisor kind of gives us this virtual Hardware layer upon which we can then install an operating system install binaries and libraries and install our applications and so now rather than having both applications on the single physical host we now have application one and application 2 each with their own virtual isolated virtual machine uh running on top of that hypervisor and so there's two types of hypervisor type one means there's no underlying OS these are running directly on the physical Hardware um this is beneficial because you don't sacrifice any performance in that sort of os to software virtualization layer uh there there's a small performance hit but much less than if you have an operating system on the hardware and then a hypervisor on top of that uh so things like AWS Nitro so that's the hypervisor that Amazon web services is using to create virtual machines on their servers uh VMware is a very popular virtualization uh company and they have vsphere which is a type 1 hypervisor if you want to just create a virtual machine let's say on your laptop you're probably going to use a type two uh hypervisor uh one such example is virtual box so you can install this program virtual box and then within that you can create virtual machines for various uh operating systems Etc and so how does this compare to those challenges that we saw with the bare metal systems uh one we've eliminated this dependency hell so now each virtual machine has an isolated set of dependencies binaries and libraries such that our application can specify whatever dependencies it needs and not have to worry about the needs of other applications uh because of this we're going to have better utilization so we can carve up a big server CPU into many different smaller uh chunks and each of those chunks can have its own virtual machine and then we can install our applications accordingly so this is what enables a cloud provider you can go on and see oh I can I can choose between all these different machine types they don't actually have all those different machine types installed in their data center they have much larger machine types and you're getting some sliver of that installed as a virtual machine you also we've also reduced the blast radius significantly such that now if if something goes wrong with application one on virtual machine 1 that should be isolated from application 2 and Virtual Machine 2 uh and it should not cause any issues and so because of that we've we've increased the safety of of making changes we've also sped up the speed at which we can start and stop these things so now it's still on the order of minutes probably um but faster than shutting down and starting a physical server and then finally uh uh one area where virtual machines have a big uh bonus over bare metal is that we can provision and decommission them much much faster and so we're all used to living In This Cloud world where we can go onto our favorite cloud provider click the button or issue a command and have a new virtual server ready uh within minutes or if we're done with it we can decommission it in a can also be done and and we can stop paying for it within minutes as well and so that's the sort of the second tier or the second rung along this path of virtualization the next one being containers and so containers obviously are the focus of this course um they change things a bit further still so now in this outer box we've got our host or our which could be a physical system uh it also could be a virtual machine so we can take either of those two uh Concepts from before and we can run containers within that context uh you can see at the bottom we've got either our virtual or physical Hardware with an operating system and then the key piece here that's kind of analogous to that hypervisor uh is the container runtime so this is a piece of software that knows how to take a container image and run it within the host system uh the key difference between virtual machines and uh containers is that virtual machines are running their own copy of the Linux kernel uh and I'm talking only about Linux containers within this course there are windows containers but that is a different topic for a different day and so with Linux containers uh they are actually running they're sharing the kernel with the host operating system uh versus in a virtual machine you have your own oper you have your own kernel and you have a copy of your own operating system that's running and So within a container you're sharing that with the host however we're using certain features that'll go into later of the Linux kernel to help provide uh isolation between the different container processes and so you don't get quite the same level of isolation that you would with a virtual machine you do get quite good isolation if you configure things properly as we can see now we have our two containers both running uh on the same host however they are still isolated and have their own copies of binaries and libraries even though they're sharing that underlying kernel now there's a couple of different sort of types of software related to containers the first category of which I'll call the desktop container platforms so these are sort of container Management Systems things like Docker fall into this category things like podman these are things that you would install on your development system to manage containers manage container images uh run containers uh set up the the necessary networking Etc um so these are kind of the holistic lifecycle systems and then there's much more targeted or specific pieces of software called container run times which include things like container D or cryo and these are very specifically designed to take a container image that aderes to the spec and run it uh with the necessary configuration options to provide that type of isolated environment that that is shown in the image now how does this compare to Virtual machines and bare metal in terms of those challenges we saw before again we don't have the issue of dependency conflicts because they each have their own underlying binaries and libraries uh We've improved our utilization efficiency even further because we now for each virtual machine we had to have an entire copy of that operating system so each of those virtual machine images is going to be on the order of gigabytes whereas container images can be much much smaller because they're able to leverage that underlying OS and so they're generally going to be on the order of megabytes uh I have seen very large multi-gigabyte container images but that's not ideal uh but because of this we're able to utilize the resources of the system much more effectively again we have a small blast radius where we have some level of isolation even if it's not quite as strong of an isolation as a virtual machine we're able to start and stop containers even more quickly than virtual machines so on the order of seconds uh we're able to in this case starting and stopping a container uh is kind of equivalent to provisioning and decommissioning so also on the order of seconds we can bring up a new container from scratch and clear it out as well and then the final sort of key piece I think with containers is that they're small and light enough that we now start to use them within our development environment sure you could use something like virtual box you could create a virtual machine that tries to replicate your uh your production environment but it's kind of slow and clunky and not that many people do some people do but but many people don't containers are light enough uh that now more and more will start to actually run containers and we're going to do this later in the course run containers within our development environment such that we can have high confidence that our our production environment is as similar as possible to that development environment and that just decreases the likelihood that we're going to hit some small nuanced bug uh between the differences there uh and so we're able to develop more efficiently uh using a technology like this and so I I pointed out here that this host can be a virtual or a physical machine and so now we can get a little bit uh Inception with it where we have a host machine with virtual machines running on it and then containers running within uh those virtual machines and so this is a very common configuration that you'll see uh most people who are working with containers in the cloud are going to have something like this where you have multiple virtual machines each running multiple containers uh and so this is just combining all these Concepts together and the real challenge now becomes how do I how do I manage all of these containers across those virtual machine hosts and that's where this concept of orchestrators comes in and so things like kubernetes Nomad from hashicorp or Docker swarm are these systems that are designed specifically to take containers and run and manage and schedule them across multiple nodes and so in this case the node is each of those virtual machines and so when we get to the deployment section of this course we'll talk a little bit more about this but it's just important to know kind of the concepts of hey I'm starting with this bare metal server it my my code has to run somewhere on some physical CPU I'm carving it up into these smaller virtual machines that I'm able to manage a little bit more effectively but then I also want to gain the benefits and the speed uh and the efficiency that a container provides and so combining these different types of virtualization in this way that we see more and more companies using in this day and age this just summarizes most of those challenges and tradeoffs uh that I talked about before uh in terms of dependency management uh virtual machines and containers really shine uh in terms of utilization containers definitely win out uh just because it's much easier to get uh to to pack applications more efficiently uh when running them in containers uh from an isolation perspective uh you can't get any more isolated than than multiple physical hosts if you're running application a on on one server and application B on one other physical server that's the strongest isolation you can get virtual machines still provide very strong isolation and those hypervisors are these very optimized systems with a small attack surface area that have really been battle tested and so I I think isolation provided by virtual machines is almost as good as as those from bare metal obviously there could still be some unknown bug that that causes uh a security issue there but I think most people have quite high confidence in the isolation that hypervisors provides and then containers um this is one area where they are less good than these other options is they do provide some level of isolation we'll talk about the the ways in which they do that but they don't give the same they're not as strong and if you make a configuration mistake the the level of isolation can be dropped significantly so that's why I gave containers a yellow on isolation um startup speed containers win out the ability to run containers within a development environment because they're lightweight enough means that we're much more likely to do so and therefore have stronger parity or have have our development environments and our production environments more similar to one another this control row is one that I haven't talked about as much this is just the fact that you can have full configuration control over the system um in a bare metal context that you might not get in a virtual machine so only you only have control over the features or the specific configuration options that the hypervisor passes through or the the container runtime passes through and so maybe there's some specific type of GPU or Hardware accelerator that is not yet supported by your particular container runtime or your hypervisor in those cases the bare metal system is going to win out because if you need that specific Hardware config that might be the only way in which you can achieve it and that leads into kind of this this next row of performance obviously ly a summary of red yellow green um here can't fully capture the nuances you don't take that much of a performance hit when you move to a virtual machine or container however I think one place that performance can win out on bare metal is because of that control you're able to do things like if you're if you're running multiple servers for example ensure that they are directly next to each other on the same server rack such the networking latency is as small as possible or ensuring that the storage devices that you're using are directly connected to that physical Machine versus accessing them over the network and so there is Performance Tuning that bare metal provides the opportunity to do um but overall the performance is fairly comparable for most use cases across these different Technologies and then the final column there just sort of how challenging it is to work with and so so if I'm if I'm buying and racking my own servers in a facility that's intensely difficult to do and and manage there are now providers that can give me access to Bare Metal systems and handle some of this operational overhead but I think it's still more challenging than virtual machines and more challenging still than containers where I can spin something up and and down very easily and there's all of these services that have container run times such that as long as I provide a compatible container image I'm able to run it within that system and we'll see that optionality uh in in a later portion of the course when we get into deploying our own containerized applications and so now that we have a better understanding of the motivation and history of containers and virtualization Technologies I think it's useful to do a bit of a technology Deep dive and start to understand the the foundational Linux kernel features that software like Docker is using to make the magic of containers happen and provide those isolated environments that we described uh then towards the end of this section we'll actually take a look at the docker system architecture itself there's three technologies that make up the core of uh a container and that is namespaces control groups and Union file system and now namespaces are a a mechanism to wrap a global system resource such that from within that namespace any processes running within the namespace it appears as though they have their own isolated instance of that resource uh and they cannot see things happening outside of their own instance uh so for example on the right here we have a listing of the various types of name spaces at one very good example example is the PID namespace so the process ID namespace within a container it is isolated with with a p namespace such that the process I'm running appears to be process ID number one even though there may be tons of other processes running on the host from inside the container we don't know anything about those host processes and we essentially think that we are the only thing running uh in this system another good one to call out is the username space uh so by using a username space containers are able to map a one user within the container to a different user on the host system so we could be running as a root user with elevated privileges inside the container but by run as long as that container is run within a user name space that's configured correctly that root user inside the container may actually map to a non-root user outside the container uh and that's very important from a security perspective in case there was an issue with with one of our containers and an attacker was able to gain access to it and break out of that container if we had properly set up username spaces then on the host they would not have the same level of privileges that they had inside the container cgroups are a Linux kernel feature that allow you to organize your processes into groups which you can then limit and monitor access to certain resources and so on the bottom left you can see within the file system under proc cgroups we can see all the different cgroups that are available these correspond to different system resources whether they're CPU memory storage devices Etc and using cgroups we can specify a configuration for a particular process or group of processes on how they should be able to access those system resources and so for example we might have one application that is enabled to use up to 30% of the CPU Cycles we would use the cpu. shares cgroup maybe it can use up to 50 megabytes of memory we would specify that with the memory. liit in bytes cgroup and then we could also limit the amount of disk read that it has access to uh using the block IO throttle read bits per second device C group uh now the reason that this is important is that it allows us to avoid what's called a Noisy Neighbor problem where one application uh is very resource hungry and sort of starves the other applications of resources by using cgroups we can isolate from a performance perspective each of our different processes and each of our different applications now the third key underlying technology is what's called a union Mount file system a specific example of which that Docker uses is called overlay FS it essentially allows you to take separate file systems and combine them into a unified view that you can then operate on and so in the image you can see we have a lower file system and an upper file system with various files we got file one 2 a and four in the lower layer in the upper layer we have a different file 2B a new file three and then we've actually deleted file 4 when we combine these layers into the overlay we end up with a a combination of those two things where the upper layer takes precedence over the lower and so we have file one file 2B and file three and so while Nam spaces and cgroups are sort of what make containers possible it's this concept of a union Mount file system that makes containers practical because of this layered approach we're able to share lower layers and cache those layers such that the amount of data that needs to be transferred and stored for container images is much much smaller because many images can share a common lower layer and then only some subset of the upper layers can be modified which enables us to move less data when we need to get a container image onto a system and allocate less space if we're running multiple copies of the same container and so with those Technologies while you can interact with them directly it's software like Docker that takes it and packages it into a really nice user experience so rather than needing to know all the different commands to create control groups and move processes into them or create Nam spaces and and block off the different portions of our system in such a way Docker has bundled that uh into an easy to use desktop application that we can install in our system and get all of that uh with a series of simple commands that we're going to learn throughout this course and so this is a diagram of the docker desktop application uh there's there's a number of pieces here here so let's call them out in turn on the left hand side is the client application and so this is the thing that we will be interacting with there's a a number of elements there's the command line interface so when I go to my command line and type Docker run or Docker pull I'm interfacing with that Docker CLI there's also a a graphical user interface which gets installed uh this is nice for browsing which images you might have on your system uh you can do things like configure how much CPU and and memory and disc space the docker application has access to so there's a number of nice user experience features that the that the guy provides there's also a credential Helper and so this is when we learn more about Registries later on in the course essentially a registry is a place where we can store we can push images to and pull images from and so in order to have a private registry you need some way to log in and so the docker credential helper is a mechanism to help store the necessary credentials to do the authentication for that and then finally uh extensions are a relatively new feature of Docker that are thirdparty pieces of software that plug into the client and provide additional functionality in some way and so that's the client application when you install Docker desktop on your system it also creates a Linux virtual machine uh so if I'm on Mac OS there's no such thing as a Mac OS uh container uh instead Docker uses a hypervisor installs a Linux virtual machine and then within that virtual machine it setss up uh two things it sets up the server host application and so that's known as the docker demon or Docker D that exposes an application programming interface the docker API and so when I execute a command from the the command line uh let's say I do Docker run that command gets passed to the docker demon which is listening via that Docker API and then it executes whatever command that is specifying uh within that server host application so let's say I did Docker run and then specified I wanted to run my container image one I might create it would then go off and create that container 1A let's say I then issued that same command Docker run with my container image number one again I get another container container 1B and so this Docker demon is managing those container objects as well as things like uh networking and and volumes all within this server host application finally there's an optional kubernetes cluster that gets configured and installed uh within that virtual machine this can be a nice to have if you are developing four kubernetes you can leverage that cluster directly without having to uh install and configure a separate cluster I mentioned for Mac OS it's a Linux virtual machine on Windows there's actually two options you can either configure it to use uh WSL Windows system subsystem for Linux as the back end or you can use hyperv as that virtualization layer uh to create a virtual machine I think either of those are viable when we go through the installation and setup in the next portion uh we can talk through that in a a little more detail and then the final portion to call out on the right I hinted at this but a registry is just a place to store your container images and share those container images uh with your team or with the world or with wherever you're deploying uh your containers to and so while it is not a part of Docker desktop it's useful to call out here uh because that Docker demon when it needs to get an image that it's not building locally it's going to go off to some registry dockerhub is a great example of one such registry and pull that image into the the host environment so that it can then run the container from it there's also a little bit of confusion around Docker Docker desktop and Docker engine I've highlighted a few of the subcomponents here that make up what's known as Docker engine and so that is the command line interface so that Docker CLI uh as well as the docker demon and the docker API those make up what is known as Docker engine and that is the open-source core of Docker because the the Docker demon only runs on Linux you can only install Docker engine on Linux you don't get sort of this niceness of the Linux virtual machine and sort of the developer experience that the GUI provides but if all you need to do is build run and interact with containers then Docker engine might be sufficient for your needs in this course though because we're using Docker for development and likely many of you are on Windows and Mac OS systems we're going to install Docker desktop so that we can leverage all of the developer experience niceness that Docker desktop provides at this point I think we are ready to go ahead and install Docker on our systems and start using it one thing that is important to call out is that for personal use Docker is completely free you can use Docker desktop without paying anything uh however if you're using it for commercial purposes and your team has either more than 250 employees or more than $10 million in annual revenue uh it does require one of the paid tier subscriptions so depending on how using it within your company uh it may or may not be free so that's just something to call out before we uh before we install it but for personal and learning purposes it is completely free now to install it we can just go to the docker docs under docs. do.com Docker and so in preparation for this video I actually uninstalled Docker so I'm going to go ahead and pull open Chrome and go to that uh site and so here I am at docs. do.com getd doer it gives us options for Windows Mac and Linux uh so choose whichever operating system you're on and then it will have some additional system requirements uh and information that you're going to need I'll just click Docker desktop for Mac with Intel and it'll start downloading uh it is quite a large file so we'll see hopefully my uh home internet is fast enough down below it has information about system requirements uh for depending on the type of system that that you're running so you just want to go through uh and read these the installation process is pretty much just like any other desktop application here it's downloading that dot DMG file I'll then open it drag it into my applications folder on Windows similarly a pretty straightforward installation process the one thing to call out is the two different options for the back end so uh I described in the architecture diagram how you have your client running directly on your system but then you're going to have this virtualized environment where the docker demon is running and so this is choosing whether you want that Docker Dage to be running within WSL or using uh hyperv as the the backend system if you are on Windows I would probably just go with this WSL backend as that's kind of the default option here so it looks like my installation file has finished I'll drag it into applications it expands from the 600 megab download into a 2 GB install and now I should be able to just go to my applications and click on Docker it's verifying it asks whether I want to trust it or not click open and so now here is that graphical user interface uh one thing that I will call out is you may want to go under settings and then resources and here you can specify how many of your system resources that Docker as a whole should have access to so here it's defaulted to four CPUs 10 gigs of memory I'm going to give it access to a little more memory uh the defaults are likely fine for just getting started but if you find yourself running up against resource constraints modifying these uh resource configurations can be very helpful in in alleviating those constraints in order to have those changes take effect you have to click apply and restart and so I'll do that now now at this point we can run Docker commands and installing that application gave us both the CLI as well as the the docker demon and so let's run our first container um one fun example that we can use is uh the whalsay container and so I can do Docker run Docker SL whales say uh and then the command is going to be C say and I'll just say hey team and so if I run that it's going to go off and find that public whale site image download it from dockerhub so we can see it downloading now and each of those lines corresponds to one of the layers in that overlay file system and so it's downloading each of those images onto the host system such that we can then run a container from that image and we see it took the phrase that I put in here uh and gave us this nice asky art output uh with that phrase built in and so with that we have uh pulled an image from dockerhub run a container from it and provided a custom command so that's kind of the the most basic thing that we can do uh with a Docker image and so that's a fun example but let's do something a little more useful let's say we needed a copy of postgres 15.1 running uh for our application uh we can just issue a Docker run command and then I'm I'm doing two options here one I'm setting the postgress password environment variable uh that's just something that's needed the container won't start if you don't set a password and then I'm also publishing Port 5432 and so because these Docker containers are running on an isolated Network the only way that I'm going to be able to connect to it from my host is to publish that port and so that publish command is saying take Port 5432 on my my my Local Host my system and connect that to Port 5432 inside the container uh I'm then specifying the postgres image and specifically uh version 15.1 uh running on the Alpine operating system again it didn't have that image locally so it's going to pull that from dockerhub once it finishes pulling it then starts up the image and now we have the database system running inside that container ready to accept connections uh just to show that it is actually running and and accept connections we can open up PG admin uh and we can see I've added this Docker database uh and it was able to successfully connect to it uh and we can for example run a query I could do select star from information schema tables uh and run that and we get all the tables from that information schema within the database running inside the container hopefully this gets you excited about the possibilities of what running inside of these isolated container environments can provide in terms of your development life cycle now that we've run a couple of simple containers on our system I want to really dive deep on using thirdparty container images and how we can leverage these within our development process to solve a number of different use cases now dockerhub is a container registry hosted by the company Docker there's over 100,000 publicly available images ranging from things like Alpine and auntu which are different flavors of Linux to specific tools such as engine X which is a a web server and reverse proxy or busy box which is a utility image that has tons of different uh useful commands that you can leverage on your system from within Docker if you go to hub. do.com you can then search the images or browse based on the different filters there on the left and so if there is some utility that you want to use likely someone has created an image for that now before we go off and create a bunch of containers on our system I do want to pause and call out the fact that by default any data that we create or modify within a container at runtime is going to be ephemeral meaning if that container is stopped and removed from our host all those data will be gone now there are two mechanisms by which we can help to solve this uh for dependencies that we want to be in the container when it's started up we'll want to build those into the image itself and we'll look at how to do that um as you can see in the diagram on the right if we remember that overlay that Union file system approach we have all of our different build layers there and those at runtime are treated as readon so we've we're starting with an auntu image we're installing uh some dependencies we're copying in our source code we're running npm install to pull in those node uh those node dependencies all of those layers when we create a container from that image or treat it as readon and then there's just this thin read WR layer on top that's where any data that's modified at runtime will go if there's some additional dependency or configuration change that is needed rather than starting the container and then making that change where that change would live in the the the read WR layer at the top we'll want to pull that into our build process such that every time we create a container from that image it will have the necessary change now the other case with data that we may want to persist are data generated by the application at runtime so this could be things like data that we're storing in a database uh when a user creates or or modifies some uh some aspect of their profile let's say on a website those data we can't incorporate into the image because they're not known at build time but instead we need to store them in such a way that they'll be that they will persist beyond the life cycle of the container itself now there's two mechanisms to do that the first of which is called a volume and so we can create what's known as a Docker volume that will designate a particular uh location within uh the docker host system so it will be within that virtual machine that Docker is running uh located at/ barli doer volumes uh we can then when we run a container tell Docker to use that volume and mount it at a specific path within our container file system such that any data we store there gets stored in the volume and that can be managed separately from the life cycle of the container itself now the second option is a bind Mount it's a where the volume Mount the data is still stored within that virtual machine on the Linux file system with a bind Mount we're actually we're actually connecting our host file system into the Container at runtime so I'm going to have some path on my system uh and then I'm going to mount that into the Container at runtime and now if I modify any files within that path they'll show up on the host file system both of these are viable options Docker suggest using volume mounts by default one reason for this is that because that bind Mount is crossing the boundary between our virtual machine and our host system performance can be much lower if we are doing things that have a lot of reads and wrs and so by default use a volume Mount if you need to have easy observability into the data that's being modified or you want to be able to see the files that are being created more easily then a bind Mount can be a good option also later in the course we'll use a bind Mount to actually Mount our source code into the Container such that when we make changes and are developing on the Fly those changes can be represented within the container file system right away I just covered a lot within this one slide but I created a walkth through in the GitHub repo that we'll do now that I think will really solidify these Concepts so the read me for the fourth module of the course uh so the running third party containers module has all of this within it and so now I'm going to walk through the process here that is outlined in this read me and I think it will make what I was just describing uh much more clear about data persistence so remember our container file system looks like this we have a number of readon layers that that exist in the image and then we have a read right layer at the very top let's do some experimentation with uh running a container installing something in it and seeing how that behaves when we create additional containers from that same image so let's start by doing a Docker run command and we'll pass it the uh-- interactive flag in the D- TTY flag those two flags together mean that when we issue this command we'll get a running shell within that container the-- RM flag tells Docker that once this container process exits so once we exit the container uh it should not store that stopped container it should actually remove that from our system so if we run this it's going to go off and pull that Ubuntu 224 image then it starts the container and now we're running a shell inside that container let's try to Ping google.com from within the container as you can see the pin command is not found it's not installed by default within that UB 2 image to install it we can use the APT package manager we can do apt update that'll go off and fetch all the metadata associated with uh the APT package manager and then we can do AP install IP utils ping I'll say yes and so now with ping installed we should be able to call Ping google.com uh one time that's the- C1 it's successfully pinged Google uh and that's great now I'm going to exit this container and if we want to take a look at the container ID here d61 we'll get a different container ID the next time we create create one so now I'm going to rerun that same command we're going to create a new container from that same image and you see we have a different container ID this container doesn't know anything about that previous one and so because we installed that pin command within the readr container image of the other container it was blown away completely when we exited and Docker removed that container so now that we're in this container again if we try to do ping ping is not found however let's change this experim ment just a little bit so I'll exit this container and now I'm going to run the same command but I'm going to add I'm going to remove this dsrm flag so Docker is not going to throw away the container after it stops and then I'm going to add a name option and so I'll name it my Ubuntu container and now we have a shell inside that container again we're going to do apt update and apt install and that will get our ping utility installed within the container okay so now with ping installed we should be able to use it great and so now when I exit because I didn't have that D- remove flag the container still exists on my system if I do a Docker PS that will list containers but because it's stopped it doesn't show by default so I'll do a dash a and we can see my Ubuntu container was running it was created 41 seconds ago it exited 13 seconds ago but it should this container even though it's in its stop State still has that read write layer associated with it uh so I can uh now restart my container with the docker start command so I'll do Docker start and pass it my container name and then I can use the docker attach command to attach to the Shell running inside that container so now I'm back inside the container and I can ping Google and it works successfully so that just shows we created a container we installed the dependency in that container layer we stopped the container but did not delete it and then we were able to jump back in that same container and that dependency existed again now normally we would never want to jump into a container and modify things like this on the Fly the better approach and the approach that we'll want to use for a dependency like this is we want this to always be there when we start a container from that particular image so instead of starting the container and resisting it uh in the container itself instead we want to build our own image containing that dependency and so we're going to do a much deeper dive on building containers and optimizing those builds in module 6 but we can write a very simple Docker file here and pass it at the command line so this is just saying start from that Ubuntu container image run the APT update apt install and that passes it to the docker build command and then we're going to tag it with this tag my Ubuntu image so if I run this command Docker is going to build a a brand new image based on the original but now this new image is going to have IP utils installed out of the box if I want to run a container based on the new image I'll use the tag specified in the build command and now I don't have to install ping because it's already installed so now I can run ping Google and it succeeds so hopefully that gives you a better idea of how this container file system works and the fact that if we want some data likely in the form of a dependency or configuration to be available at runtime every time we want to build that into the image we don't want to start an image and then make modifications to get it to the state we want and so that's the that's the case of data that we know about at build time and can build into the image directly the Second Use case that I described was Data produced by the application at runtime uh and so like I mentioned this could be data that's going into a database let's do some experimentation to build up some intuition about this type of data again I'm going to create a container from that auntu image now this time I've shortened the-- interactive D- TTY uh you get the same thing in a shorthand version with just - it and so here we have a shell running in that container to simulate uh data created at runtime I'm going to make a directory called my data and then I'm going to Echo hello from the container into a file within that directory and so now if I C my data hello text we can see hello from the container stored in a file on the container file system if I exit the container run a new container from the same image what do we expect if I try to do cat my data hello text of course that file doesn't exist just like when we installed something into the file system it's in that top readr layer that gets Blown Away away with the container but now we can no longer build this into the image because we don't know what this file should say ahead of time so instead we're going to use a volume I can run Docker volume create my volume and that creates a volume and now I can modify my run command such that I mount the volume into the file system of that container so I've added this-- Mount flag it's using the source of my volume which I just created and it's using a destination of my data root SL myy dat now I'm going to do exactly what I did before I'll do make my data oh it already exists because I mounted that volume that directory was already created so I'll Echo just like before hello from container into a file within that directory I'll C it our data exists within that file system but if I exit and then recreate a new container also mounting that same volume we can go into my data and see that hello Tex has persisted across that container destruction uh so the data was stored in the volume even though we stopped the container and Docker removed it from the system the volume is treated separately and so we were able to persist the data across that container boundary now you might be wondering where where does this file actually live um it has to live somewhere on disk and in this case because I'm on Mac OS it lives within that virtual machine that Docker is managing um so it's if I was on Linux it would just be directly at VAR lib Docker volumes because I'm in Mac OS I don't have VAR Li Docker volumes that lives within the virtual machine one way that we can take a look at that is to run this privileged container so this is a Nifty trick that I saw online where I can run this command and it will essentially give me a shell into that virtual machine that Docker is running and now I can navigate to VAR lib Docker volumes I see a whole bunch of volumes the one I care about is the one that I named my volume now if I go into the data directory if I go into the data directory and I see my hello. text and so this is showing in that virtual machine file system where that volume actually lives now this was just a trivial example where I was creating that file and wanted it to persist let's say we were running a database like we were running postgres before we would want to use the same approach with a volume and mount it into the file system of the container where postgres was going to store its data and so by default postgres stores atar lib postgres data and so we would want to modify our Command to use that mounting syntax uh there's an alternative syntax with the- v that does the same thing and this would allow us to store the databased data in a volume and manage that separately from the container now I mentioned that volume mounts were one option and bind mounts were the other let's explore what a bind mount looks like for the same use case now I'm going to take my command that I used before where I had a volume Mount and change the type to bind so now the type is bind my source is my present working directory and in particular the my data directory within my present working directory and so let's just list out to show I have a my data directory here within there there's already hello. text let's remove that so now there's nothing in my data and if I run this command I'm going to mount that directory from my host file system at the present work directory SL myy data and the destination is going to be rooty data on the container so let's Echo something in here blah blah that's going to go into hello. text so inside the container we have blah blah within hello. text if I exit that container I'm now on my host system and if I cat my data hello. text we can see that file was created and stored at the path that I specified I can also modify the host data and that will be reflected inside of a container mounted in the same fashion so I can do I'll add another blah okay so it's three BLS outside the container I'll now recreate my container it'll actually be a new container but still mounted at that same path and if I do cat my data hello. text we see the updated version reflected inside that container and so as you can see bind mounts just like volumes are a mechanism to persist data beyond the container life cycle uh the volume Mount is kind of tucked away inside the the file system that Docker is managing the bind Mount is located on my local system by default Docker suggest using volumes they're slightly easier to manage however if you want easier visibility into the data itself it can be convenient to use a bind Mount just be aware that you can suffer some performance impacts if you're doing heavy read wres into the bind Mount location so my hope is that this has helped to build your intuition around how data is handled within containers uh both sort of the default case then the case where we want dependencies available in the container when it's started and then the third case where data is generated at runtime and sort of how we should manage each of those either by building those data into the image or by mounting a volume or a bind Mount uh to capture those data beyond the life cycle of the container the other thing that I've called out within this read me that I think you'll find useful are specific use cases for leveraging thirdparty containers directly and what by that I mean not needing to build our own container image the first one is databases so databases are always a pain to deal with sometimes you need one version sometimes you need another sometimes you'll have multiple databases that you'll want to interact with for different projects Docker provides a super useful mechanism for spinning up a new container with a different database that you can use and have that completely separate from one application to another from one version to another uh very very easily I think the key considerations are to always use a volume to persist the data um if you have additional configuration that you need so let's say there's a config file that you need to have in the container when it's starting up to to in order to set some specific configuration you can use a bind mount to mount that from your host file system into the container and then finally you'll want to set the necessary environment variables so a number of the the official database images use environment variables to configure things like the root password within the container so earlier when we started that postgres container we had to set the the postgis password environment variable uh there's a number of things like that with these database container images I've laid out a number of a number of different databases and I've tried to capture the specific volume mount and environment variables that you might need to use so we've seen postres before um here I've added that volume amount for the location where the data gets stored uh if you had a custom postr SQL comp file and didn't want to use the default you would modify this command to use this bind Mount so you would have this on your local file system and then you would pass the- C flag uh to The Container when you when you run it and pass it a path to that flag flag so this is as you can see where we mounted inside the container this is that same path and that tells the container upon startup to look there for the configuration file so mongodb the official image it looks quite similar as you can see it's expecting to save the data to SL dat DB so we're we're putting that into a volume uh we're setting the initial root username and password with environment variables and we're publishing the default Port of 277 again if we have a specific configuration file we need to pass in we're going to bind Mount that into the container and then we're going to pass a configuration flag to tell where to look for that I'll let you look at the rest of these on your own time but I've I've laid out the commands for redus MySQL elastic search and neo4j now the Second Use case that I think using third party containers is super useful for is when you need an interactive test environment for something other than what you're running uh on your daily on your laptop so I'm running Mac OS but sometimes I need to test out commands in auntu or Debian or or Alpine and so being able to run this Quick Command and have a interactive shell within that specific Linux flavor is really Priceless like I can test out commands and iterate on them so much more quickly if I'm writing a script that's eventually going to be run in that environment I'll just create a container using one of these commands and then I have an interactive environment where I can test things figure out the exact syntax use the help commands Etc inter inter active test environments for operating systems or for programming runtimes uh we can do something very similar but instead of having a shell in the OS we'll have a a repple in our command line with a particular version of that programming runtime and so if we're if we're testing something that needs python 3111 versus 3110 uh or some minor tweak it's so easy using Docker to get an environment that has the exact versions that we care about for doing that type of test now the final sort of use case that I find super helpful for using thirdparty containers are command line utilities sometimes you just won't have it installed or you don't have the right version and so people have bundled up these utilities and because so many Unix tools are designed to pass data as text from one to another so we can pipe data from one command to the next um we could pass data into these containers and get the output as a stream uh we can use these without actually having them installed on our system so JQ is a utility for working with Json data and so for example uh if I run this command so if I jump back here and we look at the sample data I have here so let's say I have this test. Json it has key1 value one key2 value to if I run this command it's going to take that test Json data file pass it into this JQ image and perform this command so it's going to find the value located at key1 and combine it with the value located at key2 and so in this case I get value one value two as a string and that's the output and so without having JQ installed on my system I'm still able to use that utility directly with Docker yq is similar to JQ but is for yaml there's also command line interfaces from cloud providers such as the AWS command line or the Google Cloud platform command line uh and you can run those in container images uh you will need to mount in your credentials so by default we know that the container is not going to have access to the host file system and so if your credentials are stored at let's say the AWS directory within your home directory or the config directory within your home directory you'll need to craft the the correct volume the correct bind Mount commands to get those credentials uh into the CLI but once you do that it allows you to use different versions of the C very easily and so if there was a breaking change from one version to the next you can leverage that previous version without having to reinstall and modify things if you've already upgraded now this sounds cool but you might be thinking I don't want to have to type out Docker run blah blah blah with all these bind mounts Etc and so so one way that we can improve the ergonomics of this quite a bit is to use two concepts one is a shell function so we can write a function like this that will take this command and essentially invoke it when we run this function we also can use an alias to do the same thing to to close out this section I wanted to pull up a talk that was given by Jesse frel um I think this was in 2015 so this is quite a few years ago uh Docker was much younger in its maturity however she was an early adopter and employee at Docker and used to run all sorts of things uh within Docker and so I think this is a talk that is worth watching for sure and just sort of shows that you can pretty much run anything in containers giving it a watch is really interesting and inspiring and helps to understand sort of this mindset of I don't necessarily need to install everything on my host system she goes a little further than most with this concept uh but I think it it's still eye opening to see the number and types of things that she's able to run in containers so with that we've reached the end of the first portion of the course in which we learned the history and motivation of Docker about the underlying Technologies we installed and configured Docker such that we're now able to run containers and interact with them and then we saw some use cases for leveraging thirdparty container images for different things throughout our development life cycle at this point we're going to shift gears and start building building our own containerized applications I think it's important to realize that learning Docker in a vacuum doesn't actually provide much value and so instead we want to have some realistic web application with which we can apply this technology in this module I'm going to walk through a sample application that I've built for exactly that purpose it's a minimal three- tier web application and by three tiers we have the front the front end or the client that's built in react we have an an API layer application programming interface layer we have one API built in node.js so that's an interpreted language one built in goang a compiled language we have both of those they're almost identical but I wanted to show two different languages with two different developer tool chains so that we can showcase how Docker handles those slightly differently and then we're going to have a postgres database I've tried to keep each of these components as simple as possible while still being representative of the type of application that you might see in a modern web technology company today the implementation of each of them will be quite minimal but how we configure things like networking and passing credentials will be realistic to an application at a web technology compy at this point I'll jump over to my code editor and walk through this code at a very high level I'll point out though because this course is focused on Docker and containers and not on web application programming I'll keep this relatively high level just to give you an idea of the things that the code is actually doing and highlighting the configuration surface area where Docker is going to interface with them at this point if if you haven't already you'll want to go to the companion repo for this course I'll have a link for it down in the description uh and you can find all the source code that we'll be going through and building out uh and what you'll want to do is here under code uh you can either click here and clone it to your system but even better is if you Fork it because that way you can make modifications commit those modifications and if you have any improvements you'd like to make uh and propose back to the course for others to use you could create a poll request back to my copy of this repo on get Hub and that would be awesome while you're here if you don't mind giving it a star that would be super helpful to help the project grow and while I'm asking for your support you may as well hit the thumbs up button on the video as well and so with that let's jump over to the text editor and see this demo application so I've got the code editor up here uh and we're in module 5 the example web application I'm going to start by running the database just like we did in module 4 I'm going to run the database with a Docker run command I'm going to pass it a postgress password environment variable I'm going to use a volume Mount like we talked about to persist the data outside of the container in this volume so it can live beyond the life cycle of this particular container and then I'm going to publish Port 5432 which is the default Port that postgres runs on and I'm using postgres 15.1 uh and in particular the Alpine variant so this is a make file each of these is called a make Target we can do make run post Gres and now we have that database running in a container container I can do the docker PS command to list out the containers running and we can see here's the container this is the image it was based on uh because I didn't specify a name it generates a random one and we're publishing that Port so that I can connect from my local host at this point let's move and take a look at the node API and get that running as well so it's located within this API node uh subdirectory uh if we look in the package.json file we can see there are just a couple of dependencies we have Express which is a package for building rest based apis we have Morgan which helps us configure logging uh for that API a little more easily and then we have the PG package or the postgres package uh which is going to allow us to create a client and connect to the database so what I'm going to do here is make sure I'm running the right version of node so I have NVM installed so I can do NVM LS it'll list out the different options that I have installed on my system I want 19.4 so I'll use nbm use node 194 and then I'll do npm install it's going to grab those dependencies from the package.json and install them in my system in this node modules folder if I wanted to run the application I would do npn run Dev but I also need to pass it one environment variable and that environment variable is the database URL so this is going to give the application the information it needs to connect to that post Chris database running in the container and so again I have the same I have this command laid out here and so I'm just going to do make run API node and now it's started up and listenting on Port 3000 let's just go see how that application behaves if I load it up we see uh at the root path so I haven't specified any path it Returns the current time stamp and I'll show it I'll show you how this is working behind the scenes but it's just querying the database asking for the current time and then returning that and then I have the API node key and value just to show this is the node based API we'll have an an almost identical response from the goang API except this will say goang instead of node for each time we refresh the page uh we're seeing there's a a 200 response that's a successful HTTP response uh for the request and then we have this 404 for the favicon so because I'm loading it from uh from a browser it's also trying to load that little icon that you would see in the top left corner but it doesn't have one so that's the the 404 not found error that we're seeing there now let me just look very briefly at the source code for this application to show you what it's doing we've got two primary files index.js is the main one we're importing some dependencies here at the top we're setting the port number that we're listening on defaulting to Port 3000 uh we're setting up some logging and then here's where we Define that API endpoint where we call our get date time function which I'll show you here in a minute uh and then we populate the uh node we populate the API key within that Json object as well uh in our database do our db. JS file uh is where we have that function uh it's where we're going to load in our connection our credentials so that we can connect to the database instantiate a new uh pool which will be that database client uh and then make the necessary call to the database to get the time that then we we can return on that endpoint and so that's pretty much all there is to this node application like I said I want to keep this as simple as possible while still highlighting the types of networking and configurations that they would we would need in a real world application uh at this point with the node application running we can jump uh we can move on to the the goang application and so for this one it's located at the subdirectory API goang uh here I am within that directory uh to install the dependencies I would do go Mod download uh first I'm going to set the go path just to tell it where to download those modules to to set a workspace um I've got this go workspace subdirectory uh I can export go path and that will point to that subdirectory now if I do go Mod download it's going to look for the dependencies that are in my go mod file and so the two dependencies that I've added here are Jin which is a an API framework work uh and PGX which is a postgress client and so that go Mod download looked in go mod brought those into my workpace okay so now we should have all of our necessary uh dependencies uh and just for reference I'm using uh go19 and so to execute this program I would do go run and then point it to the the main.go file uh that's going to both build the application and run that built binary um again just like the uh node application I either need to pass it an environment variable with the database URL or pass it a file containing those credentials um I have a command in my make file that does just that uh so I can do make run API goang we see it passes that database URL in as an environment VAR variable and then executes our go L and then does gun main.go uh that's running on port 8080 so I can go here and reload this and very similar response to the no l to the node.js one uh we have API goang and then it's grabbing that time stamp from the database each time I refresh it gets an updated time and so just very briefly uh I can take a look and show you the implementation here I'll pull open the the main file uh as you can see we import our dependencies here at the top uh we then have our init function uh within there we're loading in the credentials that we need to to connect to the database once we do that uh we pass those credentials uh to this init DB function which is defined in the database module that I'll pull up here in a minute and then within our main function we have our endpoint defined where we uh call our database. getet time function and take the results of that and we've returned that Json object using Jin I'll just pop open the database module here just to show you the two functions there um we've got our init DB function where we're taking our connection string uh with those credentials uh and initiating a client uh and then I have this get time function which just like in the node implementation is calling select now is now getting those data back and and returning that uh to the caller and so that's really all there is to this application like I said almost identical to node we just want to have both of these different languages so we can show how to manage the different tool chains how we're going to build the docker container images and how we're going to create containers from those images now with the goang API running the final piece of the puzzle is going to be the react client uh so that implementation is here in the client react subdirectory uh once again we can look at the package.json uh we have a handful of dependen we have react and react Dom those are installed by vit when I created this project I used V to do so and then I added axios as well as this react query package which just enabled me to very easily call my apis get those data and feed that data into my react uh components uh so I'll want to once again just make sure I'm using the right version of node I want 19.4 great I'll do an npm uh make sure I'm in the right directory then I'll do npm install this goes off and downloads uh all those dependencies brings them into my my local um node modules folder and then to run it uh I'll just do npm run Dev and by default uh V is going to run that development server on Port 5173 so if I load the page here uh we see the result it's a react app it calls both of those apis gets the data and returns them each time I refresh you can see that the the time stamps are updating and so that's doing exactly what we wanted to do once again I can give you a quick look at the source code just so I can show you what it's doing all the logic pretty much is in this app.jsx file we're pulling in our dependencies at the top I have this function based component called current time that uses the use qu method and axios to make a get call to the uh API that's specified and so this enables me to use the same component for both those apis by passing the API in as a prop I then get the data back and can use that within my component then down here this is the actual content that's going to be displayed I just have which API it's coming from from data. API and then the time stamp that was returned in data. now while it's still fetching it will just it will have this placeholder up dating text here at the bottom is the content of our app itself uh we wrap it in this query client provider uh that just enables us to use uh the functionality within uh that react query package we've got our static header and then two instances of the current time component uh one where we're passing it the goang path one where we're passing it the node path it is important to call out here that within my V.C config I have these proxies set up such that API goang is sent to Local Host 880 where the goang application is running and then API node is passed to uh Local Host 3000 where the node API is running that's pretty much it for this configuration hopefully that didn't seem too overwhelming uh it's quite complex trying to run all of these Services independently and get them all talking to each other that's kind of the whole point of this course right I want to avoid having to set up each of these Services separately and avoid the complexity of needing to match the right version on your system versus my system and Mac versus Linux versus Windows docker's going to help us simplify this dramatically and so as we progress through the rest of the course we're going to take these components containerize them and then learn how to run those containers how to configure them properly and how to set up a development environment such that we can very easily go from zero to rapidly developing and iterating on our applications all within a containerized setup and what I really tried to do with this application is have each component be the absolute minimum complexity while still giving us that application the networking the configuration be representative of what you would see in a modern-day web technology company and so ideally don't get bogged down with any of the particular applications themselves but just try to understand here is the core of what it's doing we have our client on the front end it's calling those two apis so we need the networking to be set up properly there those two apis instantiating clients and connecting to that database and making a database call so we need to handle those credentials properly as well as the networking between them and just generally we need to have our environment set up for each of these applications to run as desired now that we have our example application we can go through the process of containerizing it now what that means is that we're going to create a specification and then we're going to build from that specification our container image the specification is what is known as a Docker file so it's a text document that's going to have all the commands that we need to create and assemble this image you can think of it almost like a recipe for your application you're going to start with some base layer like an operating system you install your language runtime and any dependencies and tooling associated with it then you would install your application dependencies you might set up your execution environment with certain environment variables and then finally at the very end you're going to run some command that starts your application now this Docker file is paired with what is called a build context we write all of our instructions in the docker file and then the build context is generally a folder or a directory on your local system containing all of your source code it can also be a URL so you can pass uh like a GitHub a public GitHub repo as a build context and we take these two things together and run the docker build command and with those information Docker is able to go off follow the instructions in the docker file and produce a container image I'll call out one additional file that's important to to know about and that's a Docker ignore file within your build context you can include this dot Docker ignore file as the name suggests you can tell Docker to ignore certain files so let's say we've installed the node modules locally we definitely don't want to copy those into the Container image because we're going to install them within the docker file and we could have incompatibilities from the installation on our host system to that within the container and so with this as sort of the highle framework let's go ahead and start writing some Docker files for our example application as we go about writing our Docker files this page within the docker documentation under docs. do.com engine reference Builder lays out all of the valid commands that you can use within a Docker file the format is actually relatively simple if we use a hash that's a comment uh we'll there's a Convention of using all caps for the instruction and then we'll pass arguments after that instruction so for example we can have a comment saying what this step is doing then we can use the Run command which is going to run this during the build phase and then Echo something out if we scroll down here on the right you can see all the different options that you can pass with the Run command as well as all the other commands that are valid we're going to use a number of these and I'll try to call out all the ones that you're going to need to know along the way so let me just jump into the code editor and I will create a Docker file in my API node subdirectory and you might notice that uh I have the 05 module as well as the 06 module I have a whole bunch of Docker files here that show kind of the sequence of events that we're going to go through uh but just for purposes of of demonstrating building it out I'm going to put it in the 05 directory and generally you would want to do this You' want to put your Docker file in the place where your code is living let's think about what that recipe might be for my node.js based application I need to start with some uh Linux distribution and so so I'm let's just use Ubuntu so I'll use the from instruction and then I'll specify an image name so Ubuntu uh we also then are going to need to install the nodejs runtime uh and I can use the APT package manager to do that so I use the Run instruction which is going to run this command uh during the build phase I'll do apt update uh that'll grab all the latest metadata from the package manager so that I can find the correct package and then I'll do run apt install nodejs um and let's just execute the build now just to make sure these steps are working properly I'll navigate to API node and then to build this I can just do Docker build with a period I didn't specify a name of a Docker file because it's the default is to use Docker file with no extension I pass my current directory of period as my context uh so it looks like uh it failed because that apt install command uh is interactive and so I need to pass it a Das Das a-y and that'll override that question and automatically install so let's try building it again now it looks like it's installing nodejs and you can see each of our steps we started one of three we we pulled that Ubuntu Public Image then step two of three we ran that run apt update and then in three of three we installed node.js and so if we think back to that union-based file system each command in this Docker file is adding a new layer to that system and so often times we want to minimize the number of layers and think about because Docker is caching Those Behind the Scenes how we can avoid invalidating the cache one such method that we can do to improve this already uh would be to run these on the same line so rather than rather than doing the APT update and then the APT install we can combine them into a single command here and so now those two actions will get combined on that single layer okay great uh so now how are we going to get our application code into this container image and for that we're going to use the copy command so this allows us to copy uh either from our host system uh or later we'll see how to copy from another container image during the build and so I'll copy and I'm just going to copy my entire current directory into the image we're going to improve upon this later but uh but this is just a simple way to do it for now and so with that we'll have our source code within the container then I can run the npm install and that will go off and grab all of my dependencies so let's just make sure this still builds okay so npm was not installed so we also need to install npm okay and now that we had npm installed in the system we're able to execute our npm install command and then the final step that we need to do for the stalker file is to add a command that will be executed at runtime when we create a container the format for this is we'll have an array of strings and so in this case I'll do npm run Dev and that would run my Dev script from the package.json file uh when we started up so I'll build it once more and one thing to note here is that I changed something in my Docker in my Docker context and now when I'm rebuilding it it's executing run npm install and that's because this copy step came before the npm install and so making that modification actually invalidated the cache and so it was much slower to build because we had to run npm install and download all those dependencies we'll address that in a future Improvement now in order to be able to execute this this Docker build C this Docker build command builds it but doesn't actually tag it with any name and so I could run this I could reference it with this shot directly or I can add a-t and I'll call this API node and call it version zero now I can do Docker image list and we can see API node version zero was built a minute ago and look it's almost a gigabyte in size now this Docker file while functional is terrible uh there are so many ways upon which we can improve it and now let's go through and think about what those are first of all starting from a to and then installing uh node and npm while we can do that there are a bunch of official images on dockerhub for language run times so if I go to dockerhub and SE search node it'll pull up the official image and so this is a Docker official image it has that tag it's been downloaded over a billion times and has over 10,000 Stars uh and they maintain all of these different tags for different variants and so if I do this I can remove this install command and instead go straight from node and so let me build that again as version one and so for any runtime any language runtime there's probably an official image that will maintain all the different versions you need and have things preconfigured out of the box so usually you can start from node or from python or from goang and we'll see that when we go to the goang API as well now you'll also notice that I don't have a tag specified here I have the image name but no tag and what that'll do behind the scenes is it just assumes that I mean uh node latest um that's a problem because that node latest tag can change based on the images that that the maintainers of that image have pushed most recently and so rather than tag it with latest it's much better practice to specify an exact version um and so what I'm going to do is look here here at the versions uh Alpine is a Linux distribution is quite small and therefore good for building Docker images because as we're thinking about final image size That Base image that we're building from that's as small as we can get and so it's really important to choose a small base image it's also good from a security perspective to have fewer utilities and dependencies installed from the Geto and so instead of using node with no tag or node latest I'm going to use node 19.6 Alpine and so I'll build that once again and I'll call that one API node 2 and just to show you what a huge difference that makes in terms of image size we can do Docker image list and scroll up here and we see node zero was the iunu based one uh this tag number one was the just node latest version so that's over a gigabyte and then and by using that Alpine base image uh We've cut our image size by a factor of five and we're down to 200 megabytes however you will noticed that each time I'm building it's taking quite a while and the reason for that is I'm copying in my entire Source directory before I do the npm install so I keep invalidating the cache and so the way that we can get around that is that rather than copi the entire Source directory right away uh we can first copy in only the package.json and package lock. Json files and then do npm install and after that copy the remainder of our source code by doing it this way this layer will only be invalidated if we change our dependencies and so I'll call that uh tag number three it's also best practice to specify what's known as a working directory within the image that's the default directory in the file system where commands in the build process are going to be run or the command at the end that we're running is going to be executed when we instantiate the container uh we do that using the work dur instruction and this is going to depend on the convention of your language or framework that you're using uh in this case for node apps I like to use user Source app that's not really going to change anything about our system except that now within the image instead of copying all of our code to the root directory and installing the node modules there it's going to copy things to this path which just keeps things a bit more organized also I have things within my API node directory that I don't want to be copied into this image um there's two things that I can do to help avoid that one I can add this Docker ignore file and in this case I'm specifying the node modules directory so I don't Mo I don't want the node modules that I've installed on my Mac OS system to be copied in because that would cause conflicts and so by putting it in this doer ignore file those will not get copied in I also can specify let's say I only want to copy in my source code directory I don't need to copy in the remainder of this stuff into the image and so I can do that here I'm only copying the source code directory into the Container image let's build it once more we'll call this four now because this is eventually going to be a production container image I don't want to be running npm run Dev here instead I want to run node and then point it to my index.js file and so rather than execute this npm script I will just say no node and then index.js and so that index.js file on my host system is in the S Source subdirectory but because I'm copying from source to my working directory it will be directly it will be in that working directory and I can just call it that way so I'll call that one five and now most of the improvements that we've been making so far either have to do with image size build speed uh Etc another key aspect that we can influence by changing and improving our Docker file is the security of our container and so one very useful technique is to execute as a non-root user and so this nodejs official image happens to have a user defined in it called node and so by by using the user argument and passing it the name node it will use that user now that I've done that though I want to make sure that the files I'm copying in here are owned by that node user so I can use the CH own command node node and now all those source files being copied in will be owned by that user and be accessible by that user so I will copy this tag this as number six now many nodejs applications look for an environment variable called node EnV and so we can use the EnV argument or instruction uh and set node n and we'll set that equal to production if it's set to production the application or the package will behave one way if it is unset or set to anything other than production it will run in a development mode and we don't want that for a production image um so by specifying this environment variable here that can be very useful I'm also going to change from an npm install command uh to an npm CI command and I'm going to add the uh only equals production option and so these two things together the the npm clean install is what CI stands for uh will look for the specific versions from my package. loock file and install exactly those versions so it gives a much more repeatable build and the D- only production option says don't in don't install my Dev dependencies so I have things like just and nodemon in my Dev dependencies within the package.json since I don't want those or need those in my final production image uh I can use this-- only flag uh to achieve that so let me call this number seven and now I know as the person who wrote this app that it is expecting to run either on an environment variable named Port that it loads or it will default to uh Port 3000 I'll add a command called expose uh and just say 3,000 and so this doesn't actually change anything about how the container image behaves or the container this is just documentation for users of this image that it is expecting to run on Port 3000 that way they know to publish that port to The Host or to whatever system they need to so that they can get traffic to the app and now there's one additional place where I can make improvements and that's in this step and this again is going to be to improve our build speeds there's the option to mount a cache location to store information from this step specifically so I'm adding this Mount type cache with a target of this location uh and then I'm setting I'm telling npm to use that location to store its cash dependencies um before calling my npm C call and what that does is if previously if I invalidated the layer 7 it would download all of my dependencies from the internet from npm and then install them now if I've run that once before on my system it will only have to download the new dependencies from the internet it can use that local cache to install the rest of them from that cache avoiding an extra download and just speeding up the process and so I'll call that API node 8 and so with that we have a Docker file that I'm pretty happy with we took that initial naive implementation and improved it along so many dimensions both from repeatability size of the image avoiding invalidating the cache so that F future builds are faster documentation around ports and this the fact that this is a production image using the production command that makes the most sense and using a non-root user within the container now let's take a look at sort of the image sizes as we progress there I'll grip for API node and we can see we started off with those huge images uh when we move to the Alpine based node image as our as our base it cut it by a factor of five uh I believe this small jump was when we when we added the Das Das only production option to our npm install command so there were 30 megab of Dev dependencies that were previously getting installed and then the the change we made there at the end where we set the cach for the npm system uh saved us a couple of megabytes so huge improvements along size security Clarity and this is a Docker file that that I would be happy to use now if you look in the repo on GitHub you won't actually see a Docker file in module 5 that's because I've included them here in module 6 and I've got each of the sequential steps that we took ining it with comments describing the commands that we built up along the way now that we have a container a Docker file and a container image uh for the nodejs API that we're happy with let's go ahead and shift gears into the goang API and build one out for that now similarly to with node I could start from auntu but instead I'm going to use the official goang image and because we're doing it the naive way we're not going to tag it we're going to use that latest uh we're going to use that latest one and then we'll come back and improve just to see how much to see how much that saves in our final image size but yeah we can start from that golang image and so I can build it with just that from command this image would be identical to the the Upstream base image at that point like with node we want to set a working directory uh in this case I'm just going to set it as app we need to get our source code into the image so again we use the copy command let's just copy it all we're going for the the simple naive implementation first and then we're going to improve upon it to get the dependencies installed uh we'll use the Run command with go Mod download so that will look at our go mod file and download those dependencies and finally we can set our Command and for this I'll use go run and then point it at my main.go file uh so this should be a a functional Docker file and there we go let's do a a Docker image LS and we'll GP for API goang okay again over 900 megabytes over the set of improvements that we're going to make we'll improve that quite a bit I will add uh tag zero to that one just so we can remember it we never want to use an untagged or latest image and so we'll add the 1.19 Aline tag and then we can rebuild with that and so already just that has cut our image size in half which is great but we're not going to stop there just like how with node we didn't want to run our npm run Dev script because that's a development specific thing uh here go run actually builds and runs our application and rather than doing that we would much prefer to build our application in the docker file and then at runtime we can just call that built binding and so instead add a run command I will do go build- o API go langang that's going to create a binary called API goang from our main.go file and then rather than use this as our Command we can just run API go langang we'll call this number two this change means that every time we start up our container application we no longer need to build it because it's already built so we're just shaving off time that it would take in that container bootup process at this point I'll also point out the docker ignore file that I'm using for this uh project I've added the make file the readme and uh any Docker file that I create to the docker ignore those are files that are that do not need to be in the image and so this prevents uh when I make that copy command from invalidating the cache uh each time I change the docker file each time I add something to the readme ETC now similar to uh node we don't want to invalidate the cache here before we download the dependencies because then we'll end up downloading those dependencies way more often than we need to and so I'm going to move this copy command after and instead we can just copy the files that contain the references to our dependencies and so that's the go mod file uh and the go. suum file and we'll copy those into the uh current directory I'll call that number three with that change we can make changes to our source code without invalidating the cache before downloading dependencies and so each build would be much quicker and now at this point we're going to add a new technique that I haven't shown yet and that's something called multi-stage builds and these can be super powerful because it allows us to have within a single Docker file multiple individual stages that are building separate container images now for a language like goang that is compiled this is super helpful because we need a lot of dependencies in the build stage where we're using them to build our code however the end result is this binary that can have all of its dependencies statically linked we don't need any of that golang tool chain in the final image I'm going to label this stage as build and then I can add a new stage down here and in this case I can use a special image called scratch and this is a official image that we can look at here let me just go to Docker Hub uh it's a Docker official image it really has nothing in it the only thing that it has in it is a valid root file system it's effectively a noop that gives us a place to start from that we can copy our binary into and so now since we're starting from that scratch image we can use a copy command but in this time instead of copying from the host we're going to copy with a D- from equals build and so now this is saying copy from this container image that was built in that first stage and now we want to copy the slapp Slash API goang and we need to use SL apppp because that was our working directory that where that binary ended up uh we want to copy that to I'll just copy it to API goang and so it will be at the uh root path and then we can call it from our uh we can call that when we instantiate a container from this image uh however that wouldn't work as is because this build command didn't statically compile all those dependencies in so I need to modify that build command and I can do so like this so I've added these uh LD Flags to change the link mode to external uh and set the compilation to static I've also added this netgo tag I found that I needed to add that to make my networking within the image work properly however by doing that I now get this compiled binary that has all of those dependencies built in and I can run it from the scratch image so if I do this and build it I'll call this one four o but when I built it uh I ran into an issue where my build stage didn't have the compiler required to do that static linking uh and so one way that I can fix this is to use Buster which is a dban based distribution my Alpine image while it was smaller that doesn't matter as much anymore because this build stage is separate from my Deployable stage I'm going to end up using this image to deploy and so even if this is slightly larger in my build stage uh my Deployable stage should still be quite a bit smaller so let me try building that again and so that can be a trick when you're using multi-stage Docker files is you care a little bit less about the size of your intermediate stages and so you can use an image that has more build tools available uh or slightly easier to work with and so in this case we're using that deian based image in the build stage and then we're still getting the benefits of that tiny scratch uh stage for the output so if I look at the image sizes here we've now jumped from 500 megabytes to that statically compiled one is only 16.8 megabytes so huge benefit by using that multi-stage build compiling to a static binary and then copying that into a separate stage a couple things I want to add to set up my production environment uh and that is environment variable for Jin which is the API framework that I'm using and so I'll do gin mode and set that to release so similar to node end production Jin behaves differently whether or not this environment variable is set and then I know that this app is going to run on port 8080 and so I'll set expose 880 there as documentation uh for the end user I'll build this as tag number five and then the final Improvement that I'm going to make is similar to the cache Mount that I use for npm but we can do the same thing for our go Mod download command uh and so I can add a cache Mount here um this slgo package mod is and the root cach go build are the two directories where uh those dependencies get cached as they're being downloaded and installed and so adding this can just speed up subsequent builds accordingly and so as my final image I'll call this six I guess one final thing that we can do from a security perspective like our node based API we would prefer to use a non-root user within the side of the container and so to do that we're going to need to set up that user uh in our build stage and then copy a couple of files into our release stage that will allow us to use it I can do that with the uh run user ad command uh and I'll do user ID 1000 and I'm calling it nonroot then in my release stage I will need to copy the password file from my build stage so I can do copy from build SL Etsy password that will copy that in and then I can do user non root and that will use that user we'll call that number seven now in this case whether that was necessary or not is a little bit questionable uh because we only have our goang binary that's the only thing pretty much in that scratch image the tax surface area is quite small and so maybe it's over the top to add this nonroot user but I just wanted to highlight the fact that we can do it and it provides one additional layer of protection uh to our sort of Defense in depth approach uh when it comes to security and so with that we have a Docker file for our goang based API that I'm relatively happy with again we've pinned our our base image uh we've used a non-root user we're avoiding invalidating the cach when possible we're leveraging this Cash Mount to speed up future builds uh we're compiling to a static binary and leveraging a multi-stage Docker file to really take advantage of a much smaller image in the end uh and we're setting up our final environment with things like this environment variable um and the expose command for the port that we plan to run on and so with this we can create a container for our API that's both small and secure just like before if you take a look at the repo you won't find a Docker file in the module 5 instead I have created a number of Docker files here in module 6 that walk through all the steps that we just went through and have commentary around why we're doing the different things that we're doing so we've got two of our three applications containerized we have the docker files created the final one that remains is our uh react client so let me navigate to that and I will create a Docker [Applause] file uh and with this one the development mode for a v based react app uh is going to be a node JS server serving those files and so you'll see that our Docker file looks pretty much identical to the one that we had uh for the node based API so this would be a valid Docker file for that again let me build it so I'll do Docker build- T we'll call it client react zero we'll build that as always before that even finishes building I'm going to add a pinned version of that base image for repeatability and image size uh in this case we're not going to we're going to end up using a multi-stage build as well so I'm going to use the Adian based base image just cuz it's a little easier to work with than Alpine sometimes and I'm not as worried about it since it's not going to end up in the final image so I'll tag version one as the one with the the 194 Bullseye tag similarly our Docker ignore file is going to have the node modules directory within it because we don't want to copy those into our our image two improvements that we saw with the node API that we're going to make here as well is we don't want to invalidate the cache by copying in our source every time instead we're going to copy in the package.json and package lock. Json and then also let's set a working directory as sluser SL Source slapp I'll tag that as version two uh changes like that don't actually modify the size that much of the final image but it's just useful to see kind of the progression uh I'm going to use the same cache Mount approach that we used before to speed up future installs I will use the npmc command here I'm not going to use the Das only production flag because in this case it's our build stage where we're going to generate some static files rather than the final image that we're going to deploy a container with and so you'll see here our Command is still currently npm runev that's going to run a development server that's not what we want what we actually want to do is run npm run build which is going to generate a set of static files HTML JavaScript and CSS that then we're going to be able to serve so this will generate those files I need to name my first stage because we're using that multi-stage build approach so this will be as build we're going to add a new stage here at the bottom uh and we can use from engine X and let's see what version of enginex we want to use uh let's use 1.22 D [Applause] Alpine we're not going to need a command we're going to use the default engine X command um but in order to make this work we need to get the files that we built in line 13 and copy them over into this final release stage and so to do that uh we'll use the copy command we'll specify from build that's saying from that top build stage look at the path user Source app so we have our work directory user Source app and then the npm Run build command is going to place those files in a subdirectory called dist for distribution uh and then this path is the location this is the convention for where you would store uh your website files if you're going to serve them with enginex and so that's what we're saying there we need one more copy though and that is to copy in our enginex config and so our enginex config is how we're going to set up uh engine X to serve our site the way we want it to and so this is being copied from the host I have it here let me just walk through this real quick we've got a few endpoints set up we've got a ping endpoint which just returns 200 um pong directly I also have these two additional endpoints set up as proxies so if you go to/ apiol Lang that gets proxied to this address um and if you go to SL API node it gets proxy to this address and we're going to set up Docker when we run it uh to host those two apis at those paths uh within the network that we set up and then finally the root path if you don't hit any of those you'll end up here it's going to serve our react app files from that directory where I copied them in the final thing that I'll do is just to once again have an expose command this is just going to be an HTTP server so we're exposing Port 80 uh that's where it's expected to listen all right uh let me build this as three um and actually this engine X image the official one runs as root and so one way to improve that as always is to run as a non-root user and so there is a uh another image that I found which should be pretty much identical to that one um let me just search for it here this engine X Inc engine x unprivileged uh is a very popular one it has over 100 million poles uh it's very similar to the official enginex image with a few minor tweaks around uh the default settings including the user that it is run as and so in this case I'm going to swap out uh from that image to this unprivileged image and rebuild it once more as tag number four and one of the key differences when you're running unprivileged is that you can no longer bind to Port 80 uh you can no longer bind to any port below 1024 I believe and so rather than expose 80 the default here is going to be to run on port 8080 uh and so I'll change that and with that we can build it once more and let's just look at the image sizes as we progress so we can do instead of that we're looking for client react uh we started with a massive 1.16 GB image uh then when we switched to the engine X image it dropped to 23 megabytes uh the unprivileged engine X image isn't quite as optimized I guess so it's 76 megabytes but I think the Improvement of running as a non-root user is worth it for for that extra image size as with the other projects uh within the module 6 I have a series of Docker files listed out here that show the improvements along the way along with commentary and so feel free to look at them and see the differences between each step why we're making the changes that we did now going through that process hopefully it was educational it might have felt a little bit repetitive but that's kind of the point we want to find a set of general principles and practices that we can apply to any Docker file for any container image that we're building and the general approach that I think about is first we want to make it work that's kind of the fundamental thing that needs to happen that's that naive base Docker file that just has a few lines gets it running then we want to think about two things we want to how do we make this more secure how do we make it faster to build so it's easy to work with I've laid out a legend here that talks about the three types of improvements that we're making whether it's impacting the security of the image uh the speed with which we can build build it or the clarity of the image how clear it is for future developers who are coming along and reading it and so for each of these techniques I've listed out with the Emojis so that you can see what they're doing we pinned specific versions within our build this can impact security speed and Clarity the base image is a huge one right if we pin a specific small secure base image that's a great starting point we can either use a tag like I was using 1.19 Alpine you can also use the hash of the image itself to use it or a combination of the two so you can have the tag and then an at signed and then shot 256 and then the hash that's actually kind of The Best of Both Worlds where you have a human readable version as well as that immutable hash that will correspond to a specific version that can't change you also want to pin specific system dependency so if we were installing something from apt or a package manager we would want to install a specific version there so that it doesn't change on us uh that can help with security as well as Clarity so you know exactly what is going into the image uh and then application dependencies so within our package lock Json file uh within our go mod or go Su files we want to install the exact depend every time so that we're not accidentally changing things from one build to the next I mentioned this in the pinning of specific versions but it's worth calling out again you want to think about what's the smallest and most secure base image that I can use that will still be nice to work with and I can install my app into it that's going to help both from a security perspective as well as a speed perspective because those smaller images are just easier to move around whenever we're adding a command to a Docker file we're adding layers and we want to think about how do we protect that layer cache such that future builds can leverage as much cash content as possible the general principle here is to order your commands by how often they're going to change so that base image is very infrequently going to change we want to install our dependencies before we copy in our source code so that we don't bust the cache there we want to use that cash Mount technique so that it can store downloaded dependencies locally and reuse them in the future this next one is a a technique that I didn't actually highlight in that process it's a relatively new feature it's an option to the copy in instruction uh and by using copy-- link it creates a new separate layer that's not tied to the previous layers and this is super useful in the multi-stage build environment if I'm copying from the previous stage I can use this such that if I change my base image on my second stage that won't invalidate this the cache because this copy. link when I'm copying from that previous stage it ties it to that layer stack instead and then the final step is to combine steps that are always going to be executed together so that we have fewer total layers in our image we also wherever possible want to be explicit within our Docker files just for clarity sake uh we want to do things like set that working directory so we know where within the file system we're operating we want to indicate the standard Port that we're going to be listening on with that expose command for clarity and we want to set the environment variables how we want them with the end instruction so it'll be automatically set when we run that container we also want to avoid unnecessary files in the container image we can do that in two ways we can add things to the docker ignore file so that they don't get copied in we can also when we issue our copy command copy specific files or subdirectories rather than just copying in the whole build context for security purposes it's always better to use a non-root user so we saw how to do that in a few different cases uh we also want to only install production dependencies so if there's certain things you only need in your development environment there's no need to build those into the Container image uh we always want to avoid leaking sensitive information uh and so good example of this that I'll show here in a minute is if you have a private repository that you need to log into you could have private npm packages maybe you're building a python app and you have and you have a private python registry we'll need to pass those credentials in a specific way so they don't get built into a layer within our container image and then the final one here is just to leverage multi-stage builds where it makes sense we saw how with the goang API we were able to cut our image from 500 megabytes to just 16 with the react client we were able to go from that node based build environment to that optimized engine X image at the end it saved quite a bit of size and just makes a lot more sense for serving those static files that are generated as I built out those images I often chose an Alpine based base image I was doing that because of the size however size is not the only consideration when choosing a base image for your container it is an important consideration because the size of that base image is kind of the lower bound on the size of your final built image however you also want to think about how well that particular base image supports the language that you're running in in this case it may have not been the best choice for a node.js application if you look at the official nodejs Docker file page you'll see that Alpine is still considered experimental uh and that's because it uses a different variant of the standard C library uh and so there can be subtle bugs associated with that if your code or the libraries that you're depending on are making assumptions about the implementation details of that core Library you'll also want to think about the ergonomics of the image how easy is it to work with if you need to install additional packages does it have utilities built in for debugging Etc and then finally the security of the image is another huge consideration things like the number of cve built in at what level uh and the attack surface area if someone is trying to compromise the container on the right hand side here I've listed out some sample images that you could use for a node.js application the first one is the default latest tag which is currently pointing to the 19.8 Bullseye image and Bullseye is a variant of dban that one is nearly a gigabyte it has five critical vul vulnerabilities in it generally you wouldn't want to use that however it is quite easy to work with you can use the app package manager to easily install lots of different Packages Etc the next one down there is the slim variant of that so it's still using that Debian based image it is cut the size by a factor of four eliminated all all of those critical high and medium vulnerabilities it still does have some lowlevel vulnerabilities cves however it's a much better choice than the fullsize image for that reason however because it's slim it won't have a lot of utilities built in out of the box and so you may need to install those if you need them at either build time or at runtime uh the node Alpine image is the one that I was using in some of the examples it doesn't have any cves built in it's quite small however as I pointed out and I actually learned this after filming that section it's still considered experimental by node and so if you go to the repo you'll notice that I'm using that slim based Bullseye image for the the base image there there's two additional images here at the bottom that I've listed out the first one is what is called Dro list that was created by Google as a security focused image that only contains the language runtime that you need it doesn't have any additional things like a package manager uh shells built in ETC and so it is quite small and reasonably good from a security perspective however one potential downside is that it only supports LTS long-term support variants of node and so if you want to run on a different version of node that's not possible it also if you need to install anything outside of node there's no package manager so that could be more difficult to do the final image that I've listed there at the bottom uh is from chainu guard and chainu guard is a security focused Cloud native company they have started producing and maintaining uh these BAS images for various languages and run times it is quite small as you can see it's the smallest one listed here it has no known cves in it and it's designed specifically with security uh and software Providence where the source of any dependencies are coming from because it is more new uh and it is really locked down it may be slightly more difficult to work with in terms of the types of packages that are supported and included if you have particular needs for your application and so I think as a general purpose image that's easy to work with the dbn based variants the slim variant in particular uh is quite a good choice and if you're really looking to lock down your eventual containers something like the the chain guard based image uh could be a good choice through the process of building out those Docker files for the demo application I highlighted a number of different instructions however I didn't cover them all there's a couple of additional features that I want to call out here included this Docker file. sample in the repo as well um the first one is paral directives so at the very top of your Docker file there's two things you can do one you can specify a specific version of the docker file syntax that's necessary if you're using some of the newer features that are not enabled by default so that copy-- link that I that I talked about a minute ago um that I I think is in Docker file version 1.5 so if you wanted to use that you would need to have this directive at the top of your Docker file uh you can also set an escape character the default Escape character is going to be that backslash but if you wanted it to be something else you could set it accordingly the next feature that I'll call out is is label and this is super useful for including important metadata within the image manifest for example there on line 14 I've included myself as the author of this Docker file you can add any arbitrary key and value pair there are a number of standard specific labels under org open containers image that people use including things like authors version Etc so I would suggest taking a look at those and deciding which ones make sense for your images I also mentioned that sometimes you want to combine multiple commands into a single step so they're combined within a single container layer one nice feature for doing that is what's called the her do syntax and if we see they're on line 20 we have run and then the less than less than eof apt update apt install eof that allows us to write multiple commands across multiple lines however they get interpreted as being a single line within the docker file and so we'll execute those two things in turn but they'll be stored within a single layer within the container container image in order to avoid sensitive data from leaking into our our image at build time we will want to use the secret Mount type so on line 30 there I have an example of that where we're mounting in a secret so this in this case it is a on our local system it's a file called secret. text and we can mount that into the file system of the container at/ container secret. text uh and that will only exist at build time it will not get included in the final artifact that is an important technique to know if you need to use credentials within your build context now so far we've been using the CMD or command option and that specifies the command that gets run when you create a container from your image there's an additional instruction called entry point which is slightly different and a bit nuanced as to how these two interact with each other it can be a little confusing as to which of these get gets run in which case if you're building your own application and it's always going to be run with the same set of arguments you want to just place all of those in the command like we've been doing however if you're building a utility where you can pass additional arguments to The Container at runtime you would want to put the executable in the entry point and then allow people to append arguments via the command at runtime when we wanted to get files from our host system or from a an earlier build stage into our container image we were always using the copy command there's a very similar command called add which is a little more ambiguous and so Docker generally prefers using the copy command you may see Docker files though that use ad and just know that they perform almost the same way but there are a little bit of nuance in the Syntax for very specific edge cases so just know that that exists and then the final thing to call out is there's a technology called build X which is built into the docker CLI which allows us to do a number of things but one of the very important things is to build and run multi-architecture images so if I'm on an x86 based system but you're on An Arm based system or I want to deploy to An Arm based system I can use build X to build versions of of my image that are compatible with either of those now I'm not going to go into detail in this course however I do have a video that does a deep dive on buildex and building and using multi-architecture images that I'll link to in that card up there I will just pull up this Docker file. sample and just interact with it a little bit though so within module 6 I've got this Docker file. sample uh and then in my make file I have a couple of commands that I want to show you here uh the first thing that I'm to do is make build sample because it's using that secret Mount that's a build kit feature that you have to enable and so by specifying the environment variable Docker build kit equals 1 I'm able to use that I'm also setting this base image tag as a build argument so one important thing to call out is that within a Docker file I can use this ARG instruction that will be an environment variable available at build time but not available at runtime and so in this case I'm setting my base image tag it was going to default to 194 but I was able to override that with this base image tag here of 193 in my build command however even though they're not available at runtime they still exist as artifacts in the image metadata that's why we can't pass credentials using this buildar uh approach instead we want to use the secret Mount approach now you'll notice that I have both an entry point and a command here uh and so I'm going to run this image and let you with a few different options and let you see how that behaves and so to do that I have this run sample entry point command so I'll do make which goes off and builds our image and then it's going to run it in a number of different configurations if we run the image with no arguments so if we just do Docker run and then the name of our image uh what do we get out we get out hey Team wave entry point plus command so with no arguments we get we Echo both this string and then we get this string as well so both of those get passed to the echo command if we run with an argument so that would be like Docker run sample plus argument now we we still get hey Team Wave entry point but instead of plus command we get plus argument so adding that argument onto my Docker run command overrode the command that was specified in my Docker file we also can override the entry point at runtime and so here if we do Docker run-- entrypoint Echo sample that overrides the entry point and it ignores our Command and so that comes back with an empty string because we ran the echo command with no arguments and then the final case is if we override the entry point and we pass additional arguments uh it's going to do it's going to override the entry point ignore the command and use the arguments that are passed at runtime and so with this command we end up with hey team overwritten entry point plus arguments hopefully that description shows you how those two things interact like I said if you're building your own application that's not going to be called with different arguments each time put everything within that Command Block it's just simpler and we don't have to worry about the entry point you're generally only going to use the entry point if you're building out a utility that's expecting additional arguments at runtime to be passed into that container now I do have a make Target here to also demonstrate Building multi-architecture images so I'll to make build multiarch uh in this case I have a a repo on Docker Hub called Sid multi Sid Palace multi-arch test so I can show you that and this is going to with this D- platform tag it's going to build one image for amd64 one for arm 64 and one for arm V7 which is a 32-bit uh arm architecture it's going to build that based on the docker file. sample and then it's going to push it to dockerhub so if we go here it is still building once that finishes building and pushing uh we would see an updated tag I last pushed it 20 20 days ago but if I update this here in a minute it will update to now we can see it's working on those different architectures in par parallel and so behind the scenes build X and buildkit are using uh a emulation technology called Kimu qmu to enable us to build the an architecture other than the one that we're actually running on this this topic is becoming more and more important as the number of developers on arm systems with apple silicon is increasing and the fact that arm-based servers in the cloud are generally quite a bit cheaper than their x86 counterpart so being able to build for whatever architecture you want can increase the likelihood that your team is all compatible and allow you to take advantage of some cost savings with your eventual deployment so it looks like it is finished so if I refresh this we should see that we just pushed a few seconds ago for all three of these architectures uh with that test image and that concludes our Deep dive on writing and optimizing Docker files hopefully you'll be able to take that knowledge and now apply it to any future application that you'll need to containerize in this portion we focused on building out those production container images in later modules we're going to think a little bit more about how do we modify these to make them easier to work with from a development perspective so that we can have both our optimized production image as well as a development and debug or test image that we can work with on a day-to-day basis so far in this course we've been building all of our container image locally but if you're working on a team or you need to deploy those containers somewhere you're going to need to start using a registry a container registry is a repository or a collection of repositories used to store and access container images so these are generally stored somewhere in the cloud and they enable us to take those images that we've built either locally or within a continuous integration server uh push them to this registry and then other team members will be able to pull them we can pull them into whatever deployment environment we're using repos can be either public or private so when you go on dockerhub and search for those public images you'll find those but you can also set them up such that they require authentication there's many different container Registries that are available dockerhub is a popular one that is provided by Docker the company GitHub has its own container registry Google Amazon Azure they all have container Registries as well as I mentioned you can have public or private container Registries for a public registry you don't need to authenticate in order to pull however in order to push to that registry you would need to authenticate and for a private registry you'll need to be authenticated for both pushing and pulling Docker desktop has what's known as a credential Helper and using this when you log into one of these container Registries it's able to access system resources such as on Mac OS it stores the necessary credentials within your system keychain so that they can remain secure on your local system additionally some Registries May leverage their own tooling to authenticate for examp example the Google container registry uses the g-cloud command line utility to authenticate your gcp user to that registry and works with the docker client to provide access now I want to demo uh building and pushing an image to dockerhub and the GitHub registry just to Showcase how we would do that I'm using the most simple Docker file you could imagine here it's just going to have a base image from scratch I'm putting that into a Docker file building my scratch image from that and then I'm deleting that doer file just because I don't want to store an extra file in the repo so if I do make build we've now built that container image and it exists locally on my system in order to push to a remote repo you have to do two things one you need to be authenticated to that repo and two you need to tag an image with a tag corresponding to that repo and so if I go to dockerhub I've already created this repo called my scratch image if we needed to create a new repo you would click the create repository button uh fill out the name add a description choose if it's public or private and then click the create button in this case I've already created this and so the tag that we need to use in order to push to this repo is going to be Sid Palace which is my dockerhub username the name of that image my scratch image and then we can append additional tags to the end in order to authenticate to dockerhub we would use the docker login command I've already authenticated so it used those existing credentials but it would ask you for your dockerhub username and password in order to log in if you hadn't done so before so in order to take this image that I just built and push it to dockerhub I need to retag it with a tag associated with that repo so in this case it was sidp Palace my scratch image uniquely identifies that repo for Docker Hub you don't need to include a prefix you can just use the the username and repo directly and it will assume that it's dockerhub if you don't have a registry prefix in front of it we'll see that's a little different for the GitHub one but in this case I'm using the docker tag command to take my original tag of mys scratch image that I applied up here uh add an additional tag of Sid Palace my scratch image and because I don't have a colon with a tag uh version it's just going to apply the latest tag to it I can then push it to dockerhub and that will show up however generally we never want to use the latest tag and I'm going to talk about tagging best practices here in a bit but it's much better to apply a specific tag to this so I'm going to once again take my original tag of my scratch image apply a new tag which corresponds to my dockerhub repo and in this case I'm adding the tag abc123 as just an example tag I can call a Docker push on that tag and it'll get pushed to to dockerhub so let me just call that make push dockerhub uh it tags my images with the ne necessary things and then pushes both of those to dockerhub I can now go to dockerhub and if I refresh the page we can see both that latest tag when I didn't specify anything as well as my abc123 tag were both pushed uh just a few seconds ago so that's dockerhub let's now go through the process of authenticating and pushing to GitHub container registry the process looks quite similar but we do need to follow the instructions um at this link and so here's a walk through of how to work with the container registry in particular I want to look at authenticating to The Container registry they give some instructions on how to do this within a GitHub actions workflow um I'm going to use this authenticating with a personal token option and so the way that we do this the set of minimal permissions that we need can be created using uh this URL that they provide zoom in just a bit uh I'll say Docker course creds it has the Right Packages permission now I'll click generate token don't worry I will delete this token once I finish recording so I'm going to export this as my uh container registry personal access token great and then there was a login command here yeah so I'm going to Echo that token out and pass it to the docker login uh command specifying that I want to log into the GitHub container registry. registry so Echo that looks like our login succeeded and now just like with dockerhub I'm going to retag my image using uh this repo name and so in this case I have the same repo name as I had on dockerhub but now I'm prefixing it with gc. which tells it to use GitHub container registry again if you don't specify tag it will default the latest and then down below I'm tagging it with abc123 and pushing both of those to GitHub so I'll do make push GitHub packages it tagged both of them pushed them to GitHub container registry and now I can navigate to GitHub container registry Sid Palace my scratch image uh and we can see we have a latest tag and an abc123 tag now let me just go delete those personal access tokens so I don't forget and that's under settings developer settings personal access tokens tokens revoke all so each registry is going to have its own instructions for logging in so find the instructions from the from the registry itself and then follow those and that'll work with the docker client to store those authentication credentials on your system now I talked about when I was tagging and pushing those images how if you don't specify a tag it defaults the latest and that's generally a bad practice you want to have descriptive tags that tell you what the image actually is doing it's also important to note the same image can have many tags and so for example if we go into dockerhub when I last checked on January 30th all five of these tags auntu latest auntu 2204 Jammy -22 1130 Jammy and this specific shot 256 hash all point to the exact same image and so when we're tagging our images we can put multiple tags that we think will be useful to the end user now ideally you want to treat any tag except for maybe a temporary one that you're using for development as immutable and by that I mean you shouldn't create a tag push it to dockerhub or or whatever you're using for your your registry modify the image rebuild and tag and then push and overwrite that can be a nightmare if someone else is using that image and they could accidentally get the updated version and cause issues within their process so you always want to treat tags as immutable where possible the one exception there is if you're doing something locally and building it over and over and running it locally you might not need to change the tag every time because you're you know the fact that you're changing it every time now usually your images are going to be built within some continuous integration system hopefully that's automated and often times I'll see people use some combination of a time stamp of when it was built so maybe like we see up above that 22 1130 uh it was like likely built on November 30th uh you might put in a build ID from your continuous integration system so maybe the GitHub action the ID of that GitHub action so if you need to go look and see when it was built you can see the process that happened uh the commit hash of the source code so let's say I'm building an image based on the latest commit to my main branch I might take that that hash from that Branch uh of the latest commit so that I can very easily find where in the code base uh that build is coming from uh and then also often times I'll see people use a sver release version so like 1.1.0 or whatever the the latest release version is these can be combined in various forms uh when I set up an automated workflow with GitHub actions in the developer experience portion I'll tag the images with a bunch of with combinations of a bunch of these also as we saw when we pushed to a registry other than dockerhub we had to prefix the name of that registry at the front end of our tag now that we've built our container images and learned how to push them to various container Registries we can move on to running them and configuring them to all talk together within Docker up until this point we've used the docker run command a number of times to run containers from third party images we've seen this example with postgres a number of times however there's another way to run Docker containers that is built into Docker and that's called Docker compose behind the scenes they do exactly the same thing but Docker compose allows you to specify all of your application configuration within a gaml file and it makes dealing with the container life cycle much much easier these two images here on the left hand side shows the docker run variant the right hand side does exactly the same thing but using that compos file instead if you're just running a one-off container I think using Docker run is fine but for our situation where we have multiple services that need to talk to each other Docker compose is much more intuitive and easier to work with I'm going to show how to do both for our setup but in most situations you'll be using Docker compose if you have an application with multiple containerized Services I'll also point out this GitHub repo called composer eyesee which you can paste in a Docker run command and it will generate the equivalent Docker compose file and so if you have a command that you've been using and you want to convert that to Docker compose that can be a very easy way to do that now there's a ton of different options and flags that you can use at runtime with Docker I've tried to pull out here the most common ones that I use on a day-to-day basis and let's go through and test these out and showcase what they do the first one is that Das D which stands for detached which allows us to run a container in the background so as an example I can run Docker run auntu sleep five and it's going to take over my shell until that sleep command exits and then it will return so after five seconds it's done however if I add A- D to that now it gives me the ID of the container and runs in the background I can see it by doing Docker PS and here we see that abun 2 container uh running my command sleep 99 in the background the next one is the D- entry point option where we can override the entry point that was defined in the docker file an example of this by default the auntu image doesn't have an entry point and uses bash as its command we can actually use the entry point like this to say roer run entry point override that with Echo and then we'll pass the argument hello so when I run this it should Echo hello back to the terminal there we go the third one there --v or- e or --v file is to set environment variables at runtime so we could do things like pass in database credentials or configure anything else in the environment that we need to Showcase that one I can do Docker run-- EnV my n equals hello auntu and then the command I'm going to run is print end which will print out all the environment variables in the environment you can see it has these three environment variables including the one that I set at runtime the init flag uh is an interesting one some programs are not designed to be run as process ID number one which is what happens by default if you run them in a container and so if you pass the init flag with your Docker run command instead Docker will run its own initialization script and that will spawn your process as a subprocess so for example I can run Docker Ubuntu PS and that PS command gets executed as process ID number one however if I pass the init flag and run the same thing we see that this Docker init is process id1 and my PS command is process ID 7 this is helpful if your application is managing subprocesses itself passing the init flag can make it easier to handle forwarding along termination signals from Docker to The Container the interactive flag or - I and the TTY flag DT allow us to have an interactive TTY session inside the container so if I just run Docker run Ubuntu it will call bash which will exit immediately however if I add the dash it doer run-it Ubuntu uh the - I will make sure that standard in remains open and the- T will give me a TTY teletype or shell session with inside the container and now I have the shell running in the container we've seen the D- Mount and D- volume options used quite a bit when we need to persist data outside of the container layer in a volume so I'm not going to demo that one now the name option gives us the opport to provide a specific name for a container otherwise Docker will assign a random name when the container started so I can do uh Docker run I'm going to use the DD flag so this runs in the background and doesn't take over my terminal I'll name the container my container and then pass it the Sleep 99 command ah and this actually shows one downside of using named containers is that you can't have two containers with the same name on your system so before I could run this I would need to remove that container now I should be able to run this and now we have that container in the background if I do a doctor PS uh we can see under names my container this can be useful if you want to be able to quickly grab the logs from the container you can now address it with the name rather than the ID that that container doesn't have any logs because it's just the Sleep command but usually I just let Docker manage my naming for me and do it randomly the network or net option allows us to connect to a specific Docker Network so if I do Docker Network LS we can see the different networks on my system I'm going to add a new one called my network with Docker Network create my network now we can see that new my network was just created and now I can create a container that attaches to it and if I inspect that container with the ID and grap for Network we can see it's running in my network specifying a network like this allows us to create isolated networks for any for our applications rather than just letting all of the docker containers end up on docker's default Network in such a way that they would be able to communicate with each other the platform option allows us to specify which architecture we want to run our container image even though I'm on a amd64 system I can run it with this Linux arm64 V8 option it went off and pulled the arm 64 version and then I'm running this D package print architecture command and we can see that it output arm 64 if I do the same thing but specify Linux amd64 we get the amd64 result we've already seen the publish or- P option used quite a bit to connect a port from our host system to that of the container the Das Das restart option there's a few different options you can have always unless stopped or Never So if I specify to restart this container unless stopped Docker is going to see if it exits and unless I told it to stop it will continue to try to restart it so in this case that bash command the default for auntu is going to exit docker's going to see that and restart it over and over so now I can do watch Docker PS and we'll see this image is exiting and restarting exiting and restarting and then lastly the-- RM flag tells Docker that if the container process exit it should remove that container rather than leave it on the system as a sto container this can help a leave at that naming conflict that we ran into before where the two containers had the same name if we had used the D- RM flag when the previous container had exited Docker would have clean that up to demonstrate this I can run one Docker container uh named this one will be there that's going to exit however it should still remain in the system as a stop container I'll now run another one with the-- RM flag called this one will be gone and now if I do a Docker container list- a the d a is necessary because otherwise it would only show running containers and I grep for this one that should find both of them if they existed however we only see this one will be there in the listing there are way more options than this including this set which I think you should be familiar with you may not use these on a day-to-day basis the first of which is capabilities add and capabilities drop this allows you to specify which Linux capabilities should be accessible from the container so this is a security feature where you can what is allowable within the context of that container cgroup parent allows you to specify which cgroup ID your container should be associated with if we think back to how we can use cgroups to Monitor and limit the resources that the container has access to this is one way to specify that directly CPU shares is another cgroup related option where you can specify what percentage of the CPU Cycles this container should be able to access the CPU set option allows you to specify which core specifically so if you're fine-tuning performance and want to pin a process to only execute on a specific CPU core you can do so these next two are related to the different devices that the container should have access to and at what throughput and bandwidth if we wanted to be able to access gpus with our container we would want to use the GPU flag we can use these health related options to specify a health check that Docker will use to periodically ping our container and make sure it is is healthy like our CPU option we can use this memory option to specify how much memory the container process should have access to if it goes over that limit Docker will kill it we can use these PID related options to specify how many subprocesses our container should be allowed to manage the privileged option pretty much overrides all of these other Security Options and effectively gives the container access to all of the Privileges that it could want the readon flag specifies that even the container layer of the file system should be readon so if you don't need to have right access you might want to set this as another security Forin the security opt option allows you to specify either app armor or set comp profiles that you want to use for the container when it's run and then this last one is not one that you execute at container runtime but is something you would specify when you're running Docker D for example if you installing Docker engine and that is to enable namespace remapping for the user namespace allowing you to map from a non-root user on the host system to a root user inside of your containers as just one more layer of defense against a potential attacker so like I said these are very useful to know that they exist however if you're working on your development system it may not be as necessary to leverage more of these Advanced configuration options let's jump over to our editor and and configure Docker run commands and Docker compos configs to run our application and so I I have all of the necessary commands to do this stored within the make file here and I'm going to walk through how they're all configured and what they're what each of them is doing the very first thing that we're going to want to do is to build a version of our container image and tag it such that we can use it in our Docker run commands so this Docker build all make Target is calling a Docker build command and then because I'm located in a different subdirectory so I'm in the 08 I'm in the module 8 directory whereas we know our Docker files are contained in the in the module 6 directory and our source code is in the the module 5 directory that's why I'm having to use this file option and pass it a context outside of my current directory uh if you had your Docker compose and Docker files living alongside the code these commands would be somewhat simpler so this is going to go off and this first one is going to build my client react Docker file number three so if you remember as we were building up our Docker files at one point for the react client we were using using the npm Run Dev script and so that's actually using V to serve a development copy of our front end uh so we're going to build one image with that Docker file we're also going to use the final the final react client Docker file where we're copying a a built version of our frontend app into an engine X based container image so we'll have one tagged client react enginex based on that image we'll also build our node-based API using the final Docker file that we came up with and then we'll build our goang API as well using that final Docker file that we came up with for that one uh so let me just do make Docker build all and that will go off and build those four Images I did clear my image cache so this is going to take some time to build from scratch all of these images however in the meantime I can walk through the docker run commands that we're going to have to use to spin up these containers so I'm going to start by creating a Docker Network and so by default there's a number of Docker networks on the system uh let me just show those now so if I do Docker Network LS we can see these are all the default networks that are created we have a bridge Network we have a host Network and we have an empty null network if I don't specify a network it's going to get attached into this default Bridge Network however I want to create an isolated Network for my application and so for that I'm creating a network called my network so I'll do just that I can do Docker Network create my network and now if I list them again we can see my network and by default it uses that bridge driver which is what we want in this case the first container that we're going to run is the postgres container we've seen seen this command a number of times the only difference here is that I've added the-- Network option and passed it the name of my network and added the restart unless stopped command to tell Docker to restart it if it if it were to crash this- D flag will run this in the background we can see that postgres image is running in the background I could look at the logs if I wanted with doer log logs and then the ID we can see it's started up and it's ready to accept connections it looks like all of my uh four custom container images have now built so I can walk through and execute the various commands such that those containers will start up as well uh I'm going to start with uh API node so I'm going to work from the database forward so we'll have Docker run again we're going to run in the background with the DD flag I'm giving it a name I'm giving it a name here so that I can have another make Target that will clean up those resources afterwards I'm putting it on that same network so that it will be able to talk to my database I'm passing it this database URL and so I've specified that environment variable here and one important thing to note is that I gave the postgres container the name DB and then I've also added this D-Link DB option referencing the name of my postgres container and that gives me an alias with which I can address my database container and that is what is used here in my connection URL I should now be able to run Docker run for my node container we can see API node and postes uh at this point it looks like my node container is crashing for some reason so let's look at the logs here Docker logs so I wonder if this string is somehow getting escaped improperly so let me try again um Docker stop API node Docker remove API node ah it's because I copied and pasted this in without setting that environment variable export database URL equals that now if I run my container uh it should be able to pick up that environment variable and pass it in so I can do Docker PS uh it is no longer restarting and so I should be able to access on Port 3000 my API so let's make sure that is working and so it looks like my node API is working properly for the goang docker run command looks pretty much identical to the API node one uh except the name is updated and the image is updated we're still passing that link option so that we can reference the database in the same way and so let's start up our goang image it is now up let's switch over I know that one is running on 8080 okay goang is working properly and now we need to get our two front end containers so we're going to have the one running engine X and the one running uh V we'll start with the V one one thing to note is that my V config inside of Docker needs to be slightly different than my V config outside of Docker let me just compare the two so I'll select for compare and then here I will compare with selected and you can see when I was running these directly on my host without Docker I was able to my proxy was set up to just forward these to Local Host 880 now that I'm inside of that Docker Network I'm and I'm passing the the link option instead of referencing on Local Host I need to reference them by the Alias for that container so in this case it'll be API goang and API node and the way that I get this updated V config into my container at runtime is via a bind Mount so I'm using the- V option to specify that uh with my present working directory and then a relative path to client react V config.js and I'm mounting that to the same location that the other one would have already been located and this will overwrite the one that was in the container image with the one on my host system at that bind Mount location I'm linking this both to API node and API goang because I'm talking to both of them and then I've referenced my name as well as my image name down here in the command so I can run [Applause] this okay it's been up for one second uh so let's go to 5173 which is where I'm publishing that Port we can see it loading properly that's awesome and then the final container that I want to run uh is going to be my engine X based front end where I took my react app I built those files and then copy them into an engine X based image almost identical to the V one with the one difference being I don't need a bind Mount because the engine X config that I wrote was already designed to work within this containerized setup I'm also going to port forward from Local Host 80 to 8080 with ins inside the container which is where that unprivileged user from my engine X base image is is going to bind to by default I'll run that and now if I navigate to just Local Host 80 or by default if I don't specify a port it will default to 80 it is working properly and so with these with all these commands I can now spin up my application and spin down my application very easily I've added this docker stop make Target that will stop all of those named containers I've added this Docker remove one that will clean up those stopped containers off of my system and delete those as well as delete that Network and so I'll do that now and so now none of those containers are running anymore in my opinion having all these doer run commands with all these different Flags uh stored in a make file is pretty clunky and hard to deal with hard to modify hard to reason about luckily for us Docker compose exists and gives us a much nicer interface with which to do this exact same thing so I'll pull up this Docker compose file I'll also show you uh that composer eyes tool so let me just go to composer eyes zoom in a bit paste in my command click output and so we see my postgres command gets translated into this compose file where we have our services in this case it's named postgres we've named the container DB we're specifying the network we're passing an environment variables we're configuring the volume we're setting ports we're setting which ports to publish the restart option and the image I've gone ahead and done this for all of those commands that we had and that gener ated this Docker compose yaml file the latest version for Docker compose is 3.9 so if you're creating a new Docker compos file and want to take advantage of the latest and greatest features you may as well specify 3.9 and then within here I've got one block for each of my different services so I'll start with client react V and we can specify both an image as well as a build context and so Docker compos can handle not only running our our our containers but can also handle building them based on a context that's specified so in this case it's pointing to module 5 where the source code lives as well as a Docker file and here it's pointing to module 6 where the docker files live where the docker file lives this init true is the equivalent of a D- init in the docker run command again we've got our bind Mount here specified for the V config I did something slightly differently with the networking here I have two custom networks I have a front-end Network and a backend Network and so because my react app only needs to talk to my apis and should never need to talk directly to the database I've specified that those are attached to the front- end Network my react app and the two apis and then I have a backend Network which my two apis and the database are connected to and so that just gives me even more fine grain control over which Services can talk to which my enginex based service uh looks very similar the main difference being I'm pointing to a different Docker file and port forwarding to a different port my node API again quite similar I will call out I've added this dependson DB option and that tells Docker compose that wait until you've started up the DB container before you spin up the node container as I mentioned it's the API layer is connected to both the front end and the backend Network so we can have a listing of multiple networks here that this container should be attached to pretty much the same thing for the goang service and then finally the DB service which we're specifying that password it's only attached to the backend Network we've got our volume Mount at the known location where postgres stores its data and then we're using that public 15.1 Alpine image as our container image down here at the bottom we specify our different volumes in this case I have my PG data volume that's getting used for postgress and I specify my two networks because I haven't specified any additional data about these Networks they will use that default type of bridge which is the desired network type for this situation now with this all defined in this yaml file interacting with it is super easy let's say I want to build all those containers I can do Docker compose build and it will look at the different build sections for each service with the context and the docker file and go off and build those for me now if I want to run them I can just do Docker compose up and it's going to run all five of those containers and log out to the console from them all with color coding for each so I can see now I'm attached all my containers are running if I then go and load the page so in this case I opened up the engine X based front end off screen and we can see the logs coming from the engine X container followed by the log from the goang container where the front end made a request there followed by the log to the node container where the front end made a request and so we can see this be very valuable and help us to see what's going on within our containers by attaching to the logs in this way when we did our Docker run command we could have not used the detached option and instead had terminals connected to everything but with doer and compos it's just much easier to manage within a single terminal if I wanted to I could uh run these in the background with the- d flag I could do Docker compose Dash up DD uh and we can see it's created all those in the background if I then wanted to stop I would do a Docker compose stop and because my Docker compos file is named as such that's the default if I named it something else I would need to pass in a reference to that file uh with my Docker compose command just like when we run a Docker build command if we have a Docker file in the non-default location we need to pass that- F flag and so hopefully you can see the power of a Docker composed setup like this you'll be able to go to the GitHub repo clone it navigate to module 8 run a Docker compos up build and it will literally build all the necessary images and run it on your system and you'll have this application running in less than 5 minutes uh and you can see exactly how everything is configured within this one file and so this can be a super powerful tool for you and your team in setting up a common shared environment so that's it's easy to build and iterate on your your applications when we go to deploy the application and work on improving the developer experience we'll make some modifications to this and change the config a bit but I think this is a great starting point that hopefully highlights uh many of the different options that you'll use when you're running containers for your multi-service applications since many of those more advanced runtime options are security related it seemed like a good time to do a little bit of a deep dive on container security and this I think is my favorite of the memes that I created for the sections of this course just because something is running in a container doesn't inherently make it secure depending how you configure things at runtime it may be secure or it may not be so here if you specify user equals root the privileged option and connect it to the host Network you're removing almost all of the security boundary that a container would provide and it's about a secure is using that Cheeto as a lock there's two primary components to container security the first one is security of the image itself so as you're building the image what features are you using and how are you configuring things to make sure that your image is secure a big part of that are the software dependencies that are installed within your image and what attack surface area exists that a potential hacker could exploit the other key area to think about container security is at runtime so if someone were to successfully compromise your container because of some vulnerability that you built in what would they be able to do would they be able to escape the container and move laterally to compromise the host or other containers or would they mostly be stuck and confined within the container because you configure the runtime security appropriately some of my main things to think about when you're considering image security are how do you keep the attack surface area as small as possible one way to do that is to use minimal base images so the smaller your image the fewer things installed in it the less likelihood there is to be a potential bug that can be compromised one great source for secure base images is a company called chain guard which maintains a set of Base images specifically with a security Focus as you're writing your Docker file don't include things in the final image that you're not going to need if it's not needed at production time don't install it or use a multi-stage build if you need it at build time but not at production time also after you've built your image there's a number of tools available that you can use to scan that image for potential vulnerabilities there's one built into Docker called sneak there's another one from Aqua security called trivy take advantage of these tools to scan your images and see if there's potential vulnerabilities and what level those vulnerabilities are at this gives you visibility and allows you to choose if you need to patch them or if you're willing to accept having some known vulnerability in your image we saw this as we were building out our Docker files ideally you don't want to use a root user inside the container you want to use a Linux user with with the minimal set of permissions to accomplish the task at hand you should avoid building things like credal into the image itself you should treat your images as if they're public and inject any sensitive information at runtime you also can cryptographically sign your images to prove who was the person who built this and that can be useful to then verify on the other end that an image is what you expect and finally as you're choosing which base images to use make sure to pin at least to the minor version number if you Pinn to major. minor but not the patch version it would allow updates to the patch version let's say for bug fixes to be automatically rolled in but hopefully not incorporate braking changes into your system alternatively if you really want to lock down the base image you can use that image hash which will always refer to the same image on the runtime side of thing we can both think about the the container run time itself and the configuration options used for each container if we're setting up the docker demon Docker D within Docker engine one good tip is to use the user namespace remap option as described in the docs at that link to ensure that containers being run are in a separate username space than the host system and then for individual containers we can set the file system as readon if our application doesn't require WR access we can use the cap drop all option to remove all capabilities and then add back anything that we might need after that we can limit the CPU in memory to prevent a denial of service situation where one process is using too much of the resources and then finally we can use Security Options to set either set comp profiles or app armor profiles both of which you can find more information at in the docs throughout the course I've used a number of Docker CLI commands as we've needed them in this module I'm going to showcase the breadth of commands that exist and how you can use the help flag to get additional information the four main types of Docker objects that we'll be interacting with are images containers volume and networks all of which we've seen so far in the course for any Docker command we can do Docker d-el and it will give us information about all of the sub commands as well as a brief description of what those commands do in this case let's look at the commands associated with Docker images Docker images Docker image uh help and when you see all of the sub commands associated with Docker image we've seen the build command quite a bit it's how you take a Docker file and build an image from it the history command shows the steps that were used to build that image uh so I have a Docker file here that is just from Ubuntu 2204 run Echo hello command Echo goodbye and I built that into a image tagged with the tag New Image so let's do Docker image history on that new image and we can see my command is here at the top that was the last step in my Docker file I echoed hello here as the previous step and then these steps are the steps that were used to build that Ubuntu base image so for example I can do Docker image history on it and we'll see these steps within that history and so Docker history can be a good way to explore the steps that we used to build a container image the inspect command will show a whole bunch of detailed information about images it can actually be quite overwhelming uh so we can do inspect on that same image and we get this big Json dump with a whole bunch of metadata associated with that container image the import command allows us to take a tarball and create an image from it honestly I've never used that before the load command similarly can take a tar archive that was generated using Docker save on a container and create an image from it I don't find that very useful all my images I'm generally building from a Docker file image LS will list all the images on our system and so as you can see I have a whole bunch here some with tags some without tags uh prune is a great way to clean up those old images if I wanted to clean up my images I would do doer image prune it would remove any image not associated with a container that's what it means by dangling images I'll skip that for now the pull command we've seen is how we can pull an image from a registry we haven't used it directly that much because when we do a Docker run if the image doesn't exist it will first execute a pull but if I did want to execute that I could do Docker image pull and then an image name and tag and it would go off and pull that from the registry push likewise pushes our our built images to a registry we've seen that one quite a bit so far if I want to delete a single image rather than using prune to delete them all I can delete just one with Docker image remove new image for example now it's untagged and deleted from my system the save command can take an image and dump it out to a tar archive this could be useful if you wanted to have if you want to be able to explore the file system uh outside of a container I haven't found this to be that useful in my workflows and then finally we can use the docker image tag command to tag an image with a new tag so I could do Docker image tag then to 22.4 mytu tag Docker image list GP mytu and we can see I now have an image which is based on that Ubuntu 20204 but it has my custom tag associated with we saw this when we built the image with one tag and then tag them with new tags to push them to those to Registries in the container registry module that's it for the sub commands of Docker images however there's one more command that's associated with images that's not a sub command and that's Docker scan so I can do Docker scan uh let's just scan the aun2 image for fun 22.4 and that will use a tool called Snick to go off query of vulnerabilities database and assess whether there's any known vulnerabilities within that image it then outputs a report of all the different vulnerabilities that we've found things like system D op SSL uh these are at the medium severity level we've got one here at the high severity level it also tells you when in the Upstream project that vulnerability was fixed so then we can decide do we want to use this base image or do we want to pick a different one that potentially has this mitigated already that covers most of the things that you'll be doing when interacting with images let's look at the options for the docker container command now as you can see there are many more sub commands for Docker containers than there were for images I'm just going to highlight a few that I think are useful to know uh the attach command if you have a running container already and you want to attach your local shell to the input output and error streams of a container you can do that with attach a similar one to that is the exec command where rather than attaching to an existing stream you're going to run a new command within the container just like with images we can run the inspect which will dump a Big Blob of Json metadata that can have useful information the Stop and kill commands are how we can either gracefully or non-g gracefully uh stop a container so if you have a container that uh you've tried to stop but it won't die you can always use kill and that will generally do the trick you would want to issue that with either the name of the container or the ID of the container so if we do Docker run uh Ubuntu sleep 999 now that's going to sleep for a th seconds and even if I do control C it nothing's going to happen uh so if I want to get out of that I can do Docker PS uh we see that container here is running and then I would just generally do Docker kill paste that in and now our containers exited and we've got our shell back if you want to view the logs of a container you would do Docker container logs and then either the ID or the name of the container if you want to Trail the logs so if you want to have a live feed of them you can add the- F command this container here that you you've seen pop up when I do Docker PS it's a system container that you'll likely see on on your host but let's just check the logs of it since we're here we can do Docker logs of that container we also can do Docker container logs those are the same command and then finally if we wanted to tail those logs we can add the- F and now any additional logs would be printed to the console automatically the ls command lists all running containers and I believe is pretty much identical to Docker PS if we add A- a flag it will show stopped containers as well so without the d a it's only running containers with d a it shows stopped as well if we wanted to clean up all those stopped containers we can issue the prune command or if we wanted to remove a single container we can use the RM command the Run command is one we've seen over and over where we're creating a container from a container image top can also be a useful one if we want to see what's running inside the container so let's do a Docker run- DTU sleep 99 and now we have this container idid docker Docker top Docker container top then ID uh and you can see the processes running inside of that container and then finally the weight option would could be useful if you're writing a script where you needed the one container to finish before you progressed you could issue a Docker container weight command which would stop your script until that container exited then print out the exit code and you can proceed on with your script from there the volumes options are much simp we only have these five options we've seen the create option where you just pass it a name Docker volume create my volume we can use the inspect volume to look at that we can see where it's mounted within that virtual machine file system the name the driver Etc the ls command will list all available volumes marker volume LS we see a few named volumes here at the bottom we also see a bunch of anonymous volumes above it that are created and managed by Docker if you have data that you want to persist it's much easier to manage if you go with a named volume create that volume and then mount it into your container at runtime just like with images and containers we can use the prune command to remove all of our unused volumes or the RM command to remove one or more volumes now finally let's look at the options for networks they'll look very familiar given that they're almost identical to the volume options Docker Network just like with volumes we can create inspect list prune and remove the two new ones here are connect and disconnect and so if we had previously created a container but we forgot to connect it to the network that we wanted with the Das Das Network flag we could then run a Docker Network connect with the ID of the container to tie it into that Network so we can do Docker Network LS um let's say for example we wanted to do Docker run vuntu sleep 99 I'll run that in the background but we didn't want it to be on the bridge default network instead we wanted to connect it to my network so that it'll be able to talk to other services that were spawning there we can do Docker Network connect my network pass it the ID and now if we inspect my network we can see the listing of containers that are active there and we have this one named awesome Brown just confirm awesome Brown is the automatically generated name for that sleep container that I created if I wanted to remove the container from that Network for whatever reason I could use the do Docker Network disconnect command uh generally with these networks because I'm bringing containers up and down so frequently rather than starting a container and then attaching it I would just want to add that to my Docker compos config or as a option in my Docker run command so that it would be connected to the proper Network at runtime now that we're fully versed in the docker CLI how we can make the developer experience working with containers extremely nice what are the main considerations that we should think about in terms of the developer experience when building out applications like this how do we go from this confused frustrated developer at the top this excited pair of programmers here at the bottom we want an easy and simple setup we want to be able to iterate without needing to rebuild the container image so far even if we made the tiniest change to our code we would need to rebuild the container image and then rerun the container to have that reflected and that's just a terrible experience that being said we can can use the bind mounts that we learned about to mount our source code from our host system into the Container at runtime and then we can use hot reloading utilities like nodon or there's one for goang called a such that with each change of our source our applications will get automatically rebuilt and restarted and those changes will be reflected immediately we need to be able to debug our code as if it was running locally and for that we can add debugging utilities into our development image and set up a special Docker compose file to use those utilities we also want to be able to execute our tests within the container again I can make a custom Docker compose file that we'll use to do that in most scenarios the only containers that you'll build be building on your local system are the ones you're currently developing on for production images you'll want those built within a continuous integration Pipeline and finally it can be super powerful to be able to spin up an ephemeral environment with each pull request that you make that will have all of your modifications built into it automatically that you can share with a team member to get feedback and iterate on things more quickly when we get to that that's where our sponsor of today's video Shipyard dobu comes into play they make it very easy to take your Docker compos setup and get a fresh ephemeral environment built and deployed automatically with a simple GitHub event like a poll request so let's jump over to the code editor and take our initial Docker compose file that we built a few modules ago and modify it to improve the developer experience in these ways so we first built out our do ER compos file in module 8 so I'm going to copy that into module 11 let's just make sure it still runs I already have a make Target set up that we'll call uh compose up and I'm going to name this Docker compose Dev make compose up build which will look at that Dev compose file build my images and then run them uh it looks like uh line nine here where I'm using my present work directory it's no longer a valid reference to that V config file in instead I'm going to have to set the relative path to that like so okay so things appear to be working as expected just had to make that one tweak to the volume Mount i' would say if we're if we're thinking of that wish list that I just had up easy and simple setup is a check we're able to get up and running with just that single command now we want to improve upon iteration without having to rebuild the container also while this volume Mount syntax works I slightly prefer the more verbose version that we can specify here with a type A source and a Target so this should do exactly the same thing and now in addition to bind mounting in that configuration let's also bind Mount the entire source s code directory so that's going to be uh in this module 5 client react and so this will bind Mount the client react directory into our container at user Source app one additional point though is that within client react we have things like the node modules directory and the uh distribution directory which we don't want to get bind mounted in and so the way that we can overcome that is to specify another volume Mount that has no source and only a Target and so by doing this it will effectively overwrite this bind mount for this particular path and avoid bringing those node modules from my host system into the container and just to remind myself what the command is for Docker file 3 let's go look at that one client react Docker file 3 this is running npm runev which in turn calls V so we should have hot reloading built in and so now with these bind mounts in place let me start up my application again now if I navigate into my source code and make a change let's see what happens client react source app.jsx save it because we're bind mounting that Source in we automatically reload our application with feet and that gets built in and we see it immediately on the front end now let's take the same technique and apply it to our two API servers here we're going to use the same bind Mount approach for the uh node API including the empty volume Mount over node modules to avoid bringing those into the container and I'll put it right after in it true uh instead of client react we want API node and let's just remind ourselves what this Docker file 8 is doing okay so we're going to want to make a few tweaks here uh this Docker file is optimized for our production deployment but in this case we want to have a separate build stage that we can Target for this development environment I'll start by naming my first stage uh as space and then where we diverge from our production versus development is going to be at this step where for production we only want to install the production dependencies so I'm going to say from base here and then I'm also going to have uh as production and then I'll do from base as Dev and so now within this stage we can have our development commands so our development commands are going to look similar uh but instead of npm C only production uh we'll do npm install and that will install both Dev and production dependencies let's also move this environment variable down such that we set that only in the production image after we've installed our dependencies we want to copy all of our source code in and then for the development stage we want our Command to be uh npm runev uh and I'm actually going to call this Docker file number n just so we can keep them keep track of them uh so we have our Dev stage where we're installing development and production dependencies we're copying our source we're running npm runev and then we have our production stage where we are doing what we were doing before with the non-root user installing only production dependencies uh Etc now in our Docker compos Dev let's use that new Docker file 9 and see if we succeeded oh one thing I forgot to do is in addition to a context and a Docker file you can pass a Target and that's allows you to specify which of your stages in your multi-stage build you want to Target so in this case it would have defaulted to production as the final stage I want it to Target that Dev stage because that's going to be where we're running nodemon for the auto restart so now let's make a change to our node API Source here Source index uh in our response let's just add Tada so we save it now refresh and we see our node reserver was detected noemon saw the change restarted uh and now when we hit that API from our front end we get the updated version back and so we've got hot reloading working for our front end in vit as well as our uh node based API let's do the same for goang our bind Mount setup should be quite similar a volume going to API goang uh we don't need the node modules Mount depending how you have your go path set up uh that will determine where you're going to install your dependencies on your host system uh in this case it's outside of my project directory so I don't have to worry about shadowing that with an empty volume however my target inside the container instead of user Source app if we think about the work directory that I used in the docker file it's just slapp uh and now I need to do something similar to my production Docker file add some development stages so that I can have both my production ready and my development images make a new one called Docker file number8 make sure we're using the new one and we're going to add a target of Dev and we will get around to adding that here uh so my first stage already has a name but now because I'm going to have I'm going to split this into two build stages so I'll have build base as this it should be the same up until here where we download our dependencies however this is where we want to split and so I'll do from build base as build Dev and so for our Dev build in addition to building in addition to pulling in our dependencies that we've specified there's two more dependencies that that I want to add from a development perspective and that is a package called air which enables a hot reloading so it'll detect a change like noon rebuild that application and then restart it as well as a package called delve which allows for remote debugging that we'll set up so we can uh do set break points and connect to our container remotely to investigate the state of all of our variables and that sort of thing and so the command to install those two dependencies is go install pass it the name of the package and go install pass it the name of the package we still need to copy in our source code and then we'll set a command using that air package so we'll call Air the and pass it this default air. toml which I created ahead of time and it just tells it sort of thing things like where the main file is what command it should run when it needs to rebuild where to look for changes Etc and so that can be our build Dev stage uh that will be actually I'll just call it our Dev stage like we did in the docker file and then for the remainder uh we build our production images and then copy them into that final scratch image so I'll do from uh build Bas as build production then down here when we copy from build instead we're actually copying from build production uh we'll add this non-root user within the build production stage and so that should be good we're now targeting our Dev stage we're bind mounting in our source code we're using that air package to do automatic reloading uh let's try it out okay it looks like our app has started up so let's uh load here uh we can see it's running let's make a change to our goang API just to see that rebuild happening if we go up to module 5 main.go and add something here we see it detected main.go is changing uh it just rebuilt and so if I reload we get goang woo awesome I'll save it again you can see it's detected it it builds it I refresh that update is reflected so now we've got hot reloading within our containers by bind mounting our source code in and then using utilities like nodon vit or air to automatically detect file system changes and update our application in real time now the next item on our wish list is remote debug so we want to be able to run a debugger within our containers and then connect to it from our development environment as much as we love doing print F debugging where we put in a bunch of print statements sometimes you really need to use a debugger uh and investigate a little more thoroughly so let's get that set up now what I'm going to do is create a new Docker compose file that I'll call Docker compose debug and one interesting feature of Docker compose is that you can take two files and pass them both into the docker compose up command and it will take the first file and then take the second file and overlay it you only have to define the fields that have changed between the two you can have your base configuration and then slot in slight differences and so in this case we're going to be able to leverage that Docker composed Dev file almost entirely but we'll add a few new Fields mostly adding additional ports to publish because we're going to have to connect on New Ports that the debugger is listening on and modifying the command that's executed when the container started uh we'll start with our version of 3.9 Services uh the services that we're modifying here are the API layer so we'll have API node and API goang we want to override the command field so we'll have a command here and so instead of npm run Dev like we were running before uh we actually want npm run debug Docker and I added that as a new npm script so we can go take a look at what that is npm debug Docker runs nodemon so we still get that hot reloading uh but the Das Dash inspect command tells node to run its debugger here we're listening on all hosts on Port 9229 so this will enable us to connect from our Local Host system on Port 9229 as long as we open as long as we publish 9229 in our composed file and then we're passing it that index.js file as the entry point for the program uh so here we'll do ports and I'm going to have both my normal 3000 Port that I had before as well as my new debug port and so when I run this command if I do a Docker compose debug build I do Docker compose and then I first pass it my development compose file so that's my Docker compos ddev and then I pass it my debug compose file Docker compos debug and Docker compos takes those two and interleaves them together such that these will be the new values but we can still take advantage of all this additional config here our goang API will be somewhat similar the debugger that we're running for goang uh runs on Port 4000 it's a tool called delve which was that other Dev dependency that I installed and our Command I'll just copy and paste it here because it's a little bit long uh uses that utility in debug mode we pass it the main.go file that should run we tell it which Port we want to listen on as well as some additional configuration options and so that should be enough to run our application in debug mode so I can do make compose up debug build as I showed it passes the dev file first then the debug uh composed file so while that's building and setting up I can show you the launch. Json configuration that I use and that is the way that VSS code sets up its debugging utilities so under the configurations block I have two of them I have one set up for node with Port 229 one set up for goang on that Port 4000 uh the key things to call out here are that I'm using the request attach mode because my container will already have that debugger running inside I don't need to spawn a new one I just want to attach to it and then this local root command tells VSS code where the root of this project is within my workspace and that's so that when I create a breakpoint it's able to map that to a path within the container which I'm passing here of user Source app oh I see I hit an error here it's because I forgot to change uh this port for the goang API to uh 8080 so let's try that again similarly for goang uh we're using a particular adapter that is associated with that delve again we're using the attach request option specifying the port giving it a a remote path and then this one's this one was a little more tricky to get set up again we need to figure out we need need to be able to map from our host system into the Container such that it's able to know when we set a break point what that corresponds to and properly apply it inside the container and so the substitute path command takes the path on the local system and modifies it such that it gets fed in properly inside the container my app is now running in debug mode I don't have any breakpoint set or anything so it just loads as normal however I can go into the run and debug portion of my uh code editor and pick one of these so let's pick attached to node click the play button aha uh it was not connecting because it was forwarding my debug request to Port 3000 where the app is listening and so instead I should be forwarding to the port where the debugger is listening now that the API have started up I'll click play and now it did successfully attach we have our breakpoint in in our source code just before it sends that response back if I reload here we see it caught at that break point and we can see all of the local variables including the response itself Wednesday February 23rd API node 1257 so if I then click skip over the breakpoint or play it will resume and allow that to be returned from the API so let's click play and we see we got back that response I'll click disconnect here and let's try the same thing with goang click the play button looks like we were able to attach let's load our goang API and set a breakpoint somewhere within our goang code under main.go uh let's do right before we send back our data breakpoint we can see it being injected here in the logs and now if we load the page once again we successfully caught at the breakpoint uh and we can see the locals here uh we have our time data as well as our gin context and now we have debugging working for both uh the goang and the node-based API uh I also added a react query debugging tools as a de as a Dev dependency that get built in here allows us to get all sorts of information about the uh queries that react query is making to the back end uh so for example if we load here we can look at this query when it was last updated uh the data that we got back we can tell it to refresh refresh the data and so without having to reload the whole page we can interact with the react query specific components directly that gives us debug capabilities for all of the services that we're building so that's awesome and now the next thing our wish list was to be able to execute tests with inside the containerized environment similar to how we had a overlay compos file for uh debugging I'm going to do I'm going to make us another one called Docker compos test uh we don't need the port access we will be overwriting the command uh we'll just execute our npm run test script which will execute just which in turn looks for our test so I can just show you the test Suite that I have set up it's it's quite minimal so API node test we have one example test that expects true to be true so that's always going to pass and the package.json specifies uh test calls just which will discover the test and execute it and then for goang similarly minimal uh I have a test module with exampl test. go uh it tests that 1 equals 1 which indeed it does and my command that I'm going to use there is go test so here instead of using delve I'm going to run go test and then I'll pass it this V option which will enable it to properly discover those tests so now in order to run this I have two make targets set up or I have uh Docker composed calls for goang and node separately so again I'm going to use the dev compos file first overlay that test compos file onto it and then I can specify run build and a specific service within that compose file and so this one should execute the goang test and this one should execute the node tests so if I do make run tests it's going to build those images and execute those tests so we see the just test passed and let's see let's go find the yeah here's the goang test here so the goang test pass the just test passed and we were able to execute those tests within the containerized environment the next item on our wish list was continuous integration uh and so that essentially refers to this idea you whenever you're pushing code to your virsion control system you should be running pipelines to do things like execute your test Suite uh build and push your images Etc GitHub actions is a continuous integration system that's built into GitHub and you can just add a couple of files to your repo and it makes it very easy to set up this type of thing I've added a workflow to the repo that I can show here that is going to do a few things it's going to build our image it's going to scan it tag it and push it to dockerhub um I'm only doing this for one of the services just as an example but you can use the same Concepts to add it to as many services as you want within the repo you place GitHub action files at the GitHub workflows directory and then any yaml file in here will be interpreted as a workflow with about 50 lines of code here we're able to configure a GitHub action that will build our images scan them test them Etc So within this file we have a number of different sections I'll just walk through them now this first section starting with on tells GitHub when it should run this workflow in this case I've specified you want to run it on push events to specific branches and tags so if I push to the GitHub action branch which I'm on now uh it will trigger this workflow or if I push a get tag with starting with the letter V so maybe it's vx. y.z so some sver version when I do a release you can also do other types of events like a cron string so you would run it periodically based on a schedule or upon creation of pull requests below this you define your job section where you have the different jobs that you're going to run and then within each job you can have a number of steps and so I've only defined a single job I'm just going to build tag and push my image you specify which uh runner you want to run on I'm running on auntu latest and then the steps that I'm going to run are executing first checkout which is a public action provided by GitHub which checks out the code I then am using a public action from Docker to generate a bunch of tags that I want to use for my image using information such as the name of the branch the name of the pr the version if I'm pushing a tag I can use that to generate a Docker image tag and then finally I'm using the date and the commit hash as another example of a potential tag that I might want to use like I talked about when we were looking at container Registries here I'm giving it the name of the repo that I'm going to push to and these actions these public actions that I'm referring to are available in the GitHub action Marketplace and so this was an action that was produced by Docker um you can see the source code here for it but if we go to the action Marketplace uh we can see here it is it tells us all the different ways we can use it the types of inputs the types of outputs Etc so I'm able to specify that this step should use that action give it these types of tags and will automatically generate those for me next again I'm going to use a public action from Docker to log in dockerhub and within my repo I can specify my username and my access token such as it's table to do that so within the repo within the repo under settings under secrets and variables actions you can specify environment Secrets or repository secrets and so here I specified my dockerhub username and a access token such that that action is able to log in next up again another public action from Docker so really I'm I'm leveraging their work to make my life super easy this one is going to point to the file that I want to build as well as the context with which I want to build it I'm saying I do want to push it to dockerhub and then I'm using the output of my metadata step here and applying those as tags so this is going to be a listing of all those tags that are generated this will apply them all and push them all and then finally I'm leveraging a tool from Aqua security using their vulnerability scanner to look in the image for any critical vulnerabilities you can specify what level you want to block on here and so I'm saying if there's any critical vulnerabilities in the image fail the run and that way I can go investigate uh if there are no critical vulnerabilities it will pass the run so I haven't made any changes on this Branch uh but I do want to trigger a run so I'm going to do an commit I do get commit DM empty allow empty get push and now if I go to my repo under actions we can see that empty commit just triggered a build here's my job if I click into it I see it starting up that Runner and and then each of my steps we can see it's checked out my code it's generated the tags so let's look at the tags that it generated we've got GitHub action is the branch I'm on uh latest is just the default and then this is the this is today's date as well as the short hash from the commit we successfully logged into Docker Hub we then successfully built and pushed our image we ran the scanner and found no critical V vulnerabilities and then there's just some cleanup at the end so now let me go to dockerhub we should be able to see for our API node repo this version was just posted a few seconds ago and so that kind of highlights The Continuous integration workflow that you would want to set up for your container images that's just a taste of what you can do with GitHub actions and with continuous integration pipelines uh Brett fer has a repo called Docker cicd automation that has a whole bunch of really cool examples around uh doing things not only building and testing and pushing your images but actually interacting with PRS adding comments to those PRS with the results of the tests and that sort of thing you can see his talk that he gave on this on YouTube I would definitely suggest checking that out now the final topic that I want to cover here in the developer experience section of the course is ephemeral environments ephemeral environments are this idea of building buing and deploying short-lived isolated environments that we can use for any sort of purpose including testing validation QA Etc and in particular I'm going to use this platform from Shipyard to very easily set up this for our sample application I'll start by just walking through their onboarding process and I'll show you with just a few lines of configuration to our Docker compos file we can have ephemeral environment set up with every new PR to the repo I'm here on the shipyard dobu website I click log in It'll ask me to log in with my gift Hub I'll do that you'll authorize Shipyard to access your account this now redirects me back to the shipyard site where I will connect my GitHub organization so that initial connection just gave them access to my user information now this is the authorization to actually access my repos you can choose either all repos or only select repos so here I'm going to pick the specific repo that we care about which is the docker course repo this will grant them the necessary permissions to the contents of the repo they also have features where they can provide information back on the GitHub interface that's why it has read and write access to checks commit statuses and pull request at this point I'll just click install now as we can see we're on the free trial period as I mentioned during the introduction students of this course have a exclusive promotion code where the first 300 people to sign up can get an extended trial period so go check out the description for more information on that and then at this point we can click the we can click this scen button to add our first application we'll select the repository that we care about it's our Docker course repo and for our base environment I'm going to use the main branch we can use the default name for application and now if we click the select Services button it's going to scan the repo for a Docker compos file in this case because of the structure of the repo based on the course it wasn't able to find specifically which one we want I'm going to choose the one that we set up initially in module 8 because that's the simplest Docker compos file that contains all the services we care about I'll just click select and now we need to pick which of the services in that Docker compose file we want to include here I'm not going to include our V based deployment instead I'm only going to have the engine X based front end um we'll leave on our two apis in the database here and then we can see that it automatically Det Ed our named volumes and an interesting feature here is that because we have that named volume whatever's stored in that volume on our main environment will get snapshotted and copied over to uh additional environments as we make PRS against that at the top here it's saying please include at least one service with a route and so we need to provide Shipyard a bit more context and configuration so that it knows how to route traffic to our various services and the way that we do that is by adding labels to our Docker compos file so let me jump over to the code code editor and add in just a few lines to our Docker compost file so that Shipyard will be able to properly detect and utilize those routes and so each of our services we can add a labels field and so here under the engine X client react enginex field I'll add a labels Target and add these two labels the first one is specifying that this is the primary route this is the route that we want traffic on the root path to come into uh and then we're specifying the route as just the forward slash so this is saying this is going to be the default service that we want you to Route traffic to and so with those two things that's good um we'll adapt for our API node Service uh we'll add this this is not the primary route so we'll get rid of that and we will add this going to slash API slash uh node just like we had before and then similarly for our API goang we'll go here uh and just replace node with goang and so with just those I think it was additional seven or eight lines of uh code into our Docker compose file we can now commit that to the main branch push it now if we go back to Shipyard and reload our latest commit now we can see that it automatically picked up those labels and it's including those in the information about our configuration here with that I'll add environment variables all the default values here should be fine we don't need any additional environment variables and click create application this is now automatically going off and building all the containers based on the latest commit to that main branch it will then bundle them up and deploy them now you'll notice that the first time you log into Shipyard you'll get this message at the top that's because it's provisioning a standalone kubernetes environment for each customer uh and that allows for isolation between customer data however it does take a few minutes to set up and so the very first time that you you're here you'll have to wait about 10 minutes after that the build and deploy should be much quicker now while we were waiting realized that I should have added one additional configuration here uh for our API layer and so that is that is the shipyard route rewrite label and this is saying if we hit the URL API node our node service is expecting that to come in at the root path and so by adding this label it will strip out that prefix uh and we'll do the same thing for goang because the routes that I've defined inside of my services are not expecting that prefix so I'll add those and push them now another neat feature that we can turn on is if we go here to this uh three dot menu and click under configure under notifications and general settings we can add GitHub commit checks and GitHub PR comments so the when Shipyard has information for us it will populate that back into GitHub so that we can see it from there we also can tell it which pull request we want to deploy on so we could have specific labels let's say we only want to deploy on bugs or we only want to deploy on enhancements or we can specify a r a regular expression to decide which branch names we want to deploy with in this case I'm going to leave these as the default so that should deploy any PR back to the main branch I'll click save there's also a slack integration you can use uh to get updates and notifications from chipyard I'm not going to go into that now but I just wanted to call it out as an option a few minutes later back on our home application page we can see that our base environment has built and is ready to be visited now you'll also see this little timer here that says stopping in 2 hours and 49 minutes so that's a feature that is called SLV or since last visit and you can configure that within your organization to tell Shipyard how long an Emeral environment should live and by putting environments to sleep if they're not being used allows you to have more ephemeral environments on a given tier within their platform if I click the visit button it's going to take us to this page where we get a login prompt so we can log in either using GitHub or Google for ooth and this allows you to have these ephemeral environments not public on the internet but actually protected behind this login layer where you can very finely control who has access to it so I'll click to log in with GitHub and we can see we've got our application running within their environment and if you look up here at the URL we see our custom URL that's associated with this particular ephemeral environment now if we wanted to if we wanted to Grant access to this we could go to the visitors Tab and under here I can add GitHub usernames or Google usernames and that way you can control who has access via single sign on so that's our base environment now let's make a slight change to our application and then redeploy base on a pull request event I'll go back to my code editor uh and let's have it deploy with a different message on that home screen so I'll go into my example web application here on the front end in my app.jsx file I'll scroll down and instead of hey team we'll say hey team from Shipyard and we'll change the wave into a ship awesome uh now I'll do get checkout DB Shipyard demo and I'll commit to this new Shipyard demo Branch change message now I'll push that now if I go over to GitHub I can see that recent Branch had a push I'm going to create a pull request from that Branch back to the main branch if I click create pull request and now jump back over to Shipyard we can see it immediately detected that pull request and is now building a new ephemeral environment specifically for that new Shipyard demo Branch once it finishes building we'll be able to see that is different than the original Emeral environment with that updated message clearly a change that small is quite trivial but it could represent any number of complex changes to our different Services we could be changing the API layer we could be changing our things like our database schema let me go here to the build details dashboard and show you how that's progressing it shows us the various services that we have within our application that we selected initially we can see the build logs as it ran our Docker build commands and looked within the docker compos file found the corresponding Docker files and built those out we can see all the logs here across our different Services now under the deploy tab we can see the pods as they come up and then under the Run tab we can see the logs coming out of the containers themselves it's now handling that final DNS setup so that we'll be able to access the environment and now we have our second ephemeral environment live let me visit that one as well as we can see it's has taken that change that we made in that PR and deployed it in a separate environment from our base environment there's an additional interface here if we click terminal it'll pull up a way for us to get additional information about all of our environments so we've got our main environment we've got our Shipyard demo environment let me just click into the main one and we can see the pods running here within that environment I can choose one of them if I select it we'll get the logs here at the bottom we can also hit the r key if we wanted to exec in and do some debugging that way and so now this is a shell running inside that pod within the environment I'll exit out there's also a CLI that I can run locally to get a lot of this functionality on my host system directly from my laptop let me jump over and show you that CLI real quick to install the CLI uh they provide either a bash script that you can use or you can use something like Homebrew to install it I've already got it installed and then the next piece is to get your Shipyard API token and so in order to turn on the CLI for your account you'll need to ask the team at Shipyard to do so but once you do you can go up here to the user and then click profile and it will show up on that page and you can grab that API key to use with the CLI we can see here all the different functionality that we have so I can do something like Shipyard get environments that's going to list out all the environments that I have and let's say I want to get the logs from one of my pods like I could see from the web interface I can do Shipyard uh Logs with the N flag I can pass The UU ID of that environment and then with the service flag I can tell it which service I want so let's do API uh node and we can see the log's coming out of my node based API application uh similarly we could port forward directly to one of those applications so let me just port forward to that one so instead of logs I'll do port forward uh and then my node application is listing on Port 3000 so I'll do 3,000 uh it wants the port flag and now my Local Host 3000 should be forwarded to that node-based API and I can go here and I'm connected directly to it this would be super useful if I wanted to do something something like uh maybe connect to my postgres database directly with PG admin or otherwise I could port forward to that environment uh make whatever changes I needed to let's say I was building out my base environment and I needed particular seed data and I wanted to run my migrations I could do that manually once on my base environment and then those changes would get snapshotted because they're in a named volume and built into my additional environments moving forward so hopefully this gives you an idea of the power of ephemeral environments and the ease with which we were able to use the shipyard platform to generate them I created That Base environment from my main branch and now every PR that I create moving forward we get an ephemeral environment to validate before we deploy to staging or production if you a call we turned on the ability for Shipyard to populate information back on GitHub so let's go see what it did on that PR if I go under pull requests demo Shipyard we can see that the shipyard app created this comment and tells us the environment was successfully built and running we can access it at at this link or see the build details at that link and so this just makes it very easy for someone reviewing this poll request to now go check out what exactly these changes are and how they impacted the application hopefully that demo kind of give you an idea of the power of ephemeral environments and why we would want to use them within our software life cycle and showcase the ease with which we were able to use the shipyard platform to build those out for our project at this point we've got all the tools within our system to have a really nice developer experience with containers and now now let's shift gears and actually learn how to deploy them into the cloud all right congratulations we've made it to the final module of the course and that will be focused on deploying containers so we now know how to build container images run containers from those images we've learned about how to do development with them and now we need to build on those skills and learn how to deploy our containers to production now one of the main benefits of containers is that they provide the standard interface upon which to run as long as you can provide a container image that meets that open container spec there are many cloud service providers that will happily take that container image and run it for you I saw this article back in 2021 from Corey Quinn who runs the last week in AWS newsletter talking about the 17 different ways to run containers on AWS now obviously AWS is kind of this sprawling massive cloud provider but even still to have 17 different ways to run containers uh ranging from container-based serverless options like Lambda uh to elastic container service to running kubernetes with elastic kubernetes engine uh there's just so many different ways to run containers and this is just within one cloud provider and so that portability is both a blessing and a curse meaning that it provides us this flexibility but also it can be a bit overwhelming in terms of deciding which of these methods to use and now what do we actually care about when it comes to choosing a method to deploy our container based workloads I'd say the first item that's top of mind for me is security and whether either the cloud provider is handling security out of the box and setting all those container runtime options such that it meets the threshold we need or they give access to those knobs and dials that we can set them ourselves the next one is the ergonomics or the developer experience so how easy is it to go from uh having a set of container images and running those and getting traffic from the internet to them also how easy is it to deploy new versions so if I make a change to my code how does that code make its way into production and then the third one in terms of ergonomics is how do I work with the running containers how do I check the logs uh can I monitor them can I collect traces easily combining those three elements make for either a good or bad developer experience when when working with any of these platforms the next one is scalability so does the platform meet my needs in terms of the scale of any individual container or the number of total containers required for example can I access particular types of gpus uh what if I need to scale beyond the capability of a single host can I launch containers across multiple hosts and how do those Services then communicate with one another another key consideration is persistent storage so we've talked about vol volumes within Docker we've mostly focused uh from a developer perspective with Docker desktop and seeing how those volumes get stored on your laptop or desktop uh however if we're now running containers across multiple hosts and they're getting scheduled and they could be scheduled on one versus the other we need to think about how storage works and if it's possible for those storage to be accessed from different virtual machines or different physical machines and the last one that I have here is cost how affordable is it to run your containers in this particular environment these range from sort of bare metal where you're paying just for the compute but handling all the operations yourself all the way up to maybe a fully managed platform as a service offering where they're charging you a premium for some of those improved uh ergonomics or developer experience features so thinking about how much it's going to cost you understanding the financial impact of your platform will be important if you're running a business based on these containerized workloads now when I was planning out the course uh I actually built out three different deployment targets so one using railway. apppp one using Dockers swarm and one using kubernetes uh since we're already at about 4 hours into the course I've decided to shift that slightly uh within this video I'm going to demonstrate the docker swarm deployment and then I'm going to create separate bonus videos uh showcasing how to deploy onto Railway and how to deploy onto kubernetes for the the sample application that we've been working with uh and once those bonus videos are released I'll have information in the description about how to access them now at this point you might be asking why can't we just take the docker compos file that we created in module 8 uh create a virtual machine and run that directly now while that would work and you would have your containers running on that host and you could set up the firewall to access them it's not ideal there's some major limitations with Docker compose uh that prevent us from wanting to use that for production workload for first there's no way to deploy a new version of our containers without having to stop and then restart the container and so by definition you're going to have some downtime there it might be very quick on the order of a second or two uh but nonetheless you generally want to avoid downtime if you can and then also there's no utilities for easily rolling back a version uh with Docker compose directly also Docker compose uh doesn't have a native way to handle credentials properly we've been passing in the that database URL or the postgress password as an environment variable there's no there's no mechanism in Docker compose to handle encrypted secrets and so that's a major limitation there also you can only run Docker compose on a single host uh so that hasn't been an issue as we've been developing on our development machines however we would like the ability to expand Beyond a single host and add things like high availability in the future and so Docker compose is limited in that way there's two other reasons you might have historically heard cited for not using Docker compose in production uh the first is that it used to not support health checks and so we'll talk about what health checks are and add some in that is no longer true they've added health checks to Docker compose relatively recently and then the other one has also been mitigated is that Docker compose used to live as a separate binary uh when you would run it and so it was just harder to install and config now that Docker space compos rather than Docker Dash compose is built into the docker binary itself that's no longer an issue either you may hear people reference these as potential reasons just know that historically they were reasons not to use Docker compose in production now I've phrased these as things that Docker compose doesn't have however as we'll see these are features that Docker swarm does have built in we can configure a deployment strategy to have zero downtime deployments we can roll back changes if we want to we can handle credentials via a feature called Secrets uh and we can scale a swarm cluster across many hosts to keep things simple I'm going to deploy a swarm cluster on a single host uh so it's going to be quite similar to the conceptually to using Docker compose up uh we're going to create a swarm cluster consisting of a single virtual machine and then we're going to modify our Docker compose file and you'll see that the actual changes in the configuration are quite minimal uh there's only a few things that we need to tweak to make our composed file compatible with swarm uh we're going to deploy that and then we'll set up DNS so that we can access it via the Internet what's the process going to look like I'm going to create a virtual machine I'm going to make sure that the firewall set up for that virtual machine allows inbound traffic on the necessary ports we'll install Docker engine and so I'll use the script located at get dod.com I'm going to continue to use the docker client running on my laptop but connect to Docker engine running on the virtual machine using this Docker host environment variable so we'll connect over SSH and have the client running locally but Docker engine running remotely we'll initialize the Swarm we'll modify our Docker compose file to add health checks uh this allows swarm to periodically ping our service and make sure it is healthy and if not replace it or restart it we're going to add a deployment strategy which allows us to deploy with zero downtime then we're going to add support for secrets so that we can handle uh the credentials for that database properly finally we're going to build and push our container images to dockerhub previously we were using Docker comp osed to also build our images for production based setup we want to have a specific image tag that we're running and so I'll build and push those and then finally we'll deploy that stack onto the virtual machine and see it running now at this point I also want to issue a slight disclaimer about the way we've configured the database here we're going to be deploying our database alongside the applications into our Docker swarm cluster now that's perfectly fine to do and we do have a Docker volume to persist the data such that it will uh exist Beyond container restarts however we're not implementing backups or anything like that and so if you have an application where your user data is very important you'll want to either Implement a more robust setup that includes periodic backups and the ability to restore from those or move the database to something like a database as a service from one of the cloud providers Amazon has their RDS service Google has Cloud SQL Etc okay so I'm just going to create a virtual machine here on SEO you can use whatever cloud provider you prefer I'll call it Docker course swarm we want one instance I'm going to choose the small machine type I'll use auntu uh let's use the auntu initial user uh we can use the default Network um we do want a public IP address uh we will create a new firewall and we'll use the SSH key that I have already uploaded to my CFO account I'll click create that will take a couple of minutes to spin up uh let's look at the firewall here so right now it's using uh default default I just want to make sure that I have inbound traffic allowed on Port 80 uh so I have this Docker test firewall that I created uh when setting things up let's look at the the rules uh so this is allowing inbound traffic on all ports let's actually turn that off uh we do want to be able to ssh in on Port 22 uh we're going to have web traffic coming in on either Port 80 or 443 uh let's also open up access for our two API ports uh so in this case it would be Port 3000 and Port 880 for the goang API just so we can access those directly now let's go to our VM and attach this firewall to it so instead of default default I want to attach it to Docker test uh and now I want to install Docker engine on this machine so I will SSH into it uh so we created that Ubuntu user it should be using my SSH key that I already have set up the first time connecting you have to accept the fingerprint and now we are connected to that machine if I just try to run Docker it doesn't exist however I know that there's this install script for Docker engine here at get. do.com so I'm just going to do curl on that pipe it to Shell and this will go off and it has conditions within it to install for Ubuntu or Debian and so this is a Ubuntu machine and it should install properly and so Docker is installed but if I try to do something like Docker PS I'm unable to connect to the docker socket I need to change the permit of that socket to be accessible by my current user and so I can do that with the CH own command and now if I do Docker PS I'm able to access Docker so I'm going to exit my SSH session uh however I can still connect to that remote Docker engine by using by using the docker host envirment variable so I'll export Docker host uh and in this case it will be Ubuntu at at that IP address and now even though I'm running this command Docker PS on my laptop it's actually talking to the API of that remote Docker demon we want to enable swarm mode on that remote VM so that's as easy as doing Docker swarm in it and we now have swarm running on that remote host next up we want to take our Docker compose file that we created in in module 8 and update it such that it is ready to go with swarm so I'll snag that Docker compose file copy it over into module 12 and so what are the modifications that we want to make here uh we could just run this directly like I was saying however there's a number of improvements that we want to make so for a production based system uh and let me actually rename it instead of Docker compose let's call it Docker swarm am uh there's a number of things we're going to do first of all we don't need our V based front end because that was a development server anyways so we'll just get rid of that entirely uh next up we're not going to be managing uh the building of our container images via swarm uh so I can get rid of this build section and instead I created uh repositories on dockerhub for each of my images and I'm going to build and push those images to that and so we can see here I've got my Docker course API goang API node and client react engine X in order to build and push my images to those I have this make Target uh that is going to navigate to module 6 and then build version 5 9 and8 of the client node and goang uh Services respectively and so that's effectively going to go into here into each of these directories and execute the build and and push and make targets and so if I do that uh this image that I'll be using for the front end will be Sid palala devops directive Docker course client react enginex version 5 the API node once again will clear out the build portion uh and we'll specify to use the image and this will be Docker course API node 9 was our final uh Docker file and then the final one uh API goang will be goang number eight uh clear out the build uh we're going to take inbound traffic on Port 80 and forward that to 8080 CU that was that unprivileged engine X container running on 8080 uh we'll take traffic from 3,000 forward to 3,000 880 on the host go to 880 in the Goen container let's run this config file using Docker swarm on that remote host we already initialized the Swarm uh in this case I need to update this IP address so this was the IP address of the virtual machine I used when testing now I'm going to set it to the IP address of the one I just created uh we've already initialized I'm going to use this uh Docker stack deploy command uh and then pass it the name of my uh swarm file so I'll do make Docker F swarm deploy stack uh because I didn't specify a composed file version in that file I actually need to do that and so at the top here I'll specify that this is version 3.7 let's try it again uh and it's warning me that the docker compose option restart is not supported by Docker swarm uh and so I'm just going to remove those it will handle restarting our services slightly differently uh and the great thing about something like doer swarm is it's declarative so now I can just redeploy the same stack over the top and it will reconfigure my services accordingly now if I do Docker PS we can see the four containers that it's spinning up uh and let me try to access them uh just at that IP address so the IP address was 212 all right and so I now have my containers running remotely on that virtual machine using Docker SW however there's still some improvements that we need to make right I talked about handling of credentials here I'm still passing in those credentials via this environment variable so we need to fix that uh by using a feature called secrets and so generally you would create these secrets via some other process so you wouldn't Define it in your config directly because you don't want to store those credentials with in your get code base and so I have a make Target that we can use to create those Secrets um it essentially uses the docker secret create command uh pass it a name of the secret and then here I'm echoing in the string that will get stored within that secret uh so if I do Docker secret list there's no secrets currently but then if I run this make Target and run it again Docker see secret LS we now have these encrypted Secrets living on that virtual machine that contain the information that we care about so food barbaz as our password uh and then the URL string containing that information as well now in order to consume those Secrets uh we're going to add another field here at the bottom called Secrets uh the first one is database URL and we're going to going to say external true and what external true is saying is that we created this secret already and Docker swarm doesn't need to manage it as a part of this stack we'll do the same for postgres password external true great and so now that we've told Docker swarm about those secrets we need to tell the services that are consuming them to do so and so what I'll do here is we can tell this database service that we want to consume the postgres password uh we no longer need the postgres password here I will change it I will specify the postgres user to be postgres so the way that Secrets get consumed is is as a file mounted into the file system um using a a temporary f a temporary Mount structure where that file only exists in memory and so I needed to adjust my application slightly whereas previously they were expecting to read those values from an environment variable now I'm expecting to read them from a file and so I need to tell the application where that file lives and so in this case uh that file will get mounted in to to this location basically under SL run/ secrets and then the name of the secret and so I'm going to tell my application this is where you should go look and read in that that password from the file uh similarly we will have for our apis something very similar so we're telling it we're telling it the path to the file where that Secret's going to be mounted in uh in this case this will be database URL and there we go and then let me just show you real quick the modification that I made to the application so that it can either read those sensitive read those Secrets as environment variables or as files so if we go to our node.js application here Source database and so when I'm populating this database URL variable I either load it from the environment or if that doesn't exist then I read from the file system at this path that I'm passing it very similar on the goang side where when I'm initializing I either get the database URL directly or if it's empty then I'll read the file and use that instead and so now I can redeploy using those secrets now also it takes my swarm config file and turns that into a stack with many services in it so I can do make swarm LS and that's going of list the different services and we see that we have our different replicas here and they all appear to be running except for the goang one let's reload our application and so it looks like our front end is still working but our apis may not be so if I navigate to and so let's take a look at the logs for those containers to see what might be going on let's look at the node API logs uh it looks like we are refusing to connect So within the the node container I can print the environment I can see that we have this database URL file that looks good um what happens if I C that file out let's see uh okay so it looks like within so it looks like when I tried to use this dashn option for the echo command to avoid new lines it actually included that in my secret file which is what I didn't want to happen and so I'm actually going to remove my stack make swarm remove stack so it looks like the dashn I was using to remove the new line at the end of the file was actually getting included in the secret so I'm just going to use print F instead do make create Secrets uh and now let me redeploy my stack let me uh make sure that the secret looks okay within those within those containers uh so in this case let's cat this out within the postgres container that looks good and then within let's say the goang container or no we'll do it within the node container because the goang wouldn't have cat within [Applause] it okay so that looks much better and now do my services look healthy uh they do and if I go to my IP address address and refresh I can now access my application again okay so minor dour there uh how I was creating those Secrets there was an issue with the that dashn was making its way into the secret itself and so I fixed that by using print F and now the secrets appear to be working properly now the next thing that I wanted to do is to add a an ability to deploy new versions without bringing down the old version first and so for that we actually use the deploy configuration option and so for our front end we will add a deploy section and within this section there's a few things We'll add we'll put it on replicated mode with a single replica so we could change this to 10 and it would spin up 10 copies of our front end but we just need one for now uh and then the key configuration here is update config we want to start the new container before we terminate the old container so that's what this order says you can actually do stop first if you want to do the reverse and so this is how you this deploy configuration is how you tell Docker swarm how you want to do rolling updates uh we'll have pretty much the same configuration for our other services as well uh let's make the goang application have two replicas just to show how easy that is to do and then the final piece of the puzzle that we wanted to add is health checks uh and so we want to have a way for Docker swarm to know whether our containers ready or not so that when it's checking should I bring down the old container is the new one ready to accept traffic the way that we do that is via health checks now the health check config uh is relatively straightforward you use the health check key you give it a test and in this case we're saying run this command inside of that container so it's going to try and curl uh Local Host 8080 and then I've configured my engine X configuration so that when you hit uh the slping route it will just return pong you then tell it how frequently you want to issue this health check how long you want to wait before you consider it to be timed out out and how many tries before you consider it to be a failure finally the start period is how long you expect it to take before the container is ready from initial boot up uh before it needs to do this test in this case our container image already had curl built in so there was no additional uh changes to the application that we needed to make for the API node service we'll have a very similar health check I'll add it right here um in this case though we're not going to be we don't have curl installed into our container image we wanted to keep our container image as minimal as possible we didn't want to have extra utilities floating around and so in this case I actually defined a health check in JavaScript that's going to be run from within the container and tell us if the container is healthy or not I'll show you what that looks like so under API node under health check uh uh we're using the HTTP package uh we say take it up to 2 seconds to Ping the slping route on Port 300 if we get back a 200 code great if we don't then fail and so this is what is getting run by that health check inside the container every 30 seconds for goang it will be almost identical uh in this case I have a separate binary that gets compiled in my container image and so if I look at the application I have this health check package within here again I'm just pinging Local Host on the route ping I have that defined in my application to return a 200 and then in my Docker file when I'm building my application I build both my health check as well as my main.go and so these are two separate binaries that live in my container image one of which is the application the other one gets run by this health check okay so with those health checks defined uh I can now redeploy my swarm stack and now I can list out those Services we'll see something interesting though if I do a Docker PS we now see under the status field see that these have status of starting meaning that it's using that health check to determine whether or not the container's healthy or not and until it is it won't Route traffic to those containers and it keeps the previous containers alive such that they can continue to receive traffic now that those health checks have started passing we can see that they've been up and they are now healthy and so we should be able to continue to access our application with no downtime and so now whenever we make a change we would build and push that container image update our Docker swarm configuration if needed redeploy and drer swarm would be intelligent enough to use those health checks to determine when we can Route traffic to the new containers or not you'll also see that because I have two replicas specified in my config we get two containers both running that goang image a couple of other additions that we could do uh we could configure our engine X front end to retrieve a certificate from let encrypt for example and store that in a volume such that it can persist across container restarts because right now we're serving traffic on Port 80 over HTTP unencrypted uh if we go to https we're not serving on 443 so that doesn't exist uh we also can update our DNS settings for a domain of our choosing and so for example we can go to cloudflare uh under devops directive I already have the setup but I'll update it to be the new IP address and so we can update this and that will take a few minutes to propagate but because I already had the old one set up we can just navigate to it directly and as you can see we're now routing traffic from that domain to our virtual machine hitting our application running within darker form and everything's behaving as expected you'll also notice that here I do have https and now this is a little this is a bit of a hack because it's using Cloud flares flexible TLS and so I have this set up such that encrypted traffic goes to cloudflare that's where it terminates TLS and then it sends that request unencrypted to the origin server and so if you're not dealing with particularly sensitive information depending on your threat model this may be sufficient however if you are dealing with sensitive information or there's requirements about the data that you're handling you would want to terminate TLS on your system uh this can be done like I was saying you could have engine X go off and get a let's encrypt certificate uh at its first startup uh you also can you can also handle that kind of at the Ingress layer for your doer swarmm cluster uh using something like traffic or another Ingress controller that can handle that and so with that we have our application deployed to a virtual machine running in the cloud being managed by Docker swarm we've set it up to use secrets to handle the credentials properly use health checks to allow us to have zero uh downtime deployments uh and so that's great for a single node if you want to add High availability to your system or or deploy to multiple nodes it starts to get a bit more complex especially around how it manages volume so in this case we have a single volume for our postgres data on that virtual machine if we wanted to have a multi-node cluster we would need to look at volume plugins and how to manage those how to allow Docker swarm to manage those across multiple hosts and so I didn't want to go into that level of depth in this course if you do want to dive a bit deeper on Docker swarm there's a great course from Brett fiser called Docker swarm Mastery that I would suggest you take a look at that'll be a full Deep dive on how to use swarm there's a recent reinvigoration of excitement around swarm They just added support for uh what's known as the CSI container storage interface which is a standard interface for managing persistent storage that is used in kubernetes has a lot of usage and maturity from that domain that's now going to get pulled hopefully back into swarm and make it a viable option for production deployments moving forward as I mentioned this is just one of many many different options for deploying containers um hopefully this gives you some ideas but do be on the lookout for the bonus content around uh how to use something like Railway or kubernetes to deploy the same sample application and with that we've reached the end of the course congratulations hopefully you feel ready to go off and incorporate containers into your software development and deployment approaches and use the lessons you've learned to improve the software systems within your organization to recap everything that we've covered we started with some fundamentals exploring why we should even care about containers in the first place how the evolution of virtualization brought us the current state of containers the underlying Technologies within Linux that enable containers to work and how Docker fits into the picture with that as our base we were then able to move on to learn how to use containers we used public third party containers for things like databases and interactive test environments we built out a multi service web application containerized it established a nice development workflow and then finally deployed our application to the cloud I hope that you enjoyed taking this course as much as I enjoyed making it also if you'd like to join a community of other devops practitioners I run a Discord server with lots of amazing folks where we talk about this type of thing all the time and answer each other's questions there's a link in the description if you'd like to join if you want more devops and Docker content YouTube should be recommending another video over there that's it for this course and remember just keep building",
        "summary": "As a technical educator, I've summarized the provided transcript into a comprehensive, structured guide on Docker and containers for learners. This summary breaks down complex concepts, explains their \"why,\" and provides practical insights derived from the course content.\n\n---\n\n## Docker & Containers: A Detailed Course Summary\n\nThis course aims to be the most efficient way to ramp up your Docker and container skills, from basics to advanced concepts. By the end, you'll be productive using containers for development and deployment, understanding their benefits and seamless integration into your workflow.\n\n### I. Course Introduction & Foundational Concepts\n\n#### A. Course Goals\nThe primary goal is to empower learners with the knowledge and skills to effectively use Docker and containers in a team setting for application development and deployment. This includes understanding the \"why\" behind containers and how to integrate them into your workflow.\n\n#### B. Prerequisites\nWhile designed for beginners, some familiarity with:\n*   **Web application development:** Understanding how different components (frontend, API, database) fit together helps differentiate container-specific vs. general architecture concerns.\n*   **Basic Linux shell commands:** Essential as they're frequently used throughout the course.\nIf these are lacking, the course can still provide value, but a quick brush-up on these topics is recommended.\n\n#### C. Course Structure\nThe course balances theory and practice. Each section covers concepts with visuals before diving into code examples. A companion GitHub repository (linked in the description) provides all code and configurations. The course is broken into progressive learning modules with timestamps for easy navigation.\n\n#### D. Why Containers? (Motivation & Evolution of Virtualization)\nContainers solve the pervasive \"it works on my machine\" problem by ensuring consistent environments. The evolution to containers involved:\n\n1.  **Bare Metal Computing:**\n    *   **Concept:** Applications run directly on physical hardware.\n    *   **Challenges:** \"Dependency hell\" (conflicting libraries/versions), low resource utilization (under-using powerful servers), large \"blast radius\" (issues with one app impacting others), slow startup/shutdown (minutes), very slow provisioning (days/weeks).\n\n2.  **Virtual Machines (VMs):**\n    *   **Concept:** A **hypervisor** (software/hardware layer) carves physical resources into isolated pools, each running a full **Guest Operating System** (OS) and applications.\n    *   **Types of Hypervisors:**\n        *   **Type 1 (Bare-metal):** Runs directly on hardware (e.g., AWS Nitro, VMware vSphere). Offers better performance.\n        *   **Type 2 (Hosted):** Runs on top of an existing OS (e.g., VirtualBox).\n    *   **Benefits over Bare Metal:** Eliminates dependency hell (isolated VMs), better utilization (many VMs on one server), reduced blast radius, faster startup/shutdown (minutes), much faster provisioning (minutes).\n\n3.  **Containers:**\n    *   **Concept:** A **container runtime** (e.g., Docker) runs applications on a host OS, *sharing the host's Linux kernel* but providing isolation through kernel features.\n    *   **Benefits over VMs:** Even higher utilization (no redundant Guest OS copies, images are MBs vs. GBs), faster startup/stop (seconds), strong isolation (though less absolute than VMs), portable to development environments, very fast provisioning (seconds).\n    *   **Common Configuration:** Running containers *within* virtual machines is common in cloud environments for combined benefits.\n\n| Feature               | Bare Metal         | Virtual Machine     | Container         |\n| :-------------------- | :----------------- | :------------------ | :---------------- |\n| **Dependency Mgmt.**  | Difficult          | Easy (Isolated)     | Easy (Isolated)   |\n| **Utilization**       | Low                | Good                | Excellent         |\n| **Isolation**         | Excellent (Phys.)  | Very Strong         | Good (Config. dep.)|\n| **Startup Speed**     | Minutes            | Minutes             | Seconds           |\n| **Dev Env. Parity**   | Low                | Possible (clunky)   | High (Lightweight)|\n| **Control**           | Full               | Hypervisor-limited  | Runtime-limited   |\n| **Performance**       | Tunable (best)     | Near-native         | Near-native       |\n| **Ease of Use**       | Difficult          | Moderate            | Easy              |\n\n#### E. What is a Container?\n*   A **Docker container image** is a lightweight, standalone, executable package of software that includes *everything* needed to run an application (OS dependencies, runtime, libraries, application code).\n*   An **Image** is like a blueprint or class (e.g., `Ubuntu image`).\n*   A **Container** is a running instance of an image (e.g., `Ubuntu container`). You can run multiple containers from a single image.\n\n**Open Container Initiative (OCI):**\n*   An industry standard created by many major tech companies (Docker, Google, Microsoft, etc.) to standardize container formats and runtimes.\n*   **Specifications:**\n    *   **Image Specification:** Defines what an image contains (metadata, serializable file system format).\n    *   **Runtime Specification:** Defines how to run an OCI-compliant image in a container.\n    *   **Distribution Specification:** Defines how images should be distributed (e.g., registries).\n*   Docker provides a specific implementation of these OCI standards.\n\n#### F. Underlying Linux Technologies\nContainers are built upon three core Linux kernel features:\n\n1.  **Namespaces:**\n    *   **Purpose:** Isolate global system resources for processes within a namespace, making them appear to have their own dedicated instance.\n    *   **Examples:**\n        *   **PID Namespace:** A process inside a container sees itself as PID 1, unaware of other host processes.\n        *   **User Namespace:** Maps a user inside the container (e.g., `root`) to a different, less privileged user on the host, crucial for security.\n        *   Other namespaces exist for network, mount points, IPC, UTS (hostname), etc.\n\n2.  **Control Groups (cgroups):**\n    *   **Purpose:** Organize processes into groups to limit and monitor their access to system resources. Prevents \"noisy neighbor\" problems.\n    *   **Examples:** Limiting CPU percentage (`cpu.shares`), memory usage (`memory.limit_in_bytes`), or disk I/O.\n\n3.  **Union File System (UnionFS) / OverlayFS (Docker's implementation):**\n    *   **Purpose:** Combines multiple separate file systems (layers) into a single, unified view.\n    *   **Mechanism:** Docker images are composed of read-only layers. When a container runs, a thin, writable layer is added on top. Changes are written to this top layer.\n    *   **Benefits:** Makes containers practical by allowing layers to be shared and cached, significantly reducing storage and transfer size for images (e.g., many images can share a common Ubuntu base layer).\n\n#### G. Docker Desktop Architecture\nDocker Desktop simplifies container management by packaging these underlying technologies into a user-friendly application.\n\n*   **Client Application (Local System):**\n    *   **CLI (Command Line Interface):** `docker run`, `docker pull`, etc.\n    *   **GUI (Graphical User Interface):** For browsing images, managing resources (CPU, memory, disk), extensions.\n    *   **Credential Helper:** Securely stores login credentials for container registries (e.g., in system keychain).\n    *   **Extensions:** Third-party plugins for additional functionality.\n\n*   **Linux Virtual Machine (VM):**\n    *   Docker Desktop creates a lightweight Linux VM on macOS (using a hypervisor) or Windows (using WSL or Hyper-V).\n    *   **Docker Daemon (`dockerd`):** The server-host application running inside the VM that exposes the **Docker API**. It manages container objects, networking, and volumes.\n    *   **Kubernetes Cluster (Optional):** Can be configured within the VM for local Kubernetes development.\n\n*   **Registry (External):**\n    *   A remote storage location for container images (e.g., Docker Hub). The Docker daemon pulls/pushes images to/from registries.\n\n**Docker Engine vs. Docker Desktop:**\n*   **Docker Engine:** The open-source core, consisting of the Docker CLI, Docker Daemon, and Docker API. Runs natively on Linux.\n*   **Docker Desktop:** A complete package for macOS/Windows, including Docker Engine, a Linux VM, GUI, and other developer tools.\n\n### II. Getting Started with Docker\n\n#### A. Installation & First Run\n*   Docker Desktop is free for personal use; commercial use may require a paid subscription for teams over 250 employees or $10M annual revenue.\n*   Installation is straightforward from `docs.docker.com`. Configure resource limits (CPU, memory) in settings.\n*   **First Container:** `docker run docker/whalesay cowsay \"Hey Team\"` (pulls image, runs command).\n*   **Useful Container:** `docker run -d -p 5432:5432 --name postgres-db -e POSTGRES_PASSWORD=mysecretpassword postgres:15.1-alpine` (runs a PostgreSQL database).\n\n#### B. Leveraging Third-Party Images & Data Persistence\n\n*   **Ephemeral Nature of Container Data:** By default, any data created or modified *within* a container at runtime (in its thin read-write layer) is *ephemeral*. If the container is stopped and removed (`--rm`), that data is lost.\n    *   **Demonstration:** Installing `ping` inside a temporary Ubuntu container; `ping` is gone when a new container from the same image starts.\n    *   **Solution for Persistent Dependencies:** Build them into the *image* itself via the Dockerfile, so they're always present when a container starts.\n\n*   **Data Persistence for Runtime-Generated Data:**\n    *   **Volumes:**\n        *   **Mechanism:** Docker manages a designated location on the Docker host's file system (within the VM on macOS/Windows) where data is stored.\n        *   **Usage:** Create a named volume (`docker volume create my-volume`), then mount it into the container at a specific path (`-v my-volume:/path/in/container`).\n        *   **Benefit:** Data persists even if the container is removed; managed separately from container lifecycle. Recommended for databases.\n    *   **Bind Mounts:**\n        *   **Mechanism:** Directly connects a path on the *host's file system* into the container at runtime.\n        *   **Usage:** `-v /path/on/host:/path/in/container`.\n        *   **Benefit:** Easy observability of files directly on your host.\n        *   **Caveats:** Can suffer performance impacts due to crossing the VM boundary (on macOS/Windows) with heavy I/O operations. Often used for mounting source code during development.\n\n*   **Practical Use Cases for Third-Party Containers:**\n    1.  **Databases:** Easily spin up different database versions (PostgreSQL, MongoDB, Redis, MySQL, Elasticsearch, Neo44j).\n        *   **Key Considerations:** Always use a **volume** for data persistence. Pass configuration via **environment variables** or **bind-mount** a custom config file.\n    2.  **Interactive Test Environments:** Quick shells for different Linux distributions (Ubuntu, Alpine, Debian) or specific programming runtimes (Python, Node, Go) for testing commands or minor version tweaks.\n    3.  **Command Line Utilities:** Run tools (JQ for JSON, YQ for YAML, AWS CLI, GCP CLI) without installing them locally.\n        *   **Mechanism:** Pipe data into the container, get output as a stream. Requires bind-mounting credentials for cloud CLIs.\n    4.  **Improving Ergonomics:** Use shell functions or aliases to simplify long `docker run` commands.\n\n### III. Containerizing Your Own Applications\n\n#### A. Demo Application Overview\nThe course uses a minimal **three-tier web application** as a practical example:\n*   **Frontend:** React client.\n*   **API Layer:** Two implementations: Node.js (interpreted) and GoLang (compiled) \u2013 to showcase different language toolchains.\n*   **Database:** PostgreSQL.\nEach service is simplified but demonstrates realistic networking and credential passing.\n\n#### B. Dockerfiles: The Container Recipe\nA **Dockerfile** is a text document containing instructions to build a container image. It's like a recipe:\n*   Start with a base image (OS).\n*   Install language runtime, dependencies, tooling.\n*   Install application dependencies.\n*   Set up the execution environment (ENV vars).\n*   Define the command to start the application.\n\n*   **Build Context:** The local directory (or URL) containing source code and `Dockerfile`.\n*   **`.dockerignore`:** Specifies files/directories to exclude from the build context (e.g., `node_modules`), preventing conflicts and reducing context size.\n*   **`docker build -t <tag> .`:** Command to build an image from a Dockerfile in the current directory (`.`).\n\n**Key Principles for Optimizing Dockerfiles (Security, Speed, Clarity):**\n\n1.  **Pin Specific Versions (Security, Speed, Clarity):**\n    *   **Base Images:** Always specify exact tags (e.g., `node:19.6-alpine`) instead of `latest` for repeatability. Using image hashes (`<tag>@sha256:...`) offers immutability.\n    *   **System Dependencies:** Pin versions when installing packages (`apt install curl=7.x`).\n    *   **Application Dependencies:** Use lock files (e.g., `package-lock.json`, `go.sum`) for repeatable builds (`npm ci`).\n\n2.  **Use Smallest & Most Secure Base Images (Security, Speed):**\n    *   Smaller images have fewer utilities/dependencies, reducing attack surface (e.g., Alpine Linux).\n    *   Consider trade-offs (e.g., Node.js Alpine images can be experimental due to C library differences; Debian \"slim\" variants offer a good balance; `distroless` for minimal runtimes; `chainguard` for hardened images).\n\n3.  **Leverage Layer Caching (Speed):**\n    *   Each Dockerfile instruction creates a new layer. Docker caches layers. Changing an instruction or a file *above* it in the Dockerfile invalidates the cache for all subsequent layers.\n    *   **Principle:** Order commands from least to most frequently changing.\n        *   `FROM` (base image) changes least often.\n        *   Copy `package.json`/`go.mod` *before* `npm install`/`go mod download` to cache dependency installation.\n    *   **Cache Mounts:** Use `--mount=type=cache` with `RUN` instructions (e.g., for `npm install` cache) to store build artifacts locally, speeding up future builds.\n    *   **`COPY --link` (New Feature):** Creates a separate layer not tied to previous layers, useful in multi-stage builds to avoid cache invalidation when changing intermediate stages.\n    *   **Combine Steps:** Use `&&` to chain commands (e.g., `apt update && apt install`) to reduce the number of layers.\n\n4.  **Be Explicit (Clarity):**\n    *   **`WORKDIR`:** Set the default directory for subsequent instructions and container runtime.\n    *   **`EXPOSE`:** Document which ports the application listens on (doesn't actually publish ports).\n    *   **`ENV`:** Set environment variables for the container's runtime environment (e.g., `NODE_ENV=production`).\n\n5.  **Avoid Unnecessary Files (Security, Speed):**\n    *   Use `.dockerignore` to prevent unwanted files (e.g., `node_modules`, build artifacts, `.git`) from being copied into the build context.\n    *   **Selective `COPY`:** Copy only necessary files/directories (e.g., `COPY src/ .`) instead of the entire context (`COPY . .`).\n\n6.  **Run as Non-Root User (Security):**\n    *   Use the `USER` instruction to run container processes as a non-root user (e.g., `USER node`). This significantly limits damage if the container is compromised.\n    *   Ensure copied files are owned by the non-root user (`chown`).\n\n7.  **Install Only Production Dependencies (Security, Speed):**\n    *   Use `npm ci --only=production` or similar flags to exclude development-only dependencies from the final image, reducing size and attack surface.\n\n8.  **Leverage Multi-Stage Builds (Speed, Security, Clarity):**\n    *   Define multiple `FROM` instructions in a single Dockerfile, creating distinct build stages.\n    *   **Benefit:** Copy only the necessary artifacts (e.g., compiled binary) from a larger \"build\" stage to a minimal \"runtime\" stage, resulting in a tiny, secure final image. (e.g., Go app: full Go SDK in build stage, only the binary in `scratch` image for deployment).\n\n**Advanced Dockerfile Features:**\n\n*   **Parser Directives:** (`# syntax=docker/dockerfile:1.x`, `# escape=\\`) at the top to specify Dockerfile syntax version or escape character.\n*   **`LABEL`:** Add arbitrary metadata to the image manifest (e.g., author, version, `org.opencontainers.image` standards).\n*   **Here-Doc Syntax (`RUN <<EOF ... EOF`):** Write multi-line commands that execute as a single layer.\n*   **Secret Mounts (`--mount=type=secret`):** Securely pass sensitive data (e.g., API keys, private repo credentials) during the *build process* without baking them into image layers. These secrets are only available during the build and are not part of the final image.\n*   **`ENTRYPOINT` vs. `CMD`:**\n    *   **`CMD`:** Defines the default command that gets executed when a container starts. Can be overridden by arguments passed to `docker run`.\n    *   **`ENTRYPOINT`:** Defines the executable that will always be run when the container starts. `CMD` arguments are appended to the `ENTRYPOINT`.\n    *   **Rule of Thumb:** For applications that run consistently, put everything in `CMD`. For utility containers where users might append arguments, use `ENTRYPOINT` for the executable and `CMD` for default arguments.\n*   **`ADD` vs. `COPY`:** `COPY` is generally preferred for clarity and predictability. `ADD` has additional features (e.g., auto-extracting tarballs, fetching URLs) but is often more ambiguous.\n*   **BuildX:** Built into Docker CLI, enables building multi-architecture images (e.g., `amd64`, `arm64`) from a single machine using QEMU emulation. Increasingly important for Apple Silicon users and cost-effective ARM cloud servers.\n\n### IV. Managing Container Images\n\n#### A. Container Registries\n*   **Purpose:** Centralized repositories for storing and accessing container images. Essential for teams and cloud deployments.\n*   **Providers:** Docker Hub, GitHub Container Registry, Google Container Registry (GCR), Amazon Elastic Container Registry (ECR), Azure Container Registry (ACR), etc.\n*   **Public vs. Private:** Public registries allow anyone to pull; private require authentication for both push and pull.\n*   **Authentication:** `docker login` (uses credential helper) or specific cloud provider CLIs (e.g., `gcloud` for GCR).\n\n#### B. Tagging Best Practices\n*   **Immutable Tags:** Treat tags as immutable. Do not re-tag and overwrite an existing tag with a new image version if others are using it.\n*   **Descriptive Tags:** Use tags that convey information about the image.\n*   **Multiple Tags:** A single image can have multiple tags (e.g., `ubuntu:latest`, `ubuntu:22.04`, `ubuntu:jammy` might all point to the same image).\n*   **Automated Tagging (CI/CD):** Combine information like:\n    *   Timestamp (`YYYYMMDD`)\n    *   Build ID (from CI system)\n    *   Commit hash (of source code)\n    *   Semantic Version (`vX.Y.Z`)\n*   **Registry Prefix:** When pushing to a registry other than Docker Hub, the image tag must be prefixed with the registry hostname (e.g., `ghcr.io/user/repo:tag`).\n\n### V. Running and Orchestrating Containers\n\n#### A. Docker `run` Command Options\nFor single, ad-hoc containers. Many options exist; key ones include:\n\n*   `-d`, `--detach`: Run container in the background (detached mode).\n*   `--entrypoint`: Override the `ENTRYPOINT` defined in the Dockerfile.\n*   `-e`, `--env`: Set environment variables inside the container.\n*   `--env-file`: Load environment variables from a file.\n*   `--init`: Run an `init` process (PID 1) inside the container to handle signal forwarding and zombie processes.\n*   `-i`, `--interactive`: Keep STDIN open, even if not attached.\n*   `-t`, `--tty`: Allocate a pseudo-TTY (for interactive shell sessions).\n*   `--mount`, `-v`: Mount volumes or bind mounts for data persistence.\n*   `--name`: Assign a specific name to the container.\n*   `--network`, `--net`: Connect the container to a specific Docker network for isolation and service discovery.\n*   `--platform`: Specify the target architecture (e.g., `linux/arm64`).\n*   `-p`, `--publish`: Publish container ports to the host.\n*   `--restart`: Define container restart policy (e.g., `always`, `unless-stopped`).\n*   `--rm`: Automatically remove the container when it exits.\n\n**Advanced/Security-Related Options:**\n*   `--cap-add`, `--cap-drop`: Add or drop Linux capabilities for fine-grained control over container privileges.\n*   `--cpu-shares`, `--cpuset`, `--memory`: Limit CPU/memory resources for containers (cgroups).\n*   `--device`, `--device-read-bps`, `--device-write-bps`: Grant access to specific host devices and limit I/O.\n*   `--gpus`: Grant access to host GPUs.\n*   `--health-cmd`, `--health-interval`, etc.: Define health checks for container status monitoring.\n*   `--privileged`: Grants all capabilities to the container (highly insecure, avoid if possible).\n*   `--read-only`: Mount the container's file system as read-only for enhanced security.\n*   `--security-opt`: Apply AppArmor or Seccomp profiles for security hardening.\n*   `--userns-remap`: Remap user namespaces (at Docker daemon level) to map container root to an unprivileged host user.\n\n#### B. Docker Compose for Multi-Service Applications\nDocker Compose defines and runs multi-container Docker applications using a YAML file (`docker-compose.yaml`).\n\n*   **Benefits over `docker run`:**\n    *   **Configuration as Code:** All service configurations (images, ports, volumes, networks, environment variables) are defined in one readable YAML file.\n    *   **Service Orchestration:** Easily manage the lifecycle of multiple interdependent services (`docker compose up`, `down`, `build`, `stop`).\n    *   **Service Discovery:** Services can refer to each other by name (e.g., `database` for the DB service).\n    *   **Dependency Management:** `depends_on` ensures services start in the correct order.\n*   **Usage:** `docker compose up` (builds and starts all services), `docker compose down` (stops and removes services/networks/volumes).\n*   **Composerize:** A tool to convert `docker run` commands into Docker Compose YAML.\n\n#### C. Docker CLI Deep Dive (Main Object Types)\nThe Docker CLI offers extensive commands for managing different Docker objects. Use `docker --help` for general commands and `docker <command> --help` for sub-commands.\n\n1.  **`docker image`:**\n    *   `build`: Build an image from a Dockerfile.\n    *   `history`: Show image layer history.\n    *   `inspect`: Display detailed JSON metadata about an image.\n    *   `ls`: List images.\n    *   `prune`: Remove unused (dangling) images.\n    *   `pull`: Download an image from a registry.\n    *   `push`: Upload an image to a registry.\n    *   `rm`: Remove one or more images.\n    *   `scan`: Scan an image for vulnerabilities (uses Snyk or Trivy).\n    *   `tag`: Add a new tag to an existing image.\n\n2.  **`docker container`:**\n    *   `attach`: Attach local standard I/O to a running container.\n    *   `exec`: Run a new command inside a running container.\n    *   `inspect`: Display detailed JSON metadata about a container.\n    *   `kill`: Forcefully stop a container.\n    *   `logs`: Fetch the logs of a container.\n    *   `ls` (or `ps`): List running containers (`-a` to show all, including stopped).\n    *   `prune`: Remove all stopped containers.\n    *   `rm`: Remove one or more containers.\n    *   `run`: Create and start a new container from an image.\n    *   `stop`: Gracefully stop a container.\n    *   `top`: Display running processes inside a container.\n    *   `wait`: Block until a container exits, then print its exit code.\n\n3.  **`docker volume`:**\n    *   `create`: Create a named volume.\n    *   `inspect`: Display detailed JSON metadata about a volume.\n    *   `ls`: List volumes.\n    *   `prune`: Remove all unused volumes.\n    *   `rm`: Remove one or more volumes.\n\n4.  **`docker network`:**\n    *   `connect`: Connect a container to a network.\n    *   `create`: Create a network.\n    *   `disconnect`: Disconnect a container from a network.\n    *   `inspect`: Display detailed JSON metadata about a network.\n    *   `ls`: List networks.\n    *   `prune`: Remove all unused networks.\n    *   `rm`: Remove one or more networks.\n\n### VI. Container Security Best Practices\n\nContainer security involves securing both the image and the runtime environment. Just using a container doesn't automatically make it secure; configuration is key.\n\n#### A. Image Security (Build Time)\n*   **Minimize Attack Surface:**\n    *   Use **minimal base images** (e.g., Chainguard images, `distroless`).\n    *   **Don't include unnecessary tools/dependencies** in the final image (leverage multi-stage builds).\n    *   **Scan images for vulnerabilities** (Snyk, Trivy) and address critical findings.\n*   **Least Privilege:**\n    *   Run as a **non-root user** inside the container.\n    *   **Avoid baking secrets into images**; inject them at runtime.\n*   **Trust & Integrity:**\n    *   **Cryptographically sign images** to verify their origin and integrity.\n    *   **Pin exact versions** of base images and dependencies to prevent unexpected changes.\n\n#### B. Runtime Security (Deployment Time)\n*   **Host Security:**\n    *   **User Namespace Remapping (`--userns-remap`):** At the Docker daemon level, map root user inside containers to an unprivileged user on the host for an extra layer of defense.\n*   **Container Configuration:**\n    *   **Read-Only Filesystem:** If the application doesn't need to write to disk, make the container's file system read-only.\n    *   **Drop Capabilities (`--cap-drop ALL`):** Remove all Linux capabilities and only add back those explicitly needed.\n    *   **Resource Limits:** Limit CPU and memory (`--cpu-shares`, `--memory`) to prevent \"Denial of Service\" attacks or resource starvation.\n    *   **Security Profiles (`--security-opt`):** Use AppArmor or Seccomp profiles to enforce granular security policies (e.g., restrict system calls).\n\n### VII. Enhancing Developer Experience\n\nA good developer experience means easy setup, fast iteration, and robust debugging/testing within containers.\n\n#### A. Key Considerations\n*   **Easy Setup:** Get up and running quickly with minimal commands.\n*   **Fast Iteration:** Make code changes and see results without slow rebuilds.\n*   **Debugging:** Debug code as if running locally.\n*   **Testing:** Execute tests inside the containerized environment.\n*   **CI/CD:** Automate builds, tests, and pushes.\n*   **Ephemeral Environments:** Short-lived, isolated environments for PRs, QA, etc.\n\n#### B. Development Workflow Improvements\n1.  **Hot Reloading with Bind Mounts:**\n    *   **Mechanism:** Bind-mount your local source code directory into the container at runtime.\n    *   **Tools:** Use hot-reloading utilities inside the container:\n        *   **Frontend (React/Vite):** Vite's built-in hot reloading.\n        *   **Node.js:** `nodemon`.\n        *   **GoLang:** `air`.\n    *   **Benefit:** Changes saved on the host are immediately reflected in the running container, triggering automatic rebuilds/restarts.\n    *   **Caveat:** Use empty volume mounts (`-v /path/to/container/node_modules`) to prevent `node_modules` from your host (which might have different architecture/OS binaries) from shadowing the `node_modules` installed inside the container.\n\n2.  **Remote Debugging Setup:**\n    *   **Mechanism:** Configure the application/runtime inside the container to listen for debugger connections on a specific port, then publish that port to the host.\n    *   **Tools:**\n        *   **Node.js:** `--inspect` flag (e.g., `node --inspect=0.0.0.0:9229 index.js`).\n        *   **GoLang:** `delve` (debugger tool).\n    *   **IDE Integration:** Configure your IDE (e.g., VS Code's `launch.json`) to attach to the remote debugger port, mapping local source paths to container paths.\n\n3.  **In-Container Testing:**\n    *   **Mechanism:** Define specific `command` overrides in Docker Compose files (e.g., `npm run test`, `go test`) to execute tests within the containerized environment.\n    *   **Benefit:** Ensures tests pass in an environment identical to production.\n\n#### C. Continuous Integration (CI) with GitHub Actions\nAutomate building, scanning, tagging, and pushing container images.\n*   **Workflow:** Defined in `.github/workflows/*.yaml` files.\n*   **Triggers:** `on: push` (to specific branches/tags), `pull_request`, `cron` schedules.\n*   **Jobs & Steps:** Define build steps, often leveraging public GitHub Actions from the Marketplace (e.g., `checkout`, `docker/metadata-action` for dynamic tagging, `docker/login-action`, `docker/build-push-action`, `aquasecurity/trivy-action` for vulnerability scanning).\n*   **Secrets:** Store sensitive credentials (Docker Hub username/token) as GitHub Repository Secrets, injected securely into the workflow.\n\n#### D. Ephemeral Environments (Shipyard Demo)\n*   **Concept:** Short-lived, isolated, on-demand environments for testing, validation, and QA for every pull request.\n*   **Benefits:** Team collaboration, faster feedback loops, confident validation of changes in a realistic environment.\n*   **Shipyard Platform:**\n    *   Integrates with GitHub to detect PRs.\n    *   Scans `docker-compose.yaml` (with specific Shipyard labels for routing).\n    *   Automatically builds (from Dockerfiles) and deploys environments in isolated Kubernetes clusters.\n    *   **Features:** Automated environment creation per PR, access control (SSO via GitHub/Google), auto-sleep for unused environments, snapshotting named volumes from base environments, CLI for local interaction (logs, port forwarding), GitHub commit checks/PR comments.\n    *   **Shipyard Labels for Routing:** `shipyard.route.primary: \"true\"` (for default root path) and `shipyard.route.path: \"/api/node/\"` (for API routes). `shipyard.route.rewrite` for stripping path prefixes.\n\n### VIII. Deploying Containers to the Cloud\n\n#### A. Deployment Landscape\n*   **Portability:** Containers provide a standard interface, leading to many cloud provider options (e.g., 17 ways on AWS: Lambda, ECS, EKS).\n*   **Key Considerations for Choosing a Platform:**\n    *   **Security:** Provider handling or explicit configuration options.\n    *   **Ergonomics/DX:** Ease of deployment, new version rollout, logging, monitoring.\n    *   **Scalability:** Handling individual container scaling, multi-host deployments, and GPU access.\n    *   **Persistent Storage:** How volumes are managed across multiple hosts (requires volume plugins for distributed storage).\n    *   **Cost:** Balancing compute cost with managed service premiums.\n\n#### B. Docker Swarm for Orchestration\nWhile `docker compose up` works on a single host, it has limitations for production:\n\n*   **Docker Compose Limitations (for Production):**\n    *   No zero-downtime deployments (requires stopping/restarting).\n    *   No native rollback.\n    *   No encrypted secret management.\n    *   Limited to a single host.\n    *   (Historically, no health checks or separate binary, but these are now mitigated).\n\n*   **Docker Swarm Benefits (over Compose):**\n    *   **Zero-Downtime Deployments:** With `update_config` strategies (start new, then stop old).\n    *   **Rollbacks:** Easy rollback to previous versions.\n    *   **Secrets Management:** Securely handle credentials (`docker secret create`, consuming secrets as files in containers).\n    *   **Multi-Host Scaling:** Distribute containers across many hosts (though persistent storage is more complex for multi-node).\n    *   **Health Checks:** Configure tests to determine container readiness, enabling intelligent routing.\n\n*   **Single-Node Swarm Deployment Walkthrough:**\n    1.  **Provision VM:** Create a Linux VM (e.g., on Civo).\n    2.  **Firewall Configuration:** Allow inbound traffic on necessary ports (22 for SSH, 80/443 for web, application ports like 3000/8080).\n    3.  **Install Docker Engine:** Use `get.docker.com` script.\n    4.  **Connect Remotely:** Set `DOCKER_HOST` environment variable on local machine to connect to remote Docker daemon via SSH.\n    5.  **Initialize Swarm:** `docker swarm init` on the remote VM.\n    6.  **Modify `docker-compose.yaml` (for Swarm):**\n        *   Rename to `docker-swarm.yaml` (or similar).\n        *   Remove `build` sections (images pushed to registry beforehand).\n        *   Remove `restart` policies (Swarm handles this).\n        *   **Add `deploy` section:** Configure `mode: replicated`, `replicas`, and `update_config` (e.g., `order: start-first`).\n        *   **Add `healthcheck`:** Define `test` command (e.g., `curl localhost:8080/ping`), `interval`, `timeout`, `retries`, `start_period`.\n        *   **Integrate `secrets`:** Define `secrets` block (e.g., `database_url: external: true`), and link secrets to services. Applications must be modified to read secrets from files (`/run/secrets/`).\n    7.  **Build & Push Images:** Build final production images locally and push to Docker Hub (or other registry).\n    8.  **Deploy Stack:** `docker stack deploy -c docker-swarm.yaml <stack-name>` (Docker Compose V3.7+ is compatible with `docker stack deploy`).\n    9.  **DNS Configuration:** Update DNS records (e.g., Cloudflare) to point your domain to the VM's public IP.\n    10. **TLS/HTTPS:** Terminating TLS at the load balancer/CDN (e.g., Cloudflare Flexible TLS) or using an Ingress controller (e.g., Traefik) within Swarm to manage Let's Encrypt certificates.\n\n*   **Database Disclaimer:** While deploying databases within Swarm with volumes persists data, it doesn't include robust backup/restore. For critical production data, consider managed database services (RDS, Cloud SQL) or a more robust self-managed setup.\n\n### IX. Conclusion\n\n*   **Recap:** The course covered container fundamentals, usage of public containers, building custom container images, establishing an efficient development workflow, and deploying applications to the cloud using Docker Swarm.\n*   **Key Takeaway:** You're now equipped to incorporate containers into your software development and deployment processes, improving your organization's software systems.\n*   **Further Learning:** Join the Discord community for devops practitioners, explore bonus content on Railway and Kubernetes deployments, and \"just keep building.\"",
        "title": "Complete Docker Course - From BEGINNER to PRO! (Learn Containers)"
    },
    {
        "videoId": "y-4CG-ptHq4",
        "timestamp": 1753734764.820562,
        "transcript": [
            {
                "chunk_id": 1,
                "content": "[Music] Hi everyone and welcome to the 100 steps campus tour. [Music] [Music] We'll start off from the outside of the building. You probably are not going to spend too much of your time here, but you have a garden and a place to sit and code for if you want to. Um, we'll set up some chairs here eventually, but this is a place where you can spend time. Usually, it's fun when it's raining. Um, but other than that, it's a fairly boring place. Um, but if you want, you can spend your time here. This is all the outside of the building. So, you know, reel it in. Uh, this is all that you get if you come to the 100x campus. When it comes to the outside of the building, let's go inside. [Music] Welcome inside the building. Um on the right side you have a reception area/ admissions area slash this will get converted to a co-working eventually but I be clear if you enter this is where you'll find the counselor in case you have more questions. Um that's right behind me and right behind you is the very first classroom. So if you go back here, I'm assuming there's a class going on right now, but that's the first classroom for the school students. Uh this is what it looks like. So that's the very first classroom. It has a capacity of around 150. Um and this is where you'll be learning/ spending most of your time honestly during classes. That's around 11 to 7 is when you'll be here and you know learning with I don't know 1 2 3 I see four five teachers right now. Uh you'll meet the faculty soon enough. That's Rouser. He teaches DSA. That's Rahul who does dev. That's Sukcham behind you also does dev. We have too many dev people. Not enough DSA people. And we have some I don't know something over here. Probably some dev. Let's go outside. That's the way we entered. And we can exit from there from behind me. [Music] This is the central courtyard area. Uh we've covered it a little too much. So it gets a little uh dark over here right now. We'll add more lights. But that's all the folks that have been placed in the past. We just got these banners done. And later I realized people change companies. So Amrit's photo is there, but he leftc.com. He's working somewhere else now. So we'll have to keep changing these more often than not. Um but that is it. That's the central courtyard area that will eventually again convert into co-working. All the open spots that you see will be co-working where you can co-work from. Um that's classroom number one. This is the central courtyard area/ your co-working space. And if there's an event that will happen in here as well. With that, we move to the first floor. The super 30 floor where you'll find the super 30 class. You see upstairs. All right. First floor has the jazzier super 30 classroom. Um so if you come to the first floor and it's open for everyone. It's not like it's only open for super 30 people. You will find the smaller classroom. This can fit around 80 to 90 people. That will be the capacity for super 30 this year. Super 30 for those of you who don't know is our most actionable program. Um by actionable I mean um it's a place where people who haven't really gotten a job right now are most probably undergrads from 2024 to 2025 uh come to learn and you know get jobs in a very actionable way. We keep the bat size really small because we want the outcomes to be really I don't know like some 80% 90% in a span of 6 months which is really hard. That's also why it's really hard to get in. Um there are around 5% of the people who got selected this year. Um it'll probably go down further. Um but yeah that's this is where I'll be spending most of my time because I'll be leading over 30 classes. Um also spend time in the school but this is the biggest goal for me because 6 months is all we have to place these 100 people. Um so that's always a challenge compared to a 4-year program. That's classroom number two. Let's move on to the hostel rooms. Follow me. So we'll go back downstairs um to enter from the main hostel area. So the hostel for guys has a different entry. For girls, it's up here, but for guys, there's a separate entry. I'll take you there next um to the boys hostel. [Music] All right, welcome to the entry for the boys hostel. Follow me. So, we have a few types of hostel rooms. Let's start with the cheapest one. This one's like 10,000 rupees a month with food. It's like pretty cheap. We've tried our best to optimize prices very less. So that you know I mean you'll spend 10k anywhere I would assume in Noa if you want food plus accommodation. So that's what you'll get in these rooms but that also you know means these rooms aren't the best. So check them out. Sorry. All right we'll start off with the I would say the mid-tier rooms. They're four sharing. Um and some of them have windows some of them don't. So the price is fairly optimized. So the hostel starts at 1.2 lakhs a year. So 10,000 rupees a month with food. Um, and I'm assuming any place you stay in Nida, probably anywhere in the country, um, you have to spend that much on food and accommodation. Um, and this is what the first set of hostel rooms looks like. I think there's someone staying here already, if I'm not wrong. Let's do a hostel raid. Yeah, there you go. Let's do a hostel raid. One student is staying right now. He came a little early. Taking long. I don't know what he was doing there. Look, this is pretty effective. All right, this is what the first hostel room looks like. Uh, as I said, someone's already here. It's a four sharing hospital room. Uh, and the AC is damn good. Like, I did not expect to be this good. Uh, this is what the room looks like. Um, it has it'll accommodate four people. We'll put up a table or chair or something else over there, but this is pretty much it. We have four beds, a cupboard, probably another cupboard, and a bunch of tables. If you want to see an empty one, um it's you just follow me. All of these are four sharing rooms. The ones on the left are good ones because they have windows. Um the ones on the right do not have windows. Like some of them do, some of them don't. This is another decent one to look at. Let me turn on the lights, which I have no idea where it happens from. Oh, there you go. So, that's the that's another idea of what the hospital room would look like. Two beds over here, two beds over here, one cupboard over here. Probably put a table over there, and that's it. You have the AC that makes it liveable honestly. Otherwise, it's like not as really hot. You see the mid-tier rooms. Let's take you to a slightly better room, slightly bigger. In case you feel like this isn't comfortable for you, we have something better right behind you. Follow me. These are the expensive rooms. Um, this one's also four sharing. We're still debating if this should be six sharing and seat or four sharing or expensive. Uh, but yeah, this will I think there's more than enough space here for four people. That's the other one you can get. Uh that's you've seen the mid-tier rooms, the slightly better rooms. There's a best tier room as well. Uh it's mostly available like I think we have four in boys and two in girls. Let's go to the girls host next and see how they're separated from the boys host. They have a separate entry and where the girls would be staying completely isolated from the rest of the campus. Follow me. We have to go back. [Music] This is a separate entry for girls. Now this staircase is only accessible to the girls. And if you go up from here, you reach the girls hostel. So all of this area that's five rooms is the complete girls hostel. U it'll be completely isolated from the rest of the building. Separate entry over here. And if you come behind me, I can show you one of the better rooms. One washroom. I mean, I haven't shown the washrooms, but they're they're like what you would expect. And this is one of the better rooms, I would say, because it has two windows facing in the front. Um, so that's what a room would look like for the girls. Um, need to clean this up a bit. Um, but yeah, that's what a girls host room looks like. We have five of these, four each. So, we have a capacity of 20 girls this year. I think we have 12. So, we're closer to capacity on that. uh for this year. Slightly complicated building, but this is a third stairwell which is a little bigger and you know takes you to the roof. Um this one if you come with me opens up directly to another sort of a classroom. I don't know if it is a classroom or what you would call it. Um we like to call it the auditorium. It has a capacity of around 150. Seats are not yet installed so do not judge me but this is what the auditorium looks like. It's in the same building on the second and the third floor. Um this is like two floors that's combined into one single auditorium. Um and that's where I don't know what will happen here. Probably if you guys like having classes here, we can have classes here for the school or for super 30. But mostly it'll be used for events. Um or if you guys want to watch movies um I think Big Shot and the social network are great movies to induct someone into computer science. So that's probably what the orientation is going to be like. Also, school isn't built alone. I won't be the only one here. You'll have a bunch of teachers, batch assess managers, TAs who will make sure you have a wholesome experience. We'll have Duv who will be leading data structures and algorithms. Rahul, Deepain and Yash will be helping me with dev as batches managers and teachers. The goal is you have a wholesome experience when you join Danex school admissions open for the 25 cohort. You also have a mess at the ground floor. So your breakfast, lunch and dinner will all be served over here. You just have to come down from your hostel room downstairs for breakfast. Go to your classroom, come back for lunch, go to your classroom, come back for dinner and back to your hostel room on the first, second or the third floor. Classroom is right over here. You can come over here, eat your food and go back to the classroom. Everything is kept in a single place to keep things as convenient as possible. And there's a canteen right over here if you want to buy something else. There's also a gym on the second floor with your washrooms right behind you. So, it's as convenient as possible. There'll be girls and boys and teachers timing over here. And you'll be given some points if you're hitting the gym regular. >> Hi everyone"
            },
            {
                "chunk_id": 2,
                "content": "and welcome to the 100 steps campus tour. You know your heart to [Music] Oh sh [Music] [Applause] [Music] The heart. [Music] [Applause] [Music]"
            }
        ],
        "summary": "Welcome, learners! As your technical educator, I'm here to provide a detailed and structured overview of the 100x Campus, designed to help you understand its facilities, academic programs, and the supportive environment you'll experience. This summary is organized to give you a clear picture of what to expect.\n\n---\n\n## 100x Campus Tour: A Detailed Learner's Guide\n\n### I. Campus Overview & General Facilities\n\nThe 100x campus is designed to be a self-contained learning and living environment, prioritizing convenience and a conducive atmosphere for intensive study.\n\n*   **Exterior:**\n    *   **Garden/Outdoor Seating:** A dedicated outdoor space with a garden, offering a quiet area to sit, relax, or even \"code for if you want to.\" Chairs will be set up for comfort. While described as \"boring\" when not raining, it provides a break from indoor spaces.\n*   **Ground Floor - Entry & Core Areas:**\n    *   **Reception/Admissions/Co-working Area:** Upon entering, this multi-purpose space serves as the initial point of contact. You'll find counselors here for admissions inquiries. This area is slated for conversion into a larger co-working space, emphasizing collaborative study.\n    *   **Central Courtyard:** This is a key communal space, strategically located.\n        *   **Placement Banners:** Features banners showcasing past successful placements, though these are subject to change as alumni advance their careers.\n        *   **Future Co-working:** Much of this open area will eventually be converted into additional co-working spaces, reinforcing the focus on collaborative learning.\n        *   **Event Space:** This courtyard is also designated as a versatile venue for campus events.\n    *   **Mess & Canteen:**\n        *   **Integrated Dining:** Located conveniently on the ground floor, this is where all your meals (breakfast, lunch, and dinner) will be served. The emphasis is on keeping everything in one central location for ease of access.\n        *   **Canteen:** A separate canteen is available for purchasing additional items.\n\n### II. Academic Environment & Programs\n\nThe campus boasts dedicated learning spaces and specialized programs tailored for different learner needs and career goals.\n\n*   **Classrooms:**\n    *   **Classroom 1 (Ground Floor):** This is the primary classroom for the \"School students\" (likely the main program).\n        *   **Capacity:** Approximately 150 students.\n        *   **Daily Schedule:** Classes typically run from 11:00 AM to 7:00 PM.\n        *   **Immersive Learning:** This is where learners will spend a significant portion of their time, engaging directly with faculty.\n    *   **Super 30 Classroom (First Floor):** A distinct classroom dedicated to the \"Super 30\" program.\n        *   **Capacity:** Smaller, accommodating 80-90 people, aligning with the program's selective nature.\n        *   **Accessibility:** Although associated with Super 30, it is open for use by all students.\n*   **The \"Super 30\" Program: A Deep Dive**\n    *   **Concept:** This is described as the \"most actionable program,\" designed for individuals who are primarily undergraduates (typically 2024-2025 cohorts) and are actively seeking to secure jobs.\n    *   **Actionable Learning:** The program's core focus is on practical, job-oriented skills and rapid career outcomes.\n    *   **Batch Size & Outcomes:** The batch size is kept deliberately small to ensure high success rates. The ambitious goal is an 80-90% placement rate within a challenging 6-month timeframe.\n    *   **Selectivity:** Due to its intensive nature and high success targets, entry into the Super 30 program is highly competitive, with only about 5% of applicants being selected. This selectivity may become even higher in the future.\n    *   **Program Leadership:** The speaker (who is giving the tour) will be heavily involved in leading the Super 30 classes, highlighting its importance to the institution's mission.\n*   **The Auditorium (Second & Third Floors):**\n    *   **Multi-Floor Design:** A unique space spanning two combined floors, creating a large, open auditorium.\n    *   **Capacity:** Designed to accommodate around 150 people. (Note: Seating was not yet installed at the time of the tour).\n    *   **Purpose:** Primarily intended for events, orientations, and potentially large-group classes if suitable. It may also be used for educational movie screenings (e.g., \"The Social Network\" for computer science induction).\n*   **Faculty and Support Staff:**\n    *   **Dedicated Educators:** The institution emphasizes a holistic learning experience supported by a team of experienced professionals.\n    *   **Key Faculty Mentioned:**\n        *   **Rouser:** Specializes in Data Structures and Algorithms (DSA).\n        *   **Rahul, Sukcham:** Specialize in Development (Dev). (Noted there are \"too many dev people, not enough DSA people.\")\n        *   **Duv:** Will be leading Data Structures and Algorithms (DSA).\n        *   **Rahul, Deepain, Yash:** Will serve as Batch Managers and Teachers, assisting with Development (Dev) aspects.\n    *   **Support System:** A comprehensive support structure includes teachers, batch managers, and Teaching Assistants (TAs) to ensure a well-rounded educational experience.\n\n### III. Residential & Support Services\n\nThe campus offers convenient on-site accommodation and wellness facilities.\n\n*   **Hostel Accommodation:** Separate and secure hostel facilities are provided for male and female students, emphasizing convenience and safety.\n    *   **Boys' Hostel:**\n        *   **Separate Entry:** Has its own distinct entrance.\n        *   **Room Tiers & Pricing:**\n            *   **Cheapest Tier:** Approximately \u20b910,000 per month (\u20b91.2 lakhs per year), inclusive of food. Described as \"pretty cheap\" for Noida, though \"not the best\" in terms of luxury.\n            *   **Mid-Tier (Four Sharing):** These rooms accommodate four students. Some have windows, others do not, which may influence price or desirability. Features include beds, cupboards, and tables. Critically, **AC is highlighted as \"damn good,\"** making the rooms comfortable even in hot weather.\n            *   **Slightly Better/Bigger Rooms (Four Sharing):** Offers more space than the mid-tier options.\n            *   **Best Tier:** A limited number of premium rooms (approx. 4 for boys) are available.\n    *   **Girls' Hostel:**\n        *   **Completely Isolated Entry:** Features a dedicated, separate entrance and staircase, ensuring enhanced privacy and security.\n        *   **Layout:** Comprises five rooms, completely isolated from the rest of the building.\n        *   **Capacity:** Each room accommodates four girls, totaling a capacity of 20 girls (with 12 currently occupied).\n        *   **Room Features:** Includes washrooms and \"better rooms\" with two windows for natural light.\n*   **Wellness & Fitness:**\n    *   **Gym (Second Floor):** A fully equipped gym facility is available for students.\n        *   **Integrated Facilities:** Washrooms are conveniently located nearby.\n        *   **Timed Access:** Separate timings will be allotted for girls, boys, and teachers to ensure equitable access.\n        *   **Incentive Program:** Students who regularly attend the gym will be awarded \"points,\" encouraging a healthy lifestyle.\n\n### IV. Admissions & Program Focus\n\n*   **Admissions:** Admissions for the 2025 cohort are open, indicating continuous intake for their programs.\n*   **Core Philosophy:** The campus is strategically designed to keep all essential facilities (classrooms, dining, accommodation, recreation) in a single, cohesive location. This integrated approach aims to maximize convenience for learners, allowing them to focus entirely on their studies and overall development within a supportive ecosystem. The goal is to ensure a \"wholesome experience\" for every student.\n\nThis structured overview should provide you with a comprehensive understanding of the 100x Campus, its unique programs like Super 30, and the holistic environment created to support your learning journey.",
        "title": "This Took 10 Years... Welcome to 100xSchool"
    }
]